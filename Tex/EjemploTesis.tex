\documentclass[a4paper,10pt]{book}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}
%\usepackage{mathtools}makeglossaries %.tex
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[spanish, es-tabla]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{incgraph,tikz}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
%\usepackage{babel}
\usepackage{color}
\usepackage{listings}
\usepackage{float}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{titlepic}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{etoolbox}
%\usepackage{cite}    
\usepackage{bm}
\usepackage{makecell}
\usepackage{placeins}



\usepackage{listings}
\lstset{ %
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=none,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=10pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=4,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}



\usetikzlibrary{arrows}

\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}

\renewcommand{\contentsname}{\'Indice General}
\renewcommand{\listfigurename}{\'Indice de Figuras}
\renewcommand{\listtablename}{\'Indice de Tablas}
\renewcommand{\lstlistingname}{Salida}
\renewcommand{\tablename}{Tabla}
\renewcommand{\figurename}{Figura}
\renewcommand\thesubfigure{(\alph{subfigure})}
\renewcommand{\baselinestretch}{1.2} 

\makeatletter
\def\verbatim{\small\@verbatim \frenchspacing\@vobeyspaces \@xverbatim}
\makeatother

\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\fancyhead[LE]{\nouppercase{\leftmark}}
\fancyhead[RO]{\nouppercase{\leftmark}}

\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0.4pt}



%\restylefloat{table}


\definecolor{TPcolor}{HTML}{A9D18E} % Verde claro para TP
\definecolor{TNcolor}{HTML}{D9EAD3} % Verde muy claro para TN
\definecolor{FPcolor}{HTML}{F4CCCC} % Rojo muy claro para FP
\definecolor{FNcolor}{HTML}{E06666} % Rojo claro para FN

\title{Comparación de Técnicas de Aprendizaje
	Automático Supervisado Aplicadas a Datos Cardiólogos}
\author{Autor: Nicolás Seivane \\ Tutora: Andrea Rey}
\date{Universidad Nacional de Hurlingham (UNAHUR)}
\titlepic{\vspace{12cm}\includegraphics[width=0.15\textwidth]{logo.jpg}}

\usepackage{titlesec}

% Chapters justificados
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\justifying} % estilo del capítulo
{\chaptername\ \thechapter}{20pt}{\Huge}

% Sections justificados
\titleformat{\section}
{\normalfont\Large\bfseries\justifying} % estilo de la sección
{\thesection}{1em}{}

% Subsections justificados
\titleformat{\subsection}
{\normalfont\large\bfseries\justifying}
{\thesubsection}{1em}{}







\begin{document}


\maketitle


	\tableofcontents

	\renewcommand{\listfigurename}{Índice de Figuras}
	\listoffigures
    \renewcommand{\listtablename}{Índice de Tablas}
	\listoftables




%% AR: Muy buena idea lo del Glosario. El mismo debería ir luego del índice de tablas o al final del documento.


\chapter*{Glosario}

\begin{longtable}{p{3cm} | p{10cm}}
	\textbf{Notación} & \textbf{Descripción} \\
	\toprule
	$n$ & Número total de registros o ejemplos en el dataset. \\
	$D$ & Cantidad de atributos de cada registro. \\
	$L$ & Número total de clases posibles en el problema de clasificación. \\
	$\hat{\theta}_{jvl}$ & Probabilidad estimada de que el atributo $X_j$ tome el valor $x_{jv}$ dado que la clase es $C_l$. \\
	$\hat{\pi}_k$ & Probabilidad a priori estimada de que un registro pertenezca a la clase $C_k$. \\
	$\bm{x}_i$ & Vector de características del registro $i$, $\bm{x}_i = (x_{i1}, x_{i2},\dots, x_{iD})$. \\
	$X_j$ & $j$-ésimo atributo de un registro. \\
	$C_l$ & $l$-ésima clase del conjunto de clases $\{C_1, C_2,\dots, C_L\}$. \\
	$\bm{w}$ & Vector normal al hiperplano en SVM. \\
	$\beta$ & Término de sesgo o intercepto en SVM o regresión logística. \\
	$\xi_i$ & Variable de holgura (\emph{slack}) para el registro $i$-ésimo en SVM de margen suave. \\
	$C$ & Parámetro de regularización en SVM que controla el equilibrio entre margen y errores. \\
	$K(\bm{a},\bm{b})$ & Función kernel que calcula la similitud entre vectores $\bm{a}$ y $\bm{b}$. \\
	$\gamma$ & Parámetro de ancho en el kernel RBF (Radial Basis Function). \\
	$r$ & Coeficiente de desplazamiento en kernels polinómicos y sigmoides. \\
	$d$ & Grado del polinomio en el kernel polinómico.
	%% AR: $d$ y $D$ son diferentes. No usar $d'$ sino $d$. 
	%% NS: Corrigo en todo el doc.
	\\
	$\tilde{J}(\bm{\beta})$ & Función de costo regularizada en regresión logística. \\
	$\alpha$ & Parámetro de regularización L1 (Lasso) o L2 (Ridge) en regresión. \\
	$\bm{H}$ & Matriz Hessiana de la función de costo en métodos Newton-CG. \\
	$\bm{B}$ & Aproximación de la matriz Hessiana en BFGS. \\
	$\hat{y}$ & Predicción final en un modelo de clasificación. \\
	$\widehat{y_a}(\bm{x})$ & Predicción del árbol $a$ en Random Forest para la observación $\bm{x}$. \\
	$A$ & Número total de árboles en un Random Forest. \\
	$M_l$ & Valor de la métrica (\emph{accuracy}, F1-score, etc.) para la clase $l$. \\
	$|C_l|$ & Cardinalidad o número de registros pertenecientes a la clase $l$.
	%% AR: Entonces sería $|C_l|$. Yo pondría $|\cdot|$ : Cantidad de elementos en el conjunto de interés.
	%% NS: Listo.
	\\
	$\widehat{M}_{\text{weighted}}$ & Métrica ponderada por tamaño de clase en problemas multiclase. \\
	$K$ & Número de pliegues en validación cruzada $K$-fold. \\
	$G_i$ & $i$-ésimo pliegue o grupo en validación cruzada. \\
	$H(D)$ & Entropía de Shannon del conjunto de datos $D$. \\
	$\text{Gini}(t)$ & Índice de Gini de un nodo $t$ en un árbol de decisión. \\
	$f(x)$ & Función de densidad de probabilidad gaussiana para atributo continuo. \\
	$\mu_{jl}$ & Media del atributo $X_j$ en la clase $C_l$. \\
	$\sigma_{jl}^2$ & Varianza del atributo $X_j$ en la clase $C_l$. \\
	\bottomrule
\end{longtable}

\chapter*{Resumen}

\noindent
En este trabajo se realiza un análisis comparativo de distintas técnicas de \textbf{Aprendizaje Automático Supervisado} aplicadas a datos cardiológicos, con el objetivo de evaluar su desempeño en la clasificación de pacientes según riesgo o presencia de enfermedad cardíaca.\\

\noindent
En primer lugar, se lleva a cabo una \textbf{preparación del \emph{dataset}}, que incluye limpieza, transformación de variables, selección de atributos y división en conjuntos de entrenamiento y prueba. Se definen las notaciones fundamentales utilizadas a lo largo del trabajo, tales como el número de muestras $n$, la cantidad de atributos $D$, el número de clases $L$, y los parámetros estimados para algunos modelos.\\

\noindent
Luego, se describen los principales algoritmos estudiados:

\begin{itemize}
	\item \textbf{Naïve Bayes}, se resalta su fundamento probabilístico y el supuesto de independencia condicional.
	\item \textbf{Regresión Logística}, incluyendo la función logística, el hiperplano de decisión y el proceso de optimización.
	\item \textbf{Máquinas de Vectores de Soporte (SVM)}, considerando márgenes duros y suaves, los hiperparámetros clave $(C, \gamma)$, el rol del vector normal $\bm{w}$ y el uso de \emph{kernels}.
	\item \textbf{Árboles de Decisión} y \textbf{Random Forest}, destacando la construcción jerárquica de reglas, los criterios de impureza y la reducción de varianza aportada por los métodos de ensamble.
\end{itemize}

\noindent
Para cada método se explican los \textbf{hiperparámetros relevantes}, su interpretación y cómo afectan el ajuste del modelo. Se especifican los valores considerados o el procedimiento utilizado para seleccionarlos.\\

\noindent
Posteriormente, se detalla la \textbf{metodología de evaluación}, incluyendo las métricas utilizadas derivadas de la matriz de confusión: exactitud, precisión, recall, especificidad, sensibilidad y F1-Score.\\

\noindent
A continuación, se presentan y discuten los \textbf{resultados obtenidos} para cada modelo. Se comparan sus desempeños utilizando las métricas seleccionadas. El método con mejor desempeño en ambos casos es el \textbf{Random Forest}, donde obtiene valores altos en las métricas utilizadas para evaluar, siendo claro el poder clasificador que se obtiene al tener varios arboles de decision distitnos que clasifican un registro, obteniendo la clasificacion más votada. En contra, se nota como a mayor cantidad de datos y claes para clasificar, el método tarda más cantidad de tiempo en procesar una decisión.\\
%% AR: Indicar el método que se destacó.
%% NS: Creo que ahí esta, no se si poner mas.

\noindent
Finalmente, se exponen las \textbf{conclusiones generales} del estudio, señalando qué métodos mostraron mejor rendimiento en el \emph{dataset} de cardiología, qué limitaciones presenta el estudio y cómo podrían extenderse o mejorarse los análisis en trabajos futuros mediante técnicas adicionales, mayor optimización o validación más exhaustiva.\\


\chapter{Introducción}



Las enfermedades cardiovasculares son la causa número uno de muerte globalmente, con un estimado de 17.9 millones de vidas cada año, aproximadamente el 31\% de todas las muertes globales. La idea central de este trabajo es encontrar una técnica de aprendizaje automático  óptima para poder realizar predicciones de si un paciente tiene altas probabilidades de tener insuficiencia cardíaca.\\

\noindent
Por otro lado, la cardiotocografía (CTG) es un registro continuo de la frecuencia cardíaca fetal que se obtiene mediante un transductor de ultrasonidos colocado en el abdomen materno. La CTG se utiliza ampliamente durante el embarazo como método para evaluar el bienestar fetal, sobre todo en embarazos con mayor riesgo de complicaciones.\\
	

\noindent
El objetivo general de este trabajo es comparar el rendimiento de diversas técnicas de Aprendizaje Automático Supervisado con el fin de recomendar aquella que presente el mejor desempeño al aplicarse sobre un conjunto de datos cardiológicos. Se expondrán las técnicas empleadas y las métricas utilizadas para medidas de calidad de un clasificador y, en consecuencia, cual técnica resulta más adecuada para el problema del pre-diagnóstico de enfermedades cardíacas.

\section{Motivación}

El proceso de diagnóstico médico puede ser extenso, incluso contando con la mejor disposición del personal de salud, ya que con frecuencia requiere la recopilación y análisis de datos provenientes de distintos estudios. El propósito de este trabajo es contribuir a agilizar dicho proceso, identificando técnicas que puedan ser utilizadas por los profesionales médicos como herramientas complementarias para realizar diagnósticos. No solo resulta fundamental la posibilidad de obtener diagnósticos más ágiles, sino también la de reconocer qué atributos o características de los estudios resultan más significativos que otros para un diagnóstico determinado.

\section{Estado del Arte}


Las técnicas de Aprendizaje Automático se utilizan cada vez más en la investigación cardiovascular. El trabajo de Isaksen et al.~\cite{isaksen2025evaluating}
%(2025) 
presenta recomendaciones y orientaciones para la evaluación adecuada de modelos de aprendizaje automático supervisado en cardiología, destacando los problemas específicos asociados con estas técnicas, como la fuga de datos (\textit{data leakage}) y el desequilibrio de clases. Por su parte, el documento de Kumar y Kumar~\cite{kumar2021machine}
%(2021) 
revisa las metodologías de aprendizaje automático para el diagnóstico de cardiopatías utilizando métodos no invasivos, un área crucial dada la alta mortalidad anual (17.9 millones de personas) asociada con los problemas cardíacos.\\

%% Algo sobre variables cardiológicas en general, y sobre trabajos que usen alguno de los que yo use a cardiología.

\noindent
En los estudios cardiológicos, las variables utilizadas pueden agruparse en tres grandes categorías principales: parámetros clínicos estructurados, señales cardíacas y datos provenientes de imágenes médicas. Los parámetros clínicos incluyen variables demográficas, antecedentes, síntomas y mediciones de laboratorio, que suelen encontrarse en bases de datos estructuradas como el conjunto de Cleveland, utilizado comúnmente en estudios de clasificación de enfermedades cardíacas. Las señales cardíacas, como los electrocardiogramas (ECG) y fonocardiogramas (PCG), aportan información temporal y frecuencial altamente relevante para la detección de arritmias, soplos o patrones eléctricos anómalos. Finalmente, los datos de imagen —que abarcan ecocardiografías, resonancias magnéticas cardíacas (CMR), tomografías computarizadas (CCT) o estudios SPECT— permiten evaluar morfología, función cardíaca y perfusión.\\

\noindent
En el ámbito de los métodos de aprendizaje automático aplicados a cardiología, varios trabajos han empleado los mismos algoritmos utilizados en este estudio. Por ejemplo, modelos basados en \textit{Support Vector Machines} (SVM) han demostrado un rendimiento competitivo en la clasificación de ECG y en la detección de enfermedades a partir de parámetros clínicos, mostrando en muchos casos un desempeño superior a métodos más simples. Los clasificadores \textit{k}-Nearest Neighbors (\textit{k}-NN) han sido utilizados para tareas como la detección de arritmias mediante características temporales del ECG o para la predicción de riesgo a partir de datos clínicos estructurados. Otros estudios han empleado Naïve Bayes o árboles de decisión para la clasificación de cardiopatías utilizando tanto datos tabulares como señales procesadas. Estas aplicaciones muestran que los modelos implementados en este trabajo están bien representados dentro de la literatura del área y cuentan con antecedentes sólidos en tareas diagnósticas dentro del dominio cardiológico.\\

%% AR: Ver de agregar algo sobre herramientas clásicas que usan los médicos.

 
\section{Conjuntos de Datos %Utilizados
}

Para la realización de este trabajo se exploraron diversas plataformas en busca de conjuntos de datos reales que resulten relevantes para el estudio mediante técnicas de aprendizaje automático. Durante esta búsqueda se identificaron múltiples \emph{datasets} de distinta naturaleza: algunos correspondientes a problemas de \textbf{clasificación binaria}, donde las observaciones se asocian a dos posibles clases, y otro \emph{dataset} de \textbf{clasificación multiclase}, con más de dos categorías posibles.\\

\noindent
Se observa, además, una marcada predominancia de conjuntos de datos provenientes del ámbito \textbf{médico}, dentro de los cuales se seleccionan aquellos considerados más adecuados para las pruebas de los métodos de aprendizaje automático, abarcando tanto casos binarios como multiclase.\\

\noindent
Todos estos \emph{datasets} son procesados anteriormente a las pruebas realizadas, en donde se eliminaron tanto registros duplicados como con datos faltantes, tampoco se tienen en cuenta aquellos con datos atípicos \textit{a priori}, como un caso donde un paciente tiene colesterol 0.\\


\noindent
Se realiza una pequeña descripción de cada atributo utilizado en cada \emph{dataset}. Si el atributo es categórico, se informa la distribución de los valores que posee dicho atributo y en caso de resultar ser un atributo numérico, se informa la media de los valores de dicho atributo, junto con el valor máximo y mínimo que posee. Se realiza lo anterior para contextualizar los atributos y ver los rangos de valores con que se trabajará. \\


\subsection{Dataset Binario: \emph{Insuficiencia  Cardíaca Predicción}}

\noindent
Este \emph{dataset}~\cite{HeartFailure}, llamado \textit{HeartFailure} fue creado mediante la combinación de cinco \emph{datasets} independientes en 11 atributos comunes, logrando el \emph{dataset} más grande de información de enfermedades cardiovasculares utilizado para investigación. Los cinco \emph{datasets} utilizados son:

\begin{itemize}
	\item Cleveland: 303 observaciones,
	\item Hungarian: 294 observaciones,
	\item Switzerland: 123 observaciones,
	\item Long Beach VA: 200 observaciones,
	\item Stalog (Heart) Data Set: 270 observaciones.
\end{itemize}

\noindent
En la  Tabla~\ref{tab:tablebinario} se muestra qué tipo de datos son los atributos del \emph{dataset} que son utilizados en este trabajo, considerando que estos mismos ya fueron previamente procesados para poder ser utilizados en las técnicas de aprendizaje automático. Luego, la Tabla~\ref{tab:cant_datos_binarios} muestra la cantidad de registros y atributos que son utilizados, junto al tipo de dato que son.\\


\begin{table}[htb]%[H]
	\centering
	\caption{Cantidad de registros utilizados.}
	\label{tab:cant_datos_binarios}
	\begin{tabular}{lc}
		\toprule
		\textbf{Cantidad de registros} & \textbf{918} \\
		\textbf{Cantidad de atributos} & \textbf{11} \\
		\textbf{Atributos Categóricos} & \textbf{5} \\
		\textbf{Atributos Numéricos} & \textbf{6} \\

		\bottomrule
	\end{tabular}
\end{table}

%% NS: Pongo lineas verticales, quedan mal lo de los tipos de datos sin lineas verticales.
%% AR: En general, no se usan líneas verticales. Fijate si te gusta cómo dejé esta tabla (la original está comentada debajo). Si te parece bien, sacar todas las líneas verticales de las tablas.
%% NS: Esta bien, el problem alo tenia en otra tabla que quedaba medio raro, pero mantengo el estilo.tabular
	
	\begin{table}[htb]
		\caption{Tipo de atributo del conjunto binario.} 
		\centering
		\label{tab:tablebinario}
			\begin{tabular}{llcc}
			\toprule
			\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{¿Está codificado?} & \textbf{Unidad} \\
			\hline
			Age        & Numérico (int) & No & Años \\
			\hline
			Sex             & Categórico (string) & No & -\\
			\hline
			ChestPainType               & Categórico (string) & No & -\\
			\hline
			RestingBP     & Numérico (int) & No & mm Hg\\
			\hline
			Cholesterol    & Numérico (int) & No & mm/dl\\
			\hline
			FastingBS                 & Numérico (int) & Sí & mg/dl\\
			\hline
			RestingECG                      & Categórico (string) & No & -\\
			\hline
			MaxHR    & Numérico (int) & No & -\\
			\hline
			ExerciseAngina               & Categórico (string) & No & -\\
			\hline
			Oldpeak                 & Numérico (float) & No & ST en depresión\\
			\hline
			ST\_Slope                 & Categórico (string) & No & -\\
			\hline
			HeartDisease                 & Numérico (int)     & Sí & -\\
			\bottomrule
		\end{tabular}
				
	\end{table}

	
%	\begin{table}[htb]
%		\caption{Tipo de atributo del conjunto binario.} 
%		\centering
%		\label{tab:tablebinario}
%			\begin{tabular}{l|c|c|c}
%			\toprule
%			\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{¿Está codificado?} & \textbf{Unidad} \\
%			\hline
%			Age        & Numérico (int) & No & Años \\
%			\hline
%			Sex             & Categórico (string) & No & -\\
%			\hline
%			ChestPainType               & Categórico (string) & No & -\\
%			\hline
%			RestingBP     & Numérico (int) & No & mm Hg\\
%			\hline
%			Cholesterol    & Numérico (int) & No & mm/dl\\
%			\hline
%			FastingBS                 & Numérico (int) & Sí & mg/dl\\
%			\hline
%			RestingECG                      & Categórico (string) & No & -\\
%			\hline
%			MaxHR    & Numérico (int) & No & -\\
%			\hline
%			ExerciseAngina               & Categórico (string) & No & -\\
%			\hline
%			Oldpeak                 & Numérico (float) & No & ST en depresión\\
%			\hline
%			ST\_Slope                 & Categórico (string) & No & -\\
%			\hline
%			HeartDisease                 & Numérico (int)     & Sí & -\\
%			\bottomrule
%		\end{tabular}
%				
%	\end{table}
	
	
\subsubsection{\underline{Descripción de los atributos:}}

	
	\noindent
	\textbf{Age}: Este atributo refiera a la edad de los pacientes. Los pacientes tienen una media de edad de 53 años, con una edad máxima de 77 y de edad mínima  de 28, con proporciones de edad bastante bien distribuidas, siendo la menor de 0.11\% para algunas edades y la mayor de 4.14\% para otras edades, teniendo otras distribuciones entre estos dos rangos.\\
	
	\noindent
	\textbf{Sex}: Refiere al sexo de los pacientes; hay una distribución de sexo del 78.98\% masculinos y el 21.02\%  femeninos.\\
	
	\noindent
	\textbf{ChestPainType}: Tipo del dolor en el pecho, con exactitud dolor torácico causado por isquemia cardíaca. Tiene una distribución 18.85\% de angina atípica, luego un 22.11\% de dolor no anginoso, un 54.03\% asintomático, y un 5.01\% de dolor de pecho anginoso típico.\\
	
	\noindent
	\textbf{RestingBP}: Se está describiendo la presión sanguínea en reposo, donde hay una distribución de 51.09\% de mujeres, codificadas en 1 y 48.91\% de hombres, codificados en 0.\\
	
	%{0.2cm}
	\noindent
	\textbf{Cholesterol}: Este atributo es el colesterol sérico, la medida total de colesterol en sangre; tiene un valor medio en los pacientes de 199.02, con un valor máximo de 603.00 y un valor mínimo de 0.00. Se encuentra en miligramos por decilitro. \\
	
	%{0.2cm}
	\noindent
	\textbf{FastingBS}: Es la Glucosa en sangre en ayuno; hay un 76.66\% de registros con  valores de glucosa en sangre menores a 120 mg/dl, codificado en 0, y un 23.34\% con valores mayores a 120 mg/dl, codificado en 1.\\
	
	%{0.2cm}
	\noindent
	\textbf{RestingECG}: Son los resultados de electrocardiogramas en reposo; hay 60.09\% codificado en Normal, un 19.41\% codificado en ST (tiene una anormalidad en el estudio) y, por último, un 20.50\% codificado en LVH (probablemente una hipertrofia en el ventrículo izquierdo).\\
	
	%{0.2cm}
	\noindent
	\textbf{MaxHR}: Este atributo es el máximo ritmo cardíaco registrado, tiene una media en los pacientes de 136.79, con un valor máximo de 202.00 y un valor mínimo de 60.00. \\
	%{0.2cm}
	
	\noindent
	\textbf{ExerciseAngina}: Es la angina producido por ejercicio, dolor en el pecho; donde hay un 59.54\% que no tenían dolor, codificado en N, y hay un 40.46\% que sí tenían dolor, codificado en Y. \\
	
	%{0.2cm}
	\noindent
	\textbf{Oldpeak}: Valor máximo de depresión del segmento ST (en milímetros) registrado en todas las derivaciones contiguas durante una prueba de esfuerzo. Forma parte del cálculo del riesgo de un paciente de isquemia o infarto de miocardio; valores más altos indican un mayor riesgo de enfermedad coronaria; tiene una media de 0.90, valor máximo de 6.20 y valor mínimo de -0.10. \\
	
	%{0.2cm}
	\noindent \textbf{ST\_Slope}: La pendiente del segmento ST durante el ejercicio máximo; hay un 43.08\% en Up, un 50.05\% en Flat y luego un 6.87\% en Down [Up: pendiente ascendente, Flat: pendiente plana, Down: pendiente descendente].\\

	
	%{0.2cm}
	\noindent
	\textbf{HeartDisease}: Variable de salida si posee una enfermedad cardíaca; donde hay un 44.71\% que no tiene enfermedad cardíaca, codificado en 0, y hay un 55.29\% que sí tienen enfermedad cardíaca, codificado en 1.
	Siendo ésta la \textbf{variable objetivo}.


\subsection{Dataset Multiclase: \emph{Cardiotocografía Predicción}}
	

\noindent
	En el \emph{dataset}~\cite{Cardiotocografia} utilizado se procesaron automáticamente 2126 cardiotocogramas fetales y se midieron sus características diagnósticas. Tres obstetras expertos clasificaron los CTG y se les asignó una etiqueta de clasificación consensuada. La clasificación se realizó con respecto al estado fetal, el cual puede ser normal, sospechoso o patológico.\\

    \noindent
En la Tabla~\ref{tab:tablaej} se muestran los tipos de las variables utilizadas en este trabajo, junto con la media y valores tanto máximos como mínimos. Luego, la Tabla~\ref{tab:cant_datos_multiclase} muestra la cantidad de registros y atributos que son utilizados, junto al tipo de dato que son.


\begin{table}[htb]%[H]
	\centering
	\caption{Cantidad de registros utilizados.}
	\label{tab:cant_datos_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Cantidad de registros} & \textbf{2115} \\
		\textbf{Cantidad de atributos} & \textbf{21} \\
		\textbf{Atributos Categóricos} & \textbf{0} \\
		\textbf{Atributos Numéricos} & \textbf{21} \\
		
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]
\caption{Tipo de atributo del conjunto multiclase.}
\label{tab:tablaej}
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{Media} & \textbf{Máximo} & \textbf{Mínimo}\\
		\hline
		LB        & Numérico (int) & 133.301 &160.000 &106.000\\
		
		\hline
		AC             & Numérico & 0.003 & 0.019 & 0.000\\
		\hline
		FM               & Numérico (float)& 0.009 & 0.481 & 0.000\\
		\hline
		UC     & Numérico (float) & 0.004 &0.015 &0.000\\
		\hline
		DL    & Numérico (float)&0.001 & 0.015 & 0.000\\
		\hline
		DS                 & Numérico (float)&- &- &-\\
		\hline
		DP                      & Numérico (float)&- &- &-\\
		\hline
		ASTV    & Numérico (int) &46.977 &87.000 &12.000\\
		\hline
		MSTV               & Numérico (float) &1.335 & 7.000&0.200\\
		\hline
		ALTV                 & Numérico (int) &9.759 &91.000 &0.000\\
		\hline
		MLTV                 & Numérico (float)&8.170 &50.700 &0.000\\
		\hline
		Width                 & Numérico (int) &70.511 &180.000 &3.000\\
		\hline
		Min                 & Numérico (int) &93.574 &159.000 &50.000\\
		\hline
		Max                 & Numérico (int)  &164.085 &238.000 &122.000\\
		\hline
		Nmax                 & Numérico (int)  &4.075 &18.000 &0.000\\
		\hline
		Nzeros                 & Numérico (int)  &- &- &-\\
		\hline
		Mode                 & Numérico (int)  &137.448 &187.000 &60.000\\
		\hline
		Mean                 & Numérico (int)& 134.596& 182.000&73.000 \\
		\hline
		Median                 & Numérico (int) &138.084 &186.000 &77.000 \\
		\hline
		Variance                 & Numérico (int)&18.891 &269.000 &0.000 \\
		\hline
		Tendency                 & Numérico (int)&- &- &- \\
		\hline
		NSP                 & Categórico (string) &- &- &-\\
		\bottomrule
	\end{tabular}
\end{table}


\textbf{\underline{Descripción de los atributos:}}

\begin{itemize}
	\item[\textbf{LB}] Frecuencia cardíaca fetal basal (latidos por minuto).
	\item[\textbf{AC}] Número de aceleraciones por segundo.
	\item[\textbf{FM}] Número de movimientos fetales por segundo.
	\item[\textbf{UC}] Número de contracciones uterinas por segundo.
	
	\item[\textbf{DL}] Número de desaceleraciones leves por segundo.
	\item[\textbf{DS}] Número de desaceleraciones severas por segundo.
	
	\item[\textbf{DP}] Número de desaceleraciones prolongadas por segundo.  
	Hay un 91.58\% con valor de 0, un 3.40\% con valor 0.002, un 1.13\% con valor 0.003, 
	un 3.31\% con valor 0.001, un 0.43\% con valor 0.004 y un 0.14\% con valor 0.005.
	
	\item[\textbf{ASTV}] Porcentaje de tiempo con variabilidad anormal a corto plazo.
	\item[\textbf{MSTV}] Valor medio de la variabilidad a corto plazo.
	\item[\textbf{ALTV}] Porcentaje de tiempo con variabilidad anormal a largo plazo.
	\item[\textbf{MLTV}] Valor medio de la variabilidad a largo plazo.
	
	\item[\textbf{Width}] Ancho del histograma de CTG.
	\item[\textbf{Min}] Mínimo del histograma de CTG.
	\item[\textbf{Max}] Máximo del histograma de CTG.
	
	\item[\textbf{Nmax}] Número de picos del histograma de CTG.
	\item[\textbf{Nzeros}] Número de ceros del histograma de CTG.  
	Hay un 76.26\% con valor 0, un 17.30\% con valor 1, un 0.99\% con valor 3, 
	un 5.11\% con valor 2, un 0.09\% con valor 4, un 0.05\% con valor 10, 
	un 0.09\% con valor 5, un 0.05\% con valor 8 y un 0.05\% con valor 7.
	
	\item[\textbf{Mode}] Moda del histograma de CTG.
	\item[\textbf{Mean}] Promedio del histograma de CTG.
	\item[\textbf{Median}] Mediana del histograma de CTG.
	\item[\textbf{Variance}] Varianza del histograma de CTG.
	
	\item[\textbf{Tendency}] Tendencia del histograma de CTG.  
	Hay un 39.67\% con valor 1, un 52.53\% con valor 0 y un 8.27\% con valor -1.
	
	\item[\textbf{CLASS}] Código de clasificación del estado fetal  
	(N = normal; S = sospechoso; P = patológico).  
	Hay un 13.81\% sospechosos, 77.92\% normales y 8.27\% patológicos.  
	Siendo ésta la \textbf{variable objetivo}.
\end{itemize}
	%{0.2cm}



\chapter{Descripción de los Métodos Utilizados}

\noindent
En este capítulo se presentan los métodos de aprendizaje automático supervisado que se emplearon en este trabajo. 
Se describen sus fundamentos teóricos, las fórmulas y notaciones utilizadas, así como los hiperparámetros y criterios de optimización más relevantes. 
El objetivo es describir y explicar cómo funcionan cada uno de los algoritmos, sus supuestos y la forma en que se aplican a los datos cardiólogos analizados. \\

\noindent
Los métodos requieren de  \textbf{hiperparámetros}, que son un parámetro cuyo valor se fija antes del proceso de entrenamiento de un modelo y que controla aspectos del aprendizaje, como la complejidad del modelo, la velocidad de convergencia o la regularización. 
A diferencia de los parámetros internos del modelo (por ejemplo, los pesos en una regresión logística o en una red neuronal), los hiperparámetros no se aprenden automáticamente a partir de los datos, sino que deben ser seleccionados mediante experimentación, validación cruzada u optimización de hiperparámetros, siendo un análisis en sí mismos.

\noindent
Los \textbf{hiperparámetros} utilizados en este trabajo corresponden a aquellos definidos por la librería \emph{scikit-learn} del lenguaje de programación \emph{Python}, la cual ofrece implementaciones estandarizadas y bien documentadas de modelos clásicos de aprendizaje automático. En particular, la búsqueda en grilla se realiza con los valores posibles que determina la librería, junto con la combinación de valores que son posibles de combinar, como el grado del polinomio cuando utilizamos el kernel de polinomia en \textbf{SVM}. 

\noindent

En particular, se emplearon los valores por defecto sugeridos por la librería y, cuando fue necesario, se ajustaron manualmente algunos hiperparámetros relevantes —como la profundidad máxima del árbol, el criterio de partición o la cantidad de vecinos en modelos basados en distancia— con el fin de evaluar su influencia en el desempeño del modelo. 
Este procedimiento permite garantizar consistencia en las comparaciones, reproducibilidad de los experimentos y una interpretación clara acerca del efecto de cada hiperparámetro sobre los resultados.


\section{Regresión Logística}

\noindent
El modelo de \textbf{Regresión Logística}~\cite{cramer2002origins} (LR, por su equivalente en inglés \emph{Logistic Regression}) es una técnica del análisis de datos utilizada para establecer relaciones entre las variables predictoras y la clase a la cual pertenece cada registro. El modelo permite predecir la probabilidad de que un nuevo registro pertenezca a una clase determinada. Este tipo de modelo de regresión es justamente utilizado para los problemas no linealmente separables, como la mayoría de los problemas.\\

\noindent
A diferencia de la regresión lineal múltiple, la regresión logística predice una probabilidad (valor entre 0 y 1). Ambos modelos son lineales en sus parámetros, pero difieren en la naturaleza de la variable dependiente. El objetivo es estimar los coeficientes de regresión que maximizan la verosimilitud de los datos observados, o encontrar los coeficientes que mejor funcionan para los datos de entrenamiento.\\

\noindent
El modelo busca estimar la probabilidad condicional de que una observación pertenezca a la clase positiva (o clase objetivo). Sea $C_l$ el conjunto de clases posibles, con $l \in \{1,2,\dots,L\}$. Para cada registro $i$ (con $i = 1,2,\dots,n$), su etiqueta verdadera se denota por $l_i$, donde definimos:
\[
l_i =
\begin{cases}
	1, & \text{si la observación pertenece a la clase positiva } C_1, \\
	0, & \text{si la observación pertenece a la clase negativa } C_0.
\end{cases}
\]

\begin{equation}
	P(l_i = 1 \mid \bm{x}_i),
	\label{eq:probabilidad_condicional}
\end{equation}

\noindent
donde $\bm{x}_i = (x_{i1}, x_{i2}, \dots, x_{ik})$ es el vector de características de la observación $i$, el cual también llamamos registro, en el cual $D$ es la cantidad de atributos que posee la observación.\\
%% AR: En el Glosario, la cantidad de atributos es $D$.
%% NS: Ya los cambie en esa parte y en las fomulas de abajo.

\noindent
Por lo cual, la función logística define la \textbf{función de probabilidad} de pertenencia de $\bm x_{i}$ a la clase $1$ como:

\begin{equation}
	p(\bm{x}_i) = P(l_i = 1 \mid \bm{x}_i) =
	\frac{\exp(\beta_0 + \sum_{j=1}^{D} \beta_j x_{ij})}
	{1 + \exp(\beta_0 + \sum_{j=1}^{D} \beta_j x_{ij})},
	\label{eq:funcion_logistica}
\end{equation}

\noindent
donde $\beta_0$ es el intercepto y $\beta_j$ son los coeficientes asociados a cada predictor, para $j=1,2,\dots,D$.\\

\noindent
La función inversa de la función logística, denominada \textbf{\textit{función logit}}, relaciona el logaritmo de las \emph{odds} con un modelo lineal, siendo \emph{odds} lo que se utiliza para analizar si la probabilidad de ocurrencia de un evento -caso/no caso- difiere o no en distintos grupos,

%\begin{equation}
%	\text{logit}(\textbf{antilogic}(x)) = x
%	\label{eq:logit}
%\end{equation}


%\begin{equation}
%	\text{odds} =
%	\frac{p}{1 - p} 
%	\label{eq:odds}
%\end{equation}


\begin{equation}
	\text{logit}(p(\bm{x}_i)) =
	\ln \left( \frac{p(\bm{x}_i)}{1 - p(\bm{x}_i)} \right)
	= \beta_0 + \sum_{j=1}^{D} \beta_j x_{ij}.
	\label{eq:funcion_logit}
\end{equation}

\noindent
Esta transformación asegura que las probabilidades estén acotadas entre 0 y 1, mientras que la combinación lineal de predictores puede tomar cualquier valor real, siendo este último punto un factor que realiza el cálculo de coeficientes muy difícil.
Para solucionar este obstáculo, el cálculo de los coeficientes de regresión se estiman mediante el método de \textbf{Máxima Verosimilitud} (MLE), donde la función de log-verosimilitud a maximizar es

\begin{equation}
	\ell(\beta_0, \beta_1, \dots, \beta_k) =
	\sum_{i=1}^{n} \left[
	l_i \ln(p(\bm{x}_i)) +
	(1 - l_i) \ln(1 - p(\bm{x}_i))
	\right],
	\label{eq:log_verosimilitud}
\end{equation}
%% NS: n lo habiía definido en Accuracy. Seguro ponga todas las notaciones al principio.
%% AR: Agregué (para $i=1,2, \dots,n$) antes de la ecuación \eqref{eq:probabilidad_condicional}.
\noindent
en donde la solución analítica no existe, por lo que se utilizan métodos numéricos iterativos para obtener los parámetros óptimos.\\

\noindent
\textbf{Hiperparámetros}
%% AR: Acá habría que aclara que son los hiperparámetros que se usan en Python. Quizás se pueda poner algo general en la intro del capítulo.
%% NS: Lo hago en la into.
%% AR: A lo que me refería es a que vas a usar los hiperparámetros que se usan en la biblioteca de Python.
%% NS: Lo agregue al principio del capitulo, nombre la libreria y el lenguaje que se usa.
\noindent
\begin{itemize}
	\item \textbf{Parámetro de Regularización ($C$):} Controla la complejidad del modelo. Valores pequeños de $C$ implican mayor regularización (menor sobreajuste), mientras que valores grandes permiten mayor flexibilidad del modelo.
	
	\item \textbf{Penalización (\textit{penalty}):} Es un término regulador que se suma a la función de coste original ($J$) para formar una función ajustada $\tilde{J}$. Controla la capacidad del modelo y reduce el error de generalización.
	
	\begin{itemize}
		
		\item  Regularización L1 (Lasso):
		\begin{equation}
			\tilde{J}_{L1}(\boldsymbol{\beta}) =
			J(\boldsymbol{\beta}) +
			\alpha \sum_{j=1}^{D} |\beta_j|,
			\label{eq:l1_penalty}
		\end{equation}
		donde
		\begin{itemize}
			\item \(\beta_j\) son los coeficientes del modelo, que representan la influencia de cada variable \(x_j\) en la predicción, para $j=1,2,\dots,k$;
			\item Donde \(\alpha \ge 0\) es el parámetro de regularización que controla la intensidad de la penalización: valores más altos implican mayor castigo a los coeficientes. 
		\end{itemize}
		%{eq:l1_penalty}{}
		
	
		\item Regularización L2 (Ridge):
		\begin{equation}
			\tilde{J}_{L2}(\boldsymbol{\beta}) =
			J(\boldsymbol{\beta}) +
			\frac{1}{2}\alpha \sum_{j=1}^{D} \beta_j^2,
			\label{eq:l2_penalty}
		\end{equation}
		donde
		\begin{itemize}
			\item \(\beta_j\) controla cuánto influye cada característica sobre la salida;
			\item \(\alpha \geq 0  \) determina cuánto se penaliza el tamaño de los coeficientes, evitando que crezcan demasiado. %% AR: ¿También es no negativo? Agregarlo.
					%% NS: Agregue asi, porque deberia de ser mayor o igual a 0, si es 0 no hay ajuste, si crece mucho puedo ser contradictorio, etc.
		\end{itemize}
		
		%{eq:l2_penalty}{}
		
		\item Regularización Elastic Net:
		\begin{equation}
			\tilde{J}_{EN}(\boldsymbol{\beta}) =
			J(\boldsymbol{\beta}) +
			\alpha \left[
			\rho \sum_{j=1}^{D} |\beta_j| +
			\frac{1 - \rho}{2} \sum_{j=1}^{D} \beta_j^2
			\right],
			\label{eq:elasticnet_penalty}
		\end{equation}
		donde
		\begin{itemize}
			\item \(\beta_j\) son los coeficientes asociados a cada característica;
			\item \(\alpha \geq 0\) controla el grado total de regularización; %% AR: Agregar restricciones para $\alpha$.
			%%NS: Listo.
			\item \(\rho \geq 0\) balancea la combinación entre L1 y L2. Si es 0 entonces tenemos L2 y si es 1 tenemos L1. %% AR: Agregar restricciones para $\rho$.
			%% NS: Listo ya lo agregue.
		\end{itemize}
		%{eq:elasticnet_penalty}{}
		
	\end{itemize}
	
	\item \textbf{Algoritmo de Optimización (\textit{solver}):}  
	El \textit{solver} es el algoritmo numérico encargado de minimizar la función de coste regularizada $\tilde{J}(\boldsymbol{\beta})$.
	
	\begin{itemize}
		\item Método Newton-CG (basado en segunda derivada).
		\begin{equation}
			\boldsymbol{\beta}^{(t+1)} =
			\boldsymbol{\beta}^{(t)} -
			\bm{H}^{-1}
			\nabla_{\boldsymbol{\beta}} \tilde{J}(\boldsymbol{\beta}^{(t)}),
			\label{eq:newtoncg}
		\end{equation}
		%% AR: ¿La flecha no sería un igual?
		%% NS: Es iterativo en t iteraciones, lo aclaro?
%% AR: No importa, la iteración se define con el igual.
		donde \(\boldsymbol{\beta}\) es el vector de parámetros del modelo que se actualiza en cada iteración. El método utiliza el gradiente $\nabla_{\boldsymbol{\beta}}$ y la inversa de la matriz Hessiana $\bm{H}$ (o una aproximación de la misma mediante \emph{Conjugate Gradient}) para determinar la dirección de actualización del vector de parámetros.
		%		%{eq:newtoncg}{}
		
		\item Método BFGS (quasi-Newton).
		\begin{equation}
			\boldsymbol{\beta}^{(t+1)} =
			\boldsymbol{\beta}^{(t)} -
			\bm{B}^{-1}
			\nabla_{\boldsymbol{\beta}} \tilde{J}(\boldsymbol{\beta}^{(t)}).
			\label{eq:bfgs}
		\end{equation}
		
		En este método, \(\boldsymbol{\beta}\) también es el vector de parámetros a optimizar. La matriz \(\bm{B}\) aproxima al Hessiano, permitiendo actualizaciones sin calcular derivadas segundas reales.
		%		%{eq:bfgs}{}
		
		\item Método Saga. 
		Un algoritmo de tipo \emph{stochastic gradient descent} con soporte para regularización L1 y L2. Permite trabajar con grandes datasets y converger de manera estable incluso con penalización L1.
		
		\item Método Liblinear.
		Optimiza la función de coste usando un método de descenso por coordenadas, eficiente para datasets grandes y modelos lineales. Soporta L1 y L2.
	\end{itemize}
	
	\item 
	\textbf{Estrategia Multiclase (\textit{multi\_class}):}
	La regresión logística está diseñada originalmente para clasificación binaria. Para extenderla a múltiples clases se emplean estrategias como:
	\begin{itemize}
		\item \textit{one-vs-rest} (OvR): Entrena un clasificador por clase.
		\item \textit{multinomial}: Optimiza una única función de verosimilitud multinomial conjunta.
	\end{itemize}
\end{itemize}

\section{Árboles de Decisión}

\noindent
El aprendizaje mediante \textbf{Árboles de Decisión} es un método no paramétrico que utiliza divisiones jerárquicas sobre los atributos de los datos, construyendo reglas de decisión del tipo \textit{if-else} para predecir el valor de una variable objetivo.\\

\noindent  
El objetivo principal es encontrar las divisiones (particiones) que maximicen la pureza de los nodos hijos, es decir, que minimicen la impureza del nodo resultante. Es un método mucho más sencillo de realizar, donde se crea un camino de decisión, por lo cual según los valores de los atributos de un registro podemos predecir a que clase pertenecen $C_l$, donde el cómputo pesado se encuentra en la creación del propio camino. \\

%% AR: puesto que las clases se usan en diversos contextos a lo largo del trabajo, usaría una notación común, que se puede introducir al comienzo de este capítulo (previo cambio con el capítulo de métricas), deciendo algo del estilo "En lo que resta del documento...".
%% NS: Ok, lo agrego.
%% AR: No lo encontré.
%% NS: Debo agregarlo si esta el glosario?
\noindent
En consecuencia, la probabilidad de que un ejemplo en el nodo $t$ pertenezca a la clase $C_l$ se define como

\begin{equation}
	p(l|t) = \frac{N_l(t)}{N(t)},
	\label{eq:probabilidad_clase_nodo}
\end{equation}


\noindent
donde $N(t)$ es la cantidad total de ejemplos en el nodo $t$, y $N_l(t)$ la cantidad de ejemplos de la clase $C_l$. Es importante este fenómeno porque es un costo computacional muy barato el calcular esta probabilidad y se asemejan a las probabilidades equiprobables, donde la probabilidad de una clase en un nodo está dada por la cantidad de ejemplos de la clase que tienen en el mismo.\\

\noindent
De esta manera, la impureza de un nodo $t$ se mide mediante una función $\phi$ que depende de las probabilidades de clase en dicho nodo. La función de impureza $im$ 
%% AR: Antes se usaba $i$ como subíndice, quizás haya que cambiar la notación en este caso.
%% NS: Listo use im para la impureza.
es un hiperparámetro en sí mismo, y debe cumplir las siguientes condiciones, para una función de densidad de probabilidad $\mathbf{p}$:

\begin{itemize}
	\item \textbf{No negatividad:}
	\[
	\phi(\mathbf{p}) \ge 0.
	\]
	
	\item \textbf{Impureza mínima en nodos puros:}
	\[
	\phi(\mathbf{p}) = 0 \quad \text{si existe } l \text{ tal que } p_l = 1.
	\]
	
	\item \textbf{Máxima impureza con distribución uniforme:}
	\[
	\phi(\mathbf{p}) \text{ es máxima cuando } p_l = \frac{1}{L} \quad \forall l.
	\]
\end{itemize}

\noindent
Tanto el índice de Gini como la entropía de Shannon satisfacen estas propiedades, por lo que son funciones de impureza válidas para la construcción del árbol.
\begin{equation}
	im(t) = \phi \big( p(1|t), p(2|t), \ldots, p(K|t) \big).
	\label{eq:impureza_general}
\end{equation}


\noindent
La impureza es máxima cuando las clases están perfectamente mezcladas y es mínima (cero) cuando el nodo contiene solo una clase, en donde la impureza máxima sería que la probabilidad de cada clase es la misma.\\

\noindent
Dentro de las funciones de impureza más utilizadas, se encuentran la entropía de Shannon y el índice de Gini.

%% AR: Incluir una frase que introduzca lo que se presenta a continuación.
%% NS: Ahí debería de etar.
%% AR: Ahí lo agregué en la oración de arriba.


\noindent
\textbf{La entropía de Shannon} mide el grado de desorden o incertidumbre en el conjunto de datos $D$.  
Toma valores cercanos a cero cuando el nodo es puro y crece a medida que las clases se distribuyen de forma más uniforme,

\begin{equation}
	H(D) = - \sum_{k=1}^K 
	\frac{N_k(D)}{N(D)} 
	\log_2 \left( \frac{N_k(D)}{N(D)} \right).
	\label{eq:entropia_shannon}
\end{equation}


\noindent
\textbf{El índice de Gini} cuantifica la probabilidad de clasificar incorrectamente una instancia si se asigna aleatoriamente según la distribución de clases en $t$.  
Al igual que la entropía, es mínimo cuando el nodo es puro y aumenta cuando las clases están mezcladas.
\begin{equation}
	\text{Gini}(t) = 
	1 - \sum_{k=1}^K \big[p(k|t)\big]^2 =
	1 - \sum_{k=1}^K \left(\frac{N_k(t)}{N(t)}\right)^2.
	\label{eq:indice_gini}
\end{equation}


\noindent
La \textbf{reducción de impureza} generada al dividir el nodo $t$ en dos nodos hijos $t_1$ y $t_2$ mediante una partición $s$ se calcula como

\begin{equation}
	\Delta im(s, t) = im(t) - q_1 im(t_1) - q_2 im(t_2),
	\label{eq:disminucion_impureza}
\end{equation}
donde $q_j = \frac{N(t_j)}{N(t)}$ para $j = 1, 2$.

\subsection{Bosques Aleatorios (Random Forest)}

\noindent
El algoritmo de \textbf{Random Forest}~\cite{Breiman2001} (RF)
%%AR: No hay equivalente ya que así se presentó.
%% NS: Ok.
combina múltiples árboles de decisión independientes construidos sobre subconjuntos aleatorios de los datos (muestreo con reemplazo o \emph{bootstrap}).  
Cada árbol se entrena sobre un subconjunto de atributos aleatorios en cada división, lo que introduce diversidad y reduce la varianza.\\

\noindent
La predicción final para clasificación se obtiene mediante el voto mayoritario de los árboles

\begin{equation}
	\hat{y} = 
	\underset{l \in \mathcal{L}}{\operatorname{argmax}}
	\sum_{a=1}^{A} \mathbb{I}
	\big( \widehat{y_a}(\bm{x}) = l \big) ,
	\label{eq:voto_random_forest}
\end{equation}


\noindent
donde $\widehat{y_a}(\bm{x})$ es la predicción del árbol $a$, $A$ es la cantidad total de árboles en el bosque $\mathbb{I}$ es la \textbf{función indicadora}, definida como
\[
\mathbb{I}( \widehat{y_a}(\bm{x}) = l ) =
\begin{cases}
	1, & \text{si el árbol } a \text{ asigna la clase } l, \\
	0, & \text{en caso contrario}.
\end{cases}
\]
Esta función contabiliza cuántos árboles votan por cada clase $l$, permitiendo que el modelo escoja aquella con mayor cantidad de votos.\\
%% AR: Falta decir que $\mathbb{I}$ es la función indicadora.
%% NS: Ahí esta, esta bien utilizada en este caso de random forest?


\noindent
\textbf{Hiperparámetros}

\begin{itemize}
	
	\item \textbf{Criterio de Partición:} 
	Función de impureza utilizada para evaluar la calidad de una división. 
	En \texttt{scikit-learn}, las opciones disponibles son:
	\texttt{``gini''} (índice de Gini) y \texttt{``entropy''} (entropía de Shannon).
	
	\item \textbf{Algoritmo de Construcción:} 
	\textit{ID3} emplea la ganancia de información (entropía), mientras que \textit{CART} utiliza el índice de Gini y construye árboles binarios.
	En \texttt{scikit-learn}, el árbol implementado corresponde a una versión optimizada de \textit{CART}.
	%% AR: Se podrían incluir referencias.
	%% NS: Las busco.
	\item \textbf{Número de Atributos Muestreados (\texttt{max\_features}):} 
	Cantidad de atributos candidatos por división.  
	En Random Forest, las opciones típicas son:
	\[
	\texttt{``sqrt''} = \sqrt{a}, \qquad
	\texttt{``log2''} = \ln(a),
	\]
	o bien un número entero, un porcentaje o \texttt{None} (usar todos los atributos).
	
	\item \textbf{Número de Árboles (\texttt{n\_estimators}):} 
	Cantidad total de árboles que componen el bosque en Random Forest. Valores altos reducen la varianza pero aumentan el costo computacional.
	
	\item \textbf{Profundidad Máxima (\texttt{max\_depth}):} 
	Límite superior a la altura del árbol. Un valor \texttt{None} permite crecer hasta que las hojas sean puras o no haya más divisiones posibles.
	
	\item \textbf{Tamaño Mínimo de Muestra para Dividir (\texttt{min\_samples\_split}):} 
	Número mínimo de muestras requerido para realizar una partición interna.
	
	\item \textbf{Tamaño Mínimo de Muestra en una Hoja (\texttt{min\_samples\_leaf}):}
	Cantidad mínima de muestras que debe contener una hoja para evitar sobreajuste.
	
	\item \textbf{Profundidad Mínima del Nodo Interno (\texttt{min\_impurity\_decrease}):}
	Umbral mínimo de reducción de impureza necesario para aceptar una división.
	
	\item \textbf{Bootstrap (\texttt{bootstrap}):}
	Indica si los árboles del bosque se entrenan con muestras generadas mediante remuestreo con reemplazo. Por defecto, \texttt{True} en Random Forest.

\end{itemize}


\section{Clasificador Naïve Bayes}

\noindent
El \textbf{Clasificador Naïve Bayes}~\cite{hand2001idiot} (NB) es un método supervisado probabilístico basado en el \textit{Teorema de Bayes}, que asume independencia condicional entre los atributos dado la clase. Si bien esta suposición rara vez se cumple estrictamente en la práctica —debido a que las variables suelen estar correlacionadas entre sí, el método continúa funcionando de manera efectiva en numerosos problemas reales.
%% AR: Esta última oración está inclompleta.
%% NS: Listo, ahi redondee. 
La regla de clasificación, o la probabilidad de pertenencia de un registro a una clase $C_l$, donde para cada clases $l \in \{1, 2, \dots, L\}$, donde cada registro tiene $D$ atributos, indexados por $d \in \{1, 2, \dots, D\}$.

\begin{equation}
	P(Y = C_l \mid X_1 = x_1, \ldots, X_d = x_d)
	= \frac{
		P(X_1 = x_1, \ldots, X_d = x_d \mid Y = C_l) \, P(Y = C_l)
	}{
		P(X_1 = x_1, \ldots, X_d = x_d)
	},
	\label{eq:bayes_general}
\end{equation}

%% AR: Observar que antes se usó $l_i$ y acá se usa $Y=C_l$. Habría que unificar.
%% NS: Modifique el anerior para que se note la diferencia.
\noindent
En donde $P(Y = C_l)$ es la \textbf{probabilidad \emph{a priori}} de la clase $C_l$ (qué tan frecuente es en el conjunto de datos). Luego, $P(X_1 = x_1, \ldots, X_d = x_d \mid Y = C_l)$ es la \textbf{verosimilitud}, es decir, cuán probable es observar las características de un registro si supiéramos que pertenece a la clase $C_l$.

\paragraph{Probabilidad Condicional:}

Bajo el supuesto de independencia condicional, la probabilidad condicional puede expresarse como

\begin{equation}
	P(X_1 = x_1, \ldots, X_d = x_d \mid Y = C_l)
	= \prod_{j=1}^{d} P(X_j = x_j \mid Y = C_l),
	\label{eq:independencia_condicional}
\end{equation}


\noindent
Por lo tanto, basta estimar individualmente, para cada atributo 
$X_j$ con $j \in \{1,2,\dots,D\}$ y para cada clase 
$C_l$ con $l \in \{1,2,\dots,L\}$, la probabilidad:

\begin{equation}
	\hat{\theta}_{jvl} =
	P(X_j = x_{jv} \mid Y = C_l) =
	\frac{\left| \{\,X_j = x_{jv} \,\wedge\, Y = C_l\,\} \right|}
	{|Y_l|} ,
	\label{eq:probabilidad_condicional}
\end{equation}
\noindent
donde $v \in \{1,2,\dots,V_j\}$ indexa los valores posibles del atributo $X_j$.

\noindent
Aquí, cada parámetro cumple una función específica:
\begin{itemize}
	
	\item $\hat{\theta}_{jvl}$: probabilidad estimada de que el atributo $X_j$ tome el valor $x_{jv}$ dentro de la clase $C_l$.  
	Representa qué tan típico es ese valor dentro de una clase.
	\item $\left| \{\,X_j = x_{jv} \wedge Y = C_l\,\} \right|$: cantidad de registros en los que el atributo $X_j$ toma el valor $x_{jv}$ y simultáneamente la clase es $C_l$.
	\item $|C_l|$: número total de registros pertenecientes a la clase $C_l$.
\end{itemize}



\noindent
Podemos pensar a $\theta_{jmk}$ como una especie de \textit{“frecuencia interna de la clase”}, una medida que indica qué características son más comunes dentro de cada grupo.\\

\paragraph{Probabilidad \emph{a priori}:}



Otra estimación fundamental a calcular y expresar es la \emph{a priori}.
%, en la cual se asemeja a la probabilidad de que un nodo pertenezca a una clase $k$ en \textit{Random Forest}
%% AR: ¿Por qué ahora se usa $n'$?
%% NS: Listo lo cambie por n.
\begin{equation}
	\hat{\pi}_l = P(Y = C_l) =
	\frac{\#\{Y = C_l\}}{n'}.
	\label{eq:probabilidad_priori}
\end{equation}

\noindent
Los parámetros involucrados son:

\begin{itemize}
	
	\item $\hat{\pi}_l$: probabilidad estimada de que un registro pertenezca a la clase $C_k$.
	Corresponde a la frecuencia relativa de esa clase en la muestra.	
	\item $|C_l|$: cantidad de registros cuya clase es $C_l$	
	\item $n$: tamaño muestral total.
	%% AR: Antes se usó n. No usaría tamaño total, sino tamaño mnuestral. 
	%% NS: Use n como tamaño total, voy a usar n prima.
 %% AR: pero es lo mismo.
\end{itemize}

\noindent
La intuición es sencilla: antes de mirar ningún atributo, ¿qué tan probable es cada clase?\\



%\noindent
%La clase predicha maximiza la probabilidad a posteriori:

%\begin{equation}
%	\hat{Y} = 
%	\underset{C_k}{\operatorname{argmax}}
%	\left[
%	P(Y = C_k)
%	\prod_{j=1}^{d} P(X_j = x_j \mid Y = C_k)
%	\right]
%	\label{eq:regla_clasificacion_nb}
%\end{equation}


\noindent
\textbf{Caso Continuo (Naïve Bayes Gaussiano)}

\noindent
Cuando los atributos son continuos y se asume distribución normal, las verosimilitudes se estiman con la función de densidad Gaussiana:

\begin{equation}
	f(x) =
	\frac{1}{\sigma \sqrt{2\pi}}
	\exp \left[
	-\frac{1}{2} \left(
	\frac{x - \mu}{\sigma}
	\right)^2
	\right],
	\label{eq:densidad_gaussiana}
\end{equation}


\noindent
por lo cual la decisión final se obtiene como

\begin{equation}
	\hat{Y} = 
	\underset{C_k}{\operatorname{argmax}}
	\left[
	\log P(Y = C_l) +
	\sum_{j=1}^{d}
	\log f(x_j \mid \mu_{jl}, \sigma_{jl}^2)
	\right].
	\label{eq:decision_gaussiana}
\end{equation}
\noindent
donde los parámetros de la distribución normal para cada atributo \(X_j\) y clase \(C_l\) se estiman como

\begin{equation}
	\hat{\mu}_{jl}
	= \frac{1}{|C_l|}
	\sum_{i : Y_i = C_l} x_{ij},
	\label{eq:media_gaussiana}
\end{equation}

\begin{equation}
	\hat{\sigma}_{jl}^2
	= \frac{1}{|C_l|}
	\sum_{i : Y_i = C_l}
	\left( x_{ij} - \hat{\mu}_{jl} \right)^2.
	\label{eq:varianza_gaussiana}
\end{equation}



\section{Máquinas de Soporte Vectorial (SVM)}

\noindent
Las \textbf{Máquinas de Soporte Vectorial}~\cite{Cortes1995} (SVM, por sus siglas en inglés) constituyen una técnica de clasificación supervisada basada en la búsqueda de una \textbf{función de decisión} que permita predecir la clase de una observación a partir de sus atributos. Dado que este método opera sobre variables numéricas, los atributos categóricos deben codificarse previamente.\\

\noindent
El objetivo fundamental de una SVM es encontrar un \textbf{hiperplano de separación óptimo} que divida los datos en función de sus clases, \textbf{maximizando el margen} $\varphi$, es decir, la distancia mínima entre el hiperplano y los puntos más cercanos de cada clase (denominados \textbf{vectores de soporte}). Dichos vectores determinan la posición y orientación del hiperplano, por lo que son los únicos puntos relevantes en el entrenamiento del modelo.\\
%% AR: M se usó para métrica.
%% NS: Uso gamma 
%% AR: gamma se usa para los kernels. =(
%% NS: Listo uso varphi
Aunque existen infinitos hiperplanos que pueden separar los datos, el principio de las SVM consiste en seleccionar aquel que logre la \textbf{máxima separación posible entre las clases}, minimizando simultáneamente el riesgo de sobreajuste y aumentando la capacidad de generalización.\\


\noindent
El caso de \textbf{Margen Rígido (Hard Margin)} separa perfectamente las clases sin errores de clasificación. En este caso, el problema de optimización se formula como:

\begin{equation}
	\text{Minimizar }
	\frac{1}{2} |\bm{w}|^2
	\quad \text{sujeto a }
	y_i (\langle \bm{w}, \bm{x}_i \rangle + \beta) \ge 1 ,
	\label{eq:hard_margin}
\end{equation}

\noindent
donde 
$\bm{w}$ es el vector normal al hiperplano, 
$\beta$ es el término de sesgo, 
$\bm{x}_i$ es el $i$-ésimo registro del conjunto de datos, 
$y_i \in \{-1, +1\}$ es la etiqueta asociada a dicho registro,
y la restricción garantiza que todas las observaciones queden correctamente clasificadas y a una distancia mínima de 
$\frac{1}{\lVert \bm{w} \rVert}$ del hiperplano.\\

\noindent
Pero hay un gran obstáculo, debido a que en la práctica los datos rara vez son perfectamente separables. Por ello, se introduce el concepto de \textbf{Margen Suave (Soft Margin)}, que permite violaciones controladas de las restricciones mediante variables de holgura $ \xi_i \ge 0 $.
El problema se redefine como:

\begin{equation}
	\text{Minimizar } 
	\frac{1}{2} \lVert \bm{w} \rVert^2 +
	C \sum_{i=1}^{n} \xi_i
	\quad \text{sujeto a }
	\begin{cases}
		y_i(\langle \bm{w}, \bm{x}_i \rangle + \beta) \ge 1 - \xi_i, \\
		\xi_i \ge 0,
	\end{cases}
	\label{eq:soft_margin}
\end{equation}
donde el parámetro $C > 0$ actúa como un \textbf{control de regularización}: valores grandes de $C$ penalizan más fuertemente los errores, buscando una separación más estricta (a costa de menor margen), mientras que valores pequeños permiten más errores, favoreciendo márgenes amplios y mayor generalización.\\


\noindent
La \textbf{Formulación Dual} del problema, en lo que respecta a los dos margenes permite expresar la solución en términos de los productos internos entre las observaciones:

\begin{equation}
	\text{Maximizar }
	-\frac{1}{2} \sum_{i,\ell=1}^{n}
	\alpha_i \alpha_\ell y_i y_\ell
	K(\bm{x}_i, \bm{x}_\ell)
	+ \sum_{i=1}^{n} \alpha_i,
	\label{eq:dualsvm}
\end{equation}

\begin{equation}
	\text{sujeto a }
	0 \le \alpha_i \le C, \quad
	\sum_{i=1}^{n} \alpha_i y_i = 0.
\end{equation}

\noindent
Aquí, los \(\alpha_i\) son los multiplicadores de Lagrange asociados a las restricciones, y \(K(\bm{x}_i, \bm{x}_\ell)\) representa el producto interno entre las observaciones en un espacio transformado.\\


\noindent
En muchos casos, las clases no son separables linealmente en el espacio original de los datos. El \textbf{Kernel Trick} permite proyectar los datos a un espacio de mayor dimensión (posiblemente infinito) donde sí exista un hiperplano separador, \emph{sin necesidad de calcular explícitamente la transformación}.\\

\noindent
Esto se logra sustituyendo los productos internos \(\langle \bm{x}_i, \bm{x}_\ell \rangle\) por una función \textbf{kernel} \(K(\bm{x}_i, \bm{x}_\ell)\), que computa directamente el producto interno en el espacio transformado.
De esta forma, se mantiene la eficiencia computacional mientras se obtiene un modelo capaz de representar fronteras de decisión no lineales.

\subsubsection{Funciones Kernel Comunes}

\noindent
En las siguientes definiciones, los vectores 
$\bm{a}, \bm{b} \in \mathbb{R}^D$ representan puntos de datos en el 
espacio original de características. Cada kernel introduce parámetros 
que controlan la flexibilidad del modelo:

\begin{itemize}
	\item $\gamma > 0$: controla la influencia local de los puntos 
	(utilizado en el kernel RBF y el sigmoide).
	\item $r \in \mathbb{R}$: término independiente o sesgo 
	(usado en los kernels polinómico y sigmoide).
	\item $d' \in \mathbb{N}$: grado del polinomio 
	en el kernel polinómico.
\end{itemize}

\paragraph{Kernel Lineal:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\langle \bm{a}, \bm{b} \rangle.
	\label{eq:kernel_lineal}
\end{equation}

\paragraph{Kernel Radial (RBF o Gaussiano):}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\exp \left( -\gamma \|\bm{a} - \bm{b}\|^2 \right).
	\label{eq:kernel_rbf}
\end{equation}

\paragraph{Kernel Polinómico:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	(\langle \bm{a}, \bm{b} \rangle + r)^\textbf{$d$}.
	\label{eq:kernel_polinomico}
\end{equation}

\paragraph{Kernel Sigmoide:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\tanh(\gamma \langle \bm{a}, \bm{b} \rangle + r).
	\label{eq:kernel_sigmoid}
\end{equation}

\noindent
\textbf{Hiperparámetros}

\begin{itemize}
	\item \textbf{Parámetro de Regularización (\(C\)):}
	Controla el equilibrio entre maximizar el margen y permitir errores de clasificación. 
	Valores altos priorizan clasificar correctamente los datos de entrenamiento; valores bajos generan un margen más amplio y toleran mayor error, donde $C > 0$.
	
	\item \textbf{Tipo de Kernel (\texttt{kernel}):}
	Determina la forma de la frontera de decisión. 
	Scikit-learn permite: \texttt{linear}, \texttt{poly}, \texttt{rbf}, \texttt{sigmoid} (y \texttt{precomputed}).
	
	\item \textbf{Parámetros del Kernel:}
	Dependiendo del tipo elegido, scikit-learn utiliza:
	\begin{itemize}
		\item \(\gamma\): controla la influencia de los puntos en kernels \texttt{rbf}, \texttt{poly} y \texttt{sigmoid}, $\gamma > 0 $.  
		Es equivalente a \( \tfrac{1}{2\sigma^2} \) en formulaciones gaussianas, aunque scikit-learn nunca usa \(\sigma\) explícitamente.
		\item \(d\) (\texttt{degree}): grado del polinomio cuando el kernel es \texttt{poly}, $d \ge 1$ .
		\item \(r\) (\texttt{coef0}): coeficiente independiente usado por los kernels \texttt{poly} y \texttt{sigmoid}.
	\end{itemize}
\end{itemize}

	

\chapter{Métricas de Rendimiento Utilizadas}


\noindent
El objetivo de este trabajo es evaluar el desempeño del calificador de cada modelo de Aprendizaje Automático. Para alcanzarlo, se utilizan \textbf{métricas de rendimiento} que permiten cuantificar la capacidad del algoritmo de clasificar, ergo son herramienta que utilizamos para saber qué tan bien predice un modelo a una clase o qué tan bien puede clasificar entre varias clases.\\

\noindent
\textbf{La importancia de las métricas} se ubica en que el objetivo central de estos algoritmos no es simplemente obtener un buen rendimiento en los datos utilizados para construir el modelo, sino en su \textbf{capacidad de generalización}, su habilidad para funcionar correctamente con entradas nuevas y previamente no observadas (no utilizadas en el entrenamiento). Esto se debe a que es perfectamente normal y sumamente esperable que el modelo funcione correctamente con el conjunto de datos que se utiliza para el entrenamiento del modelo, la idea fundamental es poder tener el mismo rendimiento o incluso uno mejor que con el conjunto de entrenamiento.\\

\noindent
Para obtener las métricas y entrenar el algoritmo se utiliza la estrategia de \textbf{Validación Cruzada $K$-fold}. En este método, el conjunto de datos se divide en $K$ grupos $\{G_1, G_2, \dots, G_K\}$ (o \textit{pliegues}) de igual tamaño. En cada iteración, un grupo $G_i$ ($1 \le i \le K$) se utiliza como conjunto de prueba, mientras que los $K-1$ restantes se emplean para el entrenamiento. Cada grupo actúa como conjunto de prueba exactamente una vez.\\

\noindent
El valor final estimado de la métrica $M$, denotado por $\widehat{M}$, es el promedio de los valores obtenidos en cada iteración, es decir,
\begin{equation}
	\widehat{M} = \frac{1}{K} \sum_{i=1}^{K} M_i,
	\label{eq:kfolds}
\end{equation}
donde $M_i$ representa el valor de la métrica de evaluación cuando $G_i$ se utiliza como conjunto de prueba, para $i = 1, 2, \dots, K$.\\

\noindent
Dentro de este trabajo no sólo se evalúan distintos modelos, sino que se utilizan distintos \emph{datasets} para lograrlos. A continuación se señalan las métricas que se utilizan en cada caso y se pueden apreciar algunas diferencias, leves, pero diferencias en sí. 

\section{Métricas para el Caso Binario}

\noindent
\subsection*{\textbf{Matriz de Confusión}}

\noindent
Una matriz de confusión, que se puede observar en la Tabla~\ref{tab:matrizconfusionbinario}, es una forma simple de saber de qué forma está clasificando el algoritmo, donde una clase es considerada \textbf{positiva $P$} y la otra \textbf{negativa $N$}. La matriz de confusión clasifica las predicciones en:

\begin{itemize}
	\item \textbf{Verdaderos Positivos (TP):} Casos positivos clasificados correctamente.\\
	\item \textbf{Verdaderos Negativos (TN):} Casos negativos clasificados correctamente.\\
	\item \textbf{Falsos Positivos (FP):} Casos negativos clasificados incorrectamente como positivos.\\
	\item \textbf{Falsos Negativos (FN):} Casos positivos clasificados incorrectamente como positivos. \\
\end{itemize}


\begin{table}[htb]
    \centering
\caption{Matriz de confusión.}
    \label{tab:matrizconfusionbinario}
    \begin{tabular}{|c|c|c|c|}
		\cline{3-4}
		\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{\textbf{Predicción}} \\
		\cline{3-4}
		\multicolumn{2}{c|}{} & \textbf{Positivo} & \textbf{Negativo} \\
		\hline
		\multirow{2}{*}{\textbf{Verdad}} & \textbf{Positivo} & \cellcolor{TPcolor} \textbf{Verdadero Positivo (TP)} & \cellcolor{FNcolor} \textbf{Falso Negativo (FN)} \\
		\cline{2-4}
		& \textbf{Negativo} & \cellcolor{FNcolor} \textbf{Falso Positivo (FP)} & \cellcolor{TPcolor} \textbf{Verdadero Negativo (TN)} \\
		\hline
	\end{tabular}
\end{table}

\noindent
A partir de la matriz de confusión se calculan varias métricas, algunas de las cuales se presentan a continuación.


\noindent
\subsection*{\textbf{\textit{Accuracy}}}


\noindent
El \textit{Accuracy} es la proporción de instancias clasificadas correctamente, es una medida ``ingenua'' que puede ser engañosa si existe un gran desbalance entre clases, ergo se puede obtener un \textit{Accuracy} alto si se predice una clase muy bien, que tiene una distribución mucho mayor que la otra, mientras que la de menor aparición casi no la predice. En términos de la matriz de confusión,
\begin{equation}
    \text{\textit{Accuracy}} = \frac{TP + TN}{TP + FP + TN + FN} = \frac{TP + TN}{\text{Total}}.
    \label{eq:accurcybinario}
\end{equation}
\noindent
En términos del conjunto de predicciones y valores verdaderos, se tiene que $n$: representa la cantidad total de ejemplos en el \emph{dataset}, mientras que $\hat{y}_l$ es el valor predicho del $l$-ésimo ejemplo, e $y_l$ es el valor verdadero correspondiente, en cual caso si $\hat{y}_l$ es igual al valor obtenido. La expresión $ 1(\hat{y}_l = y_l)$ corresponde a la función indicadora, que toma el valor 1 si la predicción del ejemplo $l$ es correcta y 0 en caso contrario. Al sumar estos valores sobre todas las muestras, se obtiene el número total de aciertos del modelo.

\begin{equation}
    \texttt{\textit{Accuracy}}(y, \hat{y}) = \frac{1}{n} \sum_{l=0}^{n_-1} \mathbb{I}(\hat{y}_l = y_l),
    \label{eq:accuracybinarioformula}
\end{equation}
%% AR: Falta aclarar lo que representa 1(\hat{y}_l = y_l)
%% NS: Lo agregue al final del parrafo anterior.
%% AR: Antes se usó otra notación para la función indicadora. Hay que unificar.
%% NS: Listo.
\noindent
\noindent
donde la \textbf{función indicadora} se define como:
\[
\mathbb{I}(\hat{y}_l = y_l) =
\begin{cases}
	1, & \text{si } \hat{y}_l = y_l, \\
	0, & \text{en caso contrario}.
\end{cases}
\]\\
\noindent
Por lo tanto, se puede simplificar como la siguiente fórmula:

\begin{equation}
    \text{\textit{Accuracy}} = \frac{\text{Número de predicciones correctas}}{\text{Número total de muestras}}.
    \label{eq:accuracygeneral}
\end{equation}

\noindent
\subsection*{\textbf{\textit{Precision}}}


\noindent
La \textit{Precision} mide la probabilidad de que la predicción positiva del clasificador sea correcta, en otras palabras, mide qué tan bien predice las clases positivas el modelo.

\noindent
En términos de la matriz de Confusión, se puede expresar lo anterior de la siguiente manera:
\begin{equation}
    \text{\textit{Precision}} = \frac{TP}{TP + FP}.
    \label{eq:precisionbinario}
\end{equation}

\noindent
\textbf{\textit{Recall}}

\noindent
El \textit{Recall} o también conocido como Sensibilidad o Tasa de Verdaderos Positivos (TPR), mide la probabilidad de que el clasificador detecte un caso positivo cuando en verdad lo es.
\noindent
En términos de la Matriz de Confusión se puede entender al \textit{Recall} de la siguiente manera:

\begin{equation}
     \text{Recall} = \text{TPR} = \frac{TP}{TP + FN} = \frac{TP}{P}.
     \label{eq:recallbinario}
\end{equation}

\noindent
\subsection*{\textbf{\textit{F-measure}}}


\noindent
El \textit{F-measure} es la media armónica ponderada de \textit{precision} y \textit{recall}. La versión más común es el \textbf{F1-score}, donde el parámetro de ponderación $\beta$ es igual a 1, donde $\beta > 0$ controla la importancia relativa entre la precisión y el recall. Un clasificador perfecto tiene un valor $F1 = 1$.
\noindent
La fórmula general es:
\begin{equation}
    F_{\beta} = \frac{(1 + \beta^2) \, precision \times recall}{\beta^2 precision + recall},
    \label{eq:formulageneral}
\end{equation}

\noindent
donde la fórmula del F1-score ($\beta=1$) en términos de \textit{precision} y \textit{recall} se puede notar de la siguiente manera: 

\begin{equation}
    F1 = 2 \cdot \frac{precision \cdot recall}{precision + recall},
    \label{eq:f1general}
\end{equation}
\noindent
o en términos de la Matriz de Confusión, de forma más simplificada:
\begin{equation}
     F1 = \frac{2\,TP}{2\,TP + FP + FN}.
        \label{eq:f1enmatriz}
\end{equation}

\noindent
\subsection*{\textbf{\textit{Área Bajo la Curva ROC ($AUC$)}}}


\noindent
La métrica \textit{$AUC$} es un valor que resume la capacidad de un clasificador para distinguir entre clases, siendo una métrica muy útil para comparar el desempeño entre modelos distintos o entre un mismo modelo con hiperparámetros distintos.\\

\noindent
\textbf{La Curva ROC} es un gráfico que ilustra el rendimiento de un clasificador binario a media que se varía su umbral de discriminación. Se crea graficando la \textbf{Tasa de Verdaderos Positivos (TPR)} versus la \textbf{Tasa de Falsos Positivos (FPR)} en varios umbrales. El \textbf{$AUC$} mide justamente el área debajo de la Curva ROC.\\

\noindent
Ejes utlizados para el gráfico:
\begin{itemize}
\item [\textbf{Eje Y:}] TPR,

\item  [\textbf{Eje X:}] FPR.
\end{itemize}


\noindent
Un clasificador \textbf{ideal} se ubica en el punto $(0, 1)$, donde TPR=1 y FPR=0, lo que resulta en un \textbf{$AUC = 1$}. Un clasificador \textbf{aleatorio} se sitúa sobre la línea TPR = FPR, lo que resulta en un \textbf{$AUC = 0.5$}. Un clasificador se considera \textbf{razonable} si \textbf{ $0.5 < AUC \leq 1$}.


\noindent
\section{Métricas para caso Multiclase}

\noindent
En este caso se utiliza el método \textit{``weighted''}, el cual tiene en cuenta el desequilibrio de clases calculando el promedio de las métricas binarias por clase, ponderadas según su frecuencia en el conjunto de datos reales.\\

\noindent
Sea $\{1, 2, \dots, L\}$ el conjunto de etiquetas o clases. Para cada clase $l$, se define $|C_l|$ como la cantidad de muestras reales pertenecientes a dicha clase (las barras verticales representan la \textbf{cardinalidad} del conjunto de ejemplos con etiqueta $l$, es decir la cantidad de muestras que tiene esa clase). Asimismo, $M_l$ es el valor de la métrica binaria correspondiente a la clase $l$.\\

%% AR: Revisar notación para K-fold y corregir el párrafo anterior de manera análoga.
%% NS: Creo que ahí esta.
%% AR: Observar que el conjunto L se define con un elemento que también se llama L. Por otra parte, antes de usaba las clases como C_1, C_2, \dots, C_L, ¿por qué no seguir con la misma notación e indicar la cantidad de ejemplos en una clase por |C_l|?
%% NS: Tenes razon, ahi modifique todos los |y_l| por |C_l|


\noindent
La métrica ponderada, $\widehat{M}_{\text{weighted}}$, se define como:
\begin{equation}
	\widehat{M}_{\text{weighted}} =
	\frac{1}{\sum_{l \in L} |C_l|}
	\sum_{l \in L} |C_l| \cdot M_l.
	\label{eq:metricasponderadas}
\end{equation}

\noindent
De esta forma, las clases con mayor cantidad de muestras tienen una contribución proporcionalmente mayor en el valor final de la métrica evaluada.

\noindent
\subsection*{\textbf{Matriz de Confusión Multiclase}}


\noindent
La matriz de confusión multiclase, que se puede ver en la Tabla~\ref{tab:matrizmulticlase}, es una matriz cuadrada de tamaño $L \times L$, donde $L$ es el número de clases. Cada celda $C_{ij}$ representa la cantidad de muestras verdaderamente pertenecientes a la clase $i$ que fueron clasificadas como clase $j$ para $i,j \in \{ 1, 2, \dots, L\}$.\\

\noindent
Para cada clase $l$ se definen los valores que antes habíamos utilizados para la matriz de confusión del caso binario, en donde $Z$ es el total de instancias o la sumatoria de todas las celdas:
\begin{itemize}
\item $TP_l = C_{ll}$ 
\item $FP_l = \sum_{i \neq l}^{L} C_{il}$
\item $FN_l = \sum_{j \neq l}^{L}  C_{lj}$
\item $TN_l = Z - TP_l - FP_l - FN_l$
\end{itemize}

\begin{table}[htb]%[htb][]
	\centering
\caption{Matriz de confusión multiclase.}
	\label{tab:matrizmulticlase}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\cline{3-8}
		\multicolumn{2}{c|}{} & \multicolumn{6}{c|}{\textbf{Predicción}} \\
		\cline{3-8}
		\multicolumn{2}{c|}{} 
		& \textbf{Clase $C_1$} 
		& \textbf{Clase $C_2$} 
		& \textbf{$\cdots$} 
		& \textbf{Clase $C_l$}
		& \textbf{$\cdots$} 
		& \textbf{Clase $C_L$} \\
		\hline
		
		\multirow{5}{*}{\textbf{Verdad}} 
		
		% --- Fila C1 ---
		& \textbf{Clase $C_1$} 
		& \cellcolor{TPcolor} TN$_l$ 
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{FNcolor} FP$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{TPcolor} TN$_l$ \\ 
		\cline{2-8}
		
		% --- Fila C2 ---
		& \textbf{Clase $C_2$} 
		& \cellcolor{TPcolor} TN$_l$ 
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{FNcolor} FP$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{TPcolor} TN$_l$ \\
		\cline{2-8}
		
		% --- Filas intermedias ---
		& $\vdots$ 
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{FNcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$ \\
		\cline{2-8}
		
		% --- Fila Cl ---
		& \textbf{Clase $C_l$} 
		& \cellcolor{FNcolor} FN$_l$
		& \cellcolor{FNcolor} FN$_l$
		& \cellcolor{FNcolor} $\cdots$
		& \cellcolor{TPcolor} TP$_l$
		& \cellcolor{FNcolor} $\cdots$
		& \cellcolor{FNcolor} FN$_l$ \\
		\cline{2-8}
		
		% --- Filas intermedias ---
		& $\vdots$ 
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{FNcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$ \\
		\cline{2-8}
		
		% --- Fila CL ---
		& \textbf{Clase $C_L$}
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{FNcolor} FP$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{TPcolor} TN$_l$ \\
		\hline
	\end{tabular}
\end{table}


\noindent
\subsection*{\textbf{\textit{Precision}}}

\noindent
La \textit{Precision} por clase $l$ mide la proporción de muestras clasificadas como positivas que realmente pertenecen a la clase $l$, donde $l \in \{1, 2, \dots, L\}$. En términos más simple, utilizando la Matriz de Confusión:

\begin{equation}
    Precision_l = \frac{TP_l}{TP_l + FP_l}.
    \label{eq:precisionmulticlase}
\end{equation}

\noindent
\subsection*{\textit{Recall}}

\noindent
El \textit{Recall} por clase $l$, donde $l \in \{1, 2, \dots, L\}$, mide la proporción de muestras verdaderamente positivas de la clase $l$ que fueron correctamente identificadas. En términos más simple, utilizando la Matriz de Confusión:

\begin{equation}
   Recall_l = \frac{TP_l}{TP_l + FN_l}.
    \label{eq:recallmulticlase}
\end{equation}

\noindent
\subsection*{\textit{F1-measure}}

\noindent
El valor global ponderado se obtiene aplicando la fórmula de $M_{\text{weighted}}$ sobre los $F_{l}$, para $\beta = 1$:

\begin{equation}
    F_{\text{weighted}} = \sum_{l \in L} w_l \, F_{l}, \quad 
\text{con } w_l = \frac{|C_l|}{\sum\limits_{l=1}^{L} |C_l|}.
    \label{eq:f1formula}
\end{equation}

\noindent
\subsection*{\textit{Área Bajo la Curva ROC ($AUC$)}}
%% AR: Antes era $AUC$, elegir un estilo y unificar.
%% NS: Ok.
%% AR: Sigue habiendo diferencia.
%% NS: Ahí lo hice con buscar y reemplazar, debería de estar.

\noindent
Para extender la métrica $AUC$ a clasificación multiclase se emplea el enfoque \textbf{One-vs-Rest (OVR)}. Para cada clase $l$ (donde $l \in \{1, 2, \dots, L\}$), se considera la clase $l$ como positiva y el resto como negativas; luego, se calcula el valor correspondiente $\text{AUC}_l$ a partir de la curva ROC del problema binario generado para esa clase. Finalmente, el $AUC$ multiclase se obtiene como un promedio ponderado por el soporte de cada clase.


\begin{equation}
     \text{$AUC$}_{\text{OVR, weighted}} = \sum_{l \in L} w_l \, \text{$AUC$}_l \quad 
     \text{con } w_l = \frac{|C_l|}{\sum\limits_{l=1}^{L} |C_l|}.
     \label{eq:onevsrest}
\end{equation}
%{eq:onevsrest}{One-vs-Rest (OVR)}


\noindent
\subsection*{Importancia de la característica}


\noindent Sea un modelo predictivo $\mathcal{M}$ entrenado sobre un conjunto de datos $X$, y sea $M$ la métrica de referencia del modelo sobre los datos originales. El procedimiento se detalla a continuación:

\begin{enumerate}

	\item Para cada atributo $j$ del conjunto de datos:
	\begin{enumerate}
		\item Repetir el siguiente proceso $K$ veces (para reducir la varianza de la estimación):
		\begin{enumerate}
			\item Generar una versión alterada del conjunto de datos, $X^{(k,j)}$, en la que se permuta aleatoriamente, se intercambian de orden los datos de la columna correspondiente a la característica $j$, manteniendo las demás columnas sin cambios, para ver si el modelo empeora o no en su rendimiento.
			
			\item Calcular la métrica $M$ del modelo sobre los datos permutados:
			\begin{equation}
				M_{k,j} = \text{valor de la métrica $M$ del modelo $\mathcal{M}$ con $X^{(k,j)}$}.
				\label{eq:puntaje_referencia_datos_permutados}
			\end{equation}

		\end{enumerate}
		\item Calcular la importancia de la característica $j$ como la disminución promedio en el puntaje del modelo respecto del puntaje de referencia:
		\begin{equation}
			I_j = M - \frac{1}{K} \sum_{k=1}^{K} M_{k,j}.
			\label{eq:puntaje_referencia_final}
		\end{equation}

	\end{enumerate}
\end{enumerate}


\noindent De esta manera, $I_j$ mide la pérdida de desempeño al romper la relación entre la característica $j$ y la variable objetivo. Valores más altos de $I_j$ indican características más relevantes para el modelo.\\





\chapter{Resultados}
\label{ch:resultados}

\noindent
En este capítulo se presentan los resultados obtenidos mediante la aplicación de los modelos de aprendizaje supervisado sobre los distintos conjuntos de datos. Se analizan las métricas de evaluación alcanzadas, las configuraciones óptimas halladas mediante búsqueda en malla (\emph{Grid Search}) y la importancia relativa de las características más influyentes en las predicciones.
Se busca evaluar el rendimiento de cada modelo bajo diferentes configuraciones de hiperparámetros, a través de métricas como la precisión (\textit{Accuracy}), el F1-Score y el área bajo la curva ROC ($AUC$), entre otras.\\

\noindent
Durante la fase experimental se exploraron distintas estrategias de balanceo de clases, dado que varios de los conjuntos presentan desbalances significativos entre las clases. En particular, se probó la técnica de \textbf{undersampling}, reduciendo la cantidad de ejemplos de la clase mayoritaria para equilibrar el \emph{dataset}. Sin embargo, esta estrategia no arrojó resultados satisfactorios: los modelos tienden a perder capacidad de generalización, mostrando un descenso notable en las métricas de validación, aunque se sostiene la mejor configuración con mejor valor de métricas. Por este motivo, se opta finalmente por mantener la distribución original y aplicar técnicas de regularización y ajuste de hiperparámetros para mitigar el sesgo hacia la clase dominante.\\


\section{Dataset Binario}

\noindent
En esta sección se presentan los resultados obtenidos al aplicar los distintos modelos de aprendizaje automático al \textbf{dataset binario}. En este caso, la tarea de clasificación consiste en distinguir entre dos clases posibles, lo que típicamente permite a los algoritmos aprender fronteras de decisión más simples en comparación con escenarios multiclase.


%% AR: Agregar una frase introductoria a la sección.
%% NS: Listo.
\subsubsection*{Regresión Logística}

\noindent
\textbf{RL} obtuvo un correcto desempeño en la clasificación binaria, siempre tomando un \textbf{buen resultado} cuando cualquier métrica por lo menos supere el valor de 0.80.\\

\noindent
Los resultados cuantitativos se resumen en la Tabla~\ref{tab:rl_resultados_binario}, en la cual se muestra la mejor configuración obtenida de los hiperparámetros de la Tabla~\ref{tab:grid_rl_bin}. 
El modelo logra un \emph{Accuracy}
y un F1-Score de 0.8445,
junto con un $AUC$ de 0.9050, lo que refleja una buena capacidad discriminatoria, pero siendo datos médicos se requieren incluso mejores valores.\\

\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros - Regresión Logística (binario).}
	\label{tab:grid_rl_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0, 0.1, 0.01] \\
		Penalty & [None, l1, l2, elasticnet] \\
		Solver & [lbfgs, saga, newton-s] \\
		Multiclass & [ovr, multinomial] \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del modelo de Regresión Logística.}
	\label{tab:rl_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Penalty = l1\\
			Solver = saga \\
			Multiclass = ovr
		} & 0.8485 & 0.8485 & 0.8481 & 0.9050 & 0.13 & 0.8495 \\
		\bottomrule
	\end{tabular}
\end{table}


\noindent 
El tiempo de entrenamiento fue de apenas 0.13 segundos, lo que lo convierte en una opción eficiente para este tipo de problema, teniendo en cuenta el tamaño pequeño del \emph{dataset}.



\subsubsection*{Máquinas de Soporte Vectorial}

\noindent
\textbf{SVM} obtuvo un desempeño sólido en la clasificación binaria, mostrando un equilibrio adecuado entre precisión y generalización. Demostrando la solidez habitual con que es descripto este método.\\

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:svm_resultados_binario}. 
El modelo logra un \emph{Accuracy} y un F1-Score de 0.8616, junto con un $AUC$ de 0.9232, lo que refleja la mejor capacidad de clasificación hasta ahora.\\

\noindent
El tiempo de entrenamiento fue de apenas 0.60 segundos, siendo un tiempo acotado, pero costoso si vemos que el tamaño del \emph{dataset} es bastante pequeño, refleja un costo de evaluación más grande que con RL.\\



\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros - SVM (binario).}
	\label{tab:grid_svm_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0.001, 0.01, 0.1, 1, 10, 15, 20, 25] \\
		Kernel & [linear, poly, rbf, sigmoid] \\
		Gamma & [scale, auto, 0.001, 0.01, 0.1, 1] \\
		Degree & [2--10] \\
		\bottomrule
	\end{tabular}
\end{table}



\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del SVM.}
	\label{tab:svm_resultados_binario}
	\begin{tabular}{lcccccc}
	\toprule
	\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Kernel = rbf\\
			Gamma = 0.1\\
		} & 0.8616 & 0.8616 & 0.8609 &0.9232 &0.60 & 0.8628 \\
		\bottomrule
	\end{tabular}
\end{table}







\subsubsection*{Naive Bayes}



\noindent
\textbf{NB} obtuvo un desempeño sólido en la tarea de clasificación binaria, especialmente considerando su supuesto fuerte de independencia condicional entre atributos.Si bien este supuesto rara vez se cumple estrictamente en datos reales, el modelo suele mantener un rendimiento competitivo.
En este trabajo, evaluamos su desempeño utilizando la métricas distitas métricas, y consideramos que un valor superior a 0.80 constituye un nivel de desempeño adecuado para el tipo de problema abordado.\\
%% AR: No estiendo bien la oración anterior.
%% NS: Ahí la corregí un poco más.

\noindent
La mejor configuración se alcanzó con cualquier suavizado, no hubo diferencias. Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:nbg_resultados_binario}.  El modelo logra un \emph{Accuracy} y un F1-Score de 0.8420, junto con un $AUC$ de 0.9138, lo que refleja una muy buena capacidad clasificadora siendo un método tan eficaz, además de ser bastante similar con los resultados de los modelos a los cuales se compara. \\


\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros - Naive Bayes Gaussiano (binario).}
	\label{tab:grid_nb_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Suavizado & $\{10^{-9},\; 10^{-8},\; 10^{-7},\; 10^{-6},\; 10^{-5}\}$
%% AR: Esto no es muy preciso.
%% NS: Listo.
 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del Naive Bayes Gaussiano.}
	\label{tab:nbg_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			var\_smoothing =\\
			 Cualquiera\\
		} & 0.8420 & 0.8420 & 0.8420 & 0.9138 & 0.10 & 0.8433 \\
		\bottomrule
	\end{tabular}
\end{table}



\noindent 
El tiempo de entrenamiento fue de apenas 0.10 segundos, siendo que la mayor fortaleza es la rapidez de su entrenamiento.




\subsubsection*{Random Forest}

\noindent
\textbf{RF} demostró un desempeño bastante sólido en la clasificación binaria, mostrando alta capacidad de generalización para registros no vistos durante el entrenamiento.\\

\noindent 
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rf_resultados_binario}, donde el modelo logra un \emph{Accuracy} y un F1-Score de 0.8780, junto con un $AUC$ de 0.9315, lo que refleja una excelente capacidad de clasificación, pero evidentemente la tarea de creación de los árboles y del voto mayoritario lo vuelven un método bastante costoso aunque efectivo.\\

\noindent 
El tiempo de entrenamiento fue de 1.38 segundos, siendo considerablemente el modelo en el cual más se tarda en obtener los resultados.\\


\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros - Random Forest (binario).}
	\label{tab:grid_rf_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Criterion & [gini, entropy] \\
		Max Depth & [None, 3, 5, 7, 9] \\
		Min Samples Split & [2, 5, 10] \\
		Min Samples Leaf & [1, 2, 4] \\
		Max Features & [None, sqrt, log2] \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del Random Forest.}
	\label{tab:rf_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Criterion = entropy\\
			Max Depth = 7\\
			Min Samples Split = 5\\
			Min samples Leaf = 1\\
			Max Features = sqrt\\
		} & 0.8780 & 0.8780 & 0.8775 & 0.9315 & 1.38 & 0.8731 \\
		\bottomrule
	\end{tabular}
\end{table}





\begin{figure}[htb]%[H]
	\centering
	\includegraphics[width=1\textwidth]{../mejores_modelos_barras_binario}
	\caption{Comparación de desempeño de los mejores modelos (Binario).}
	\label{fig:mejores_modelos_binario}
\end{figure}
%% AR: Cambiar "metricas" por "Métricas" (revisar el resto del documento).

\noindent
En la Figura \ref{fig:mejores_modelos_binario} se observa que Random Forest obtiene la mayor puntuación en todas las métricas, seguido por la Regresión Logística y SVM. Esto indica que, para nuestro \emph{dataset}, el modelo RF presenta mejor capacidad de generalización, mientras que los demás modelos muestran un desempeño bastante competitivo. Esto no quiere decir que sea el mejor modelo, sino que es el que mejor clasifica este \emph{dataset}, pero si se quieren resultados sólidos con menor tiempo de cálculo, entonces el NB es una buena solución. Podemos afirmar que todos los modelos tuvieron desempeños correctos.

\subsubsection*{Importancia de las Características}


\noindent La importancia de las características se calculó mediante el método de \textit{Permutation Feature Importance}. Este método evalúa cuánto se degrada el desempeño del modelo cuando se altera aleatoriamente una característica, manteniendo fijas las demás. Cuanto mayor sea la disminución en la métrica de desempeño, mayor será la importancia atribuida a dicha característica.\\


\noindent Los resultados obtenidos se presentan en las Tablas~\ref{tab:rl_importancia_},~\ref{tab:svm_importancia},~\ref{tab:nb_importancia} y \ref{tab:rf_importancia}, correspondientes a los modelos  RL, NB, SVM y RF, respectivamente.
%% AR: Habría que respetar el mismo orden previo para los métodos: RL, SVM, NB, RF. Lo mismo aplica para el caso multiclase.
%% NS: Listo ya lo hice.





%% AR: Ver que esta tabla queda cortada en el texto.
%% NS: Ahora esta bien, raro.
\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (RL).}
	\label{tab:rl_importancia_}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ST\_Slope & 0.072440 \\
ExerciseAngina & 0.026797 \\
ChestPainType   &  0.020806\\
Sex           &    0.011329\\
FastingBS      &   0.010240\\
Cholesterol    &   0.009150\\
Oldpeak        &   0.006100\\
Age            &   0.003922\\
MaxHR          &   0.003268\\
RestingBP      &   0.000545\\
RestingECG     &   0.000218\\
		\bottomrule
	\end{tabular}
\end{table}



\begin{table}[H]%[H]
	\centering
	\caption{Importancia de las características según permutación (SVM).}
	\label{tab:svm_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ST\_Slope     &     0.106209\\
Cholesterol    &   0.031808\\
Oldpeak        &   0.024510\\
ChestPainType  &   0.023312\\
Sex            &   0.011438\\
MaxHR           &  0.008715\\
ExerciseAngina  &  0.008388\\
Age            &   0.007952\\
RestingBP       &  0.005773\\
RestingECG      &  0.004902\\
FastingBS      &   0.004575\\
		\bottomrule
	\end{tabular}
\end{table}



%% AR: Revisar porque no puede aparecer la Tabla 4.12 antes que la 4.9.
%% NS: Modifique con H la anterior, es por como funciona htb, que quedaba la anerior.
\begin{table}[htb]
	\centering
	\caption{Importancia de las características según permutación (NB).}
	\label{tab:nb_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ST\_Slope & 0.027015 \\
ExerciseAngina & 0.023747 \\
Oldpeak & 0.018736 \\
ChestPainType & 0.018519 \\
Cholesterol & 0.014815 \\
Sex & 0.014270 \\
FastingBS & 0.004575 \\
RestingBP & 0.001852 \\
MaxHR & -0.000218 \\
RestingECG & -0.001198 \\
Age & -0.003595 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (RF).}
	\label{tab:rf_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ST\_Slope & 0.254265 \\
		ChestPainType & 0.127319 \\
		Oldpeak & 0.113156 \\
		ExerciseAngina & 0.105952 \\
		Cholesterol & 0.099872 \\
		MaxHR & 0.088635 \\
		Age & 0.065807 \\
		RestingBP & 0.055053 \\
		Sex & 0.040916 \\
		FastingBS & 0.030069 \\
		RestingECG & 0.018956 \\
		\bottomrule
	\end{tabular}
\end{table}



\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_binario}
	\caption{Evolución de \textit{Accuracy} (Binario).}
	\label{fig:evolucion_metricas_binario}
\end{figure}
%% AR: Sacar título porque hay un caption. (Revisar en el tresto del documento.)

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_binario_auc}
	\caption{Evolución de $AUC$ (Binario).}
	\label{fig:evolucion_metricas_binario_auc}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_binario_f1}
	\caption{Evolución de F1-Score (Binario).}
	\label{fig:evolucion_metricas_binario_f1}
\end{figure}



\noindent
La Figuras \ref{fig:evolucion_metricas_binario}, \ref{fig:evolucion_metricas_binario_auc} y \ref{fig:evolucion_metricas_binario_f1}  muestran cómo las métricas del modelo mejoran al incorporar las características más relevantes según su importancia. Se observa que inicialmente, con pocas características, el desempeño es limitado, siendo prudente incrementar la cantidad de características. En algunos casos es muy sorprendentemente, ya que se obtiene un buen resultado, y a medida que se agregan las variables de mayor relevancia, las métricas tienden a incrementarse hasta estabilizarse. Este comportamiento permite identificar el conjunto de características que maximiza el rendimiento sin necesidad de incluir todas las variables disponibles, optimizando tanto la complejidad del modelo como el tiempo de entrenamiento.\\

\noindent
Siendo los modelos \textbf{SVM} y \textbf{RF} los que mejor comportamiento tienen a lo largo del incremento de características, donde podemos ver en su gran mayoría un incremento del valor de las métricas a medida que se aumentan las caracterizaras. Además, como se puede observar, tienen valores aceptables para pocas características.


\section{Dataset Multiclase}


\noindent
En esta sección se presentan los resultados obtenidos al aplicar los distintos modelos de aprendizaje automático al \textbf{dataset multiclase}. A diferencia del problema binario, este conjunto de datos requiere que los algoritmos distingan entre varias categorías posibles, lo cual introduce desafíos adicionales como la separación más compleja entre clases, mayor variabilidad interna y posibles desbalances entre categorías. 

%% AR: Agregar frase que introduzca la sección.
%% NS: Listo.

%% AR: Revisar cómo modifiqué título y siglas en en caso binario y modificar unificando estilo.
%% NS: Listo quedo más bonito.
\subsubsection*{Regresión Logística}

\noindent
\textbf{RL} obtuvo un muy buen desempeño en la clasificación multiclase, mostrando un equilibrio adecuado entre precisión y generalización.\\

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rl_resultados}, junto con la configuración de hiperparámetros que logro esos resultados. 
El modelo logra un \emph{Accuracy} y un F1-Score de 0.8978 respectivamente, junto con un $AUC$ de 0.9644, lo que refleja una buena capacidad discriminatoria, logrando un buen resultado al incremento de datos y de dificultad al existir más de dos clases.\\

\noindent 
El tiempo de entrenamiento fue de apenas 0.30 segundos, lo que lo convierte en una opción eficiente para este tipo de problema, teniendo en cuanta que se mantiene en rangos acotados, incluso con más datos que en el caso binario.


\begin{table}[H]
	\centering
	\caption{Grilla de hiperparámetros - Regresión Logística (multiclase).}
	\label{tab:grid_rl_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0.01, 0.1, 1, 10] \\
		Penalty & [None, l2, elasticnet] \\
		Solver & [lbfgs, saga, newton-s] \\
		Multiclass & [ovr] \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption{Resultados finales del modelo de Regresión Logística.}
	\label{tab:rl_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Penalty = None\\
			Solver = newton-cg\\
			Multiclass = ovr
		} & 0.8978 & 0.8978 & 0.8953 & 0.9644 & 0.30 & 0.8945 \\
		\bottomrule
	\end{tabular}
\end{table}






\subsubsection*{Máquinas de Soporte Vecotorial}

\noindent
\textbf{SVM} obtuvo un desempeño bastante sólido en la clasificación multiclase, mostrando unos grandes valores en sus métricas. \\

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:svm_resultados}, junto con la mejor configuración que logro las mejoras métricas. 
El modelo logra un \emph{Accuracy} y un F1-Score de 0.9082, junto con un $AUC$ de 0.9556, lo que refleja una muy buena capacidad de clasificación, logrando ser un método útil para \emph{datasets} acotados y grandes, como a casos binarios y multiclase. 
El tiempo de entrenamiento fue de apenas 1.42 segundos, mostrando una clara demora a cuantos más datos deben de procesar, si comparamos como sube el tiempo de ejecución con el caso binario.


\begin{table}[H]
	\centering
	\caption{Grilla de hiperparámetros - SVM (multiclase).}
	\label{tab:grid_svm_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0.001, 0.01, 0.1, 1, 10, 15, 20, 25] \\
		Kernel & [linear, poly, rbf, sigmoid] \\
		Gamma & [scale, auto, 0.001, 0.01, 0.1, 1] \\
		Degree & [2--10] \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[H]
	\centering
	\caption{Resultados finales del SVM.}
	\label{tab:svm_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
		$C = 0.1$\\
			Kernel = poly\\
			Gamma = 1\\
			Degree = 2\\
		} & 0.9082 & 0.9082 & 0.9064 & 0.9556 & 1.45 & 0.9057 \\
		\bottomrule
	\end{tabular}
\end{table}









\subsubsection*{Naive Bayes}

\noindent
\textbf{NB} obtuvo un buen desempeño en la clasificación multiclase, considerando su supuesto de independencia entre atributos. \\

\begin{table}[H]
	\centering
	\caption{Grilla de hiperparámetros - Naive Bayes Gaussiano (multiclase).}
	\label{tab:grid_nb_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Suavizado & $\{10^{-9},\; 10^{-8},\; 10^{-7},\; 10^{-6},\; 10^{-5}\}$\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[H]
	\centering
	\caption{Resultados finales del Naive Bayes Gaussiano.}
	\label{tab:nbg_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			var\_smoothing =\\
			 Cualquiera\\
		} & 0.8208 & 0.8208 & 0.8366 & 0.9112 & 0.11 & 0.8747 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:nbg_resultados}. 
El modelo logra un \emph{Accuracy} y un F1-Score de 0.8208, junto con un $AUC$ de 0.91, lo que refleja una buena capacidad clasificadora, pero lo suficientemente menor a los modelos con los cuales se compara, para decidir que no es útil en cuanto a tener buenos valores de métricas a la hora de resolver este problema. Es evidente que se mantuvo en valores similares al caso binario, sin tener caídas aún cuando se incrementa el tamaño del \emph{dataset} o se dificulta el mismo con un caso multiclase, pero tampoco muestra mejora como el resto de modelos.\\

\noindent
El tiempo de entrenamiento fue de apenas 0.11 segundos, siendo su mayor fortaleza, incluso con más datos el modelo se mantiene en valores parecidos de tiempo de ejecución.





\subsubsection*{Random Forest}

\noindent
\textbf{RF} demostró un desempeño claramente superador en la clasificación multiclase, mostrando alta capacidad de generalización incluso para un \emph{dataset} más grande y con caso multiclase.\\

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rf_resultados}, junto con la mejor configuración con la cual se obtuvieron estos resultados. 
El modelo logra un \emph{Accuracy} y un F1-Score de 0.9432, junto con un $AUC$ de 0.9868, lo que refleja una buena excelente capacidad clasificadora. Se entienden que los árboles de decisión no se ven afectados por casos multiclase y que la presencia de más cantidad de registros los enriquecen para realizar mejor clasificaciones. 
El tiempo de entrenamiento fue de 2.24 segundos, mostrando como estos modelos incrementan su tiempo de ejecución a mayores datos requieren procesar.


\begin{table}[htb]
	\centering
	\caption{Grilla de hiperparámetros - Random Forest (multiclase).}
	\label{tab:grid_rf_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Criterion & [gini, entropy] \\
		Max Depth & [None, 3, 5, 7, 9] \\
		Min Samples Split & [2, 5, 10] \\
		Min Samples Leaf & [1, 2, 4] \\
		Max Features & [None, sqrt, log2] \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption{Resultados finales del Random Forest.}
	\label{tab:rf_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & \textbf{$AUC$} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Criterion = gini\\
			Max Depth = None\\
			Min Samples Split = 2\\
			Min samples Leaf = 1\\
			Max Features = sqrt\\
		} & 0.9432 & 0.9432 &0.9412 & 0.9868 & 2.24& 0.9417\\
		\bottomrule
	\end{tabular}
\end{table}





\FloatBarrier
\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../mejores_modelos_barras_multiclase}
	\caption{Comparación de desempeño de los mejores modelos (Multiclase).}
	\label{fig:mejores_modelos_multiclase}
\end{figure}
\FloatBarrier
\noindent
En la Figura \ref{fig:mejores_modelos_multiclase} se observa que Random Forest obtiene la mayor puntuación en todas las métricas, seguido por la Regresión Logística y SVM. Esto indica que, para nuestro \emph{datasets}, el modelo de Random Forest presenta mejor capacidad de generalización, mientras que los demás modelos muestran un desempeño bastante competitivo.


\subsubsection*{Importancia de las Características}

\noindent Los resultados obtenidos se presentan en las Tablas~\ref{tab:rf_importancia_multiclase},~\ref{tab:rl_importancia_multiclase},~\ref{tab:nb_importancia_multiclase} y ~\ref{tab:svm_importancia_multiclase} correspondientes a los modelos RF, RL, NB y SVM.



\begin{table}[htb]%[htb]
	\centering
	\caption{Importancia de las características según permutación (RL).}
	\label{tab:rl_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
Mean    &    0.098487\\
AC       &   0.084113\\
ASTV     &   0.057069\\
Median   &   0.031631\\
DP       &   0.029740\\
LB       &   0.023404\\
Variance &   0.022270\\
UC       &   0.022080\\
ALTV     &   0.019243\\
Max      &   0.018109\\
Nmax     &   0.014374\\
Mode     &   0.011348\\
Min      &   0.005910\\
MSTV     &   0.004208\\
FM       &   0.003830\\
Tendency &   0.003546\\
MLTV     &   0.002979\\
Nzeros   &   0.002837\\
DL       &   0.001655\\
Width    &   0.000189\\
DS       &   0.000000\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (SVM).}
	\label{tab:svm_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ASTV     &   0.050355\\
ALTV     &   0.037069\\
UC       &   0.030638\\
AC       &   0.026903\\
DP       &   0.018345\\
Mean     &   0.015887\\
Mode     &   0.014988\\
Median   &   0.014043\\
Nmax     &   0.011915\\
MSTV     &   0.009125\\
DL       &   0.005059\\
Variance &   0.004775\\
Nzeros   &   0.004586\\
Min      &   0.004444\\
Max      &   0.004350\\
MLTV     &   0.003357\\
Tendency &   0.003026\\
FM       &   0.002459\\
Width    &   0.001418\\
DS       &   0.000000\\
LB       &  -0.000804\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (Naive Bayes Gaussiano).}
	\label{tab:nb_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		AC       & 0.057163 \\
		DP       & 0.018676 \\
		ALTV     & 0.015461 \\
		ASTV     & 0.005106 \\
		DS       & 0.002695 \\
		UC       & 0.002364 \\
		FM       & 0.001371 \\
		Variance & 0.001087 \\
		Nzeros   & 0.001040 \\
		Nmax     & -0.000993 \\
		Tendency & -0.001040 \\
		Mode     & -0.001324 \\
		Max      & -0.001371 \\
		Min      & -0.001986 \\
		LB       & -0.001986 \\
		MLTV     & -0.002222 \\
		Width    & -0.002459 \\
		Median   & -0.003310 \\
		MSTV     & -0.004965 \\
		Mean     & -0.005768 \\
		DL       & -0.006809 \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]
	\centering
	\caption{Importancia de las características según permutación (RF).}
	\label{tab:rf_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ASTV      &  0.139807\\
		ALTV      &  0.109941\\
		MSTV      &  0.104823\\
		Mean      &  0.091579\\
		AC        &  0.063645\\
		Mode       & 0.061986\\
		Median      &0.060633\\
		DP        &  0.047945\\
		LB        &  0.045324\\
		MLTV      &  0.045132\\
		Variance  &  0.040531\\
		UC        &  0.039166\\
		Width     &  0.030551\\
		Min       &  0.030109\\
		Max       &  0.027147\\
		FM        &  0.020801\\
		Nmax      &  0.018407\\
		DL        &  0.011128\\
		Tendency  &  0.007652\\
		Nzeros    &  0.003405\\
		DS        &  0.000287\\
		\bottomrule
	\end{tabular}
\end{table}



\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_multiclase}
	\caption{Evolución de \textit{Accuracy} (Multiclase).}
	\label{fig:evolucion_metricas_multiclase}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_multiclase_auc}
	\caption{Evolución de $AUC$ (Multiclase).}
	\label{fig:evolucion_metricas_multiclase_auc}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_multiclase_F1}
	\caption{Evolución de F1-score (Multiclase).}
	\label{fig:evolucion_metricas_multiclase_f1}
\end{figure}


\noindent
Las Figuras \ref{fig:evolucion_metricas_multiclase}, \ref{fig:evolucion_metricas_multiclase_f1} y \ref{fig:evolucion_metricas_multiclase_auc} muestran cómo las métricas del modelo mejoran al incorporar las características más relevantes según su importancia. Se observa que inicialmente, con pocas características, el desempeño es menor al obtenido anteriormente, pero mucho mejor del esperable. En algunos casos, de forma muy sorprendentemente, se obtiene un buen resultado ya a los pocos atributos utilizados, y a medida que se agregan las variables de mayor relevancia, las métrica tienden a incrementarse hasta estabilizarse.\\

\noindent
Este comportamiento permite identificar el conjunto de características que maximiza el rendimiento sin necesidad de incluir todas las variables disponibles, optimizando tanto la complejidad del modelo como el tiempo de entrenamiento. Se ven como \textbf{RF} muestra resultados muy prometedores incluso con pocas métricas, siendo claramente superior en los valores de métricas incluso con pocos atributos.





\chapter{Conclusiones}

\section{Análisis General e Inferencias}

\noindent
Del análisis de los resultados obtenidos se puede observar que el modelo \textbf{Random Forest} alcanzó reiteradamente los mejores valores en todas las métricas, tanto en el problema binario como en el multiclase. Esto se debe a su capacidad de combinar múltiples árboles de decisión, lo que permite capturar relaciones no lineales y reducir el sobreajuste, sobretodo a la hora de hacer un voto mayoritario de estos mismos árboles que permiten una representación mejor.\\

\noindent
El modelo de \textbf{Máquinas de Soporte Vectorial} mostró también un rendimiento muy bueno, especialmente con el kernel \textbf{RBF} en el caso binario y el \textbf{polinómico} en el caso multiclase, destacándose su capacidad para definir fronteras de decisión complejas en espacios transformados.\\

\noindent
\textbf{Regresión Logística} presentó resultados positivos y de buena generalización, aunque con menor capacidad para capturar patrones que los otros modelos, no por ser malos resultados, sino que el resto tuvo mejores valores de métricas. Por su parte, el modelo \textbf{Naive Bayes} ofreció un rendimiento aceptable, siendo el más liviano computacionalmente, aunque con limitaciones inherentes a su supuesto de independencia de las variables. Esto no quita que aunque posea este supuesto, es el más liviano y rápido de los modelos obteniendo resultados sumamente buenos.\\

\noindent
En cuanto a la \textbf{importancia de las características}, se identificaron atributos dominantes en cada conjunto de datos:\\

\noindent
En el caso binario, variables como \textit{ST\_Slope}: este segmento del ECG se analiza porque cambia cuando hay isquemia, es decir, cuando el corazón recibe menos sangre de la necesaria; \textit{ChestPainType}: el tipo de dolor en el pecho, asociado a isquemia cardíaca; y \textit{Oldpeak}: componente utilizado para estimar el riesgo de isquemia o infarto de miocardio, fueron recurrentemente relevantes.\\

\noindent
En el caso multiclase destacaron \textit{ASTV}: porcentaje de tiempo con variabilidad anormal a corto plazo; \textit{ALTV}: porcentaje de tiempo con variabilidad anormal a largo plazo; y \textit{MSTV}: valor medio de la variabilidad a largo plazo.
%% AR: Pondría las variables de modo que sea comprensible y no haya que ir a buscar lo que significan. Esto es porque son las conclusiones. 
%% NS: Listo.

\section{Mejoras Potenciales y Consideraciones}

\noindent
Para optimizar aún más las métricas, podrían explorarse las siguientes estrategias:

\begin{itemize}
	\item \textbf{Ajuste más fino de hiperparámetros:} empleando \emph{Randomized Search} o \emph{Bayesian Optimization} para reducir tiempos de búsqueda, y luego utlizar un \emph{Grid Search} en los hiperparámetros encontrados.
	\item \textbf{Manipulación de características:} Reducción de dimensionalidad (PCA) o creación de variables sintéticas, para observar mejor las importancias de cada característica.
	\item \textbf{Validación cruzada más robusta:} utilizando más particiones para estimar mejor la generalización.
\end{itemize}

En conjunto, los modelos demostraron un desempeño satisfactorio, con un claro potencial de mejora mediante el refinamiento de hiperparámetros y una mejor comprensión de la estructura de los datos.

\bibliographystyle{plain}
\bibliography{Referencias}

\end{document}