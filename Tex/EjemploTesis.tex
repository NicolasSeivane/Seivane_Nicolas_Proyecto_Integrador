\documentclass[a4paper,10pt]{book}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}
%\usepackage{mathtools}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[spanish]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{incgraph,tikz}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
%\usepackage{babel}
\usepackage{color}
\usepackage{listings}
\usepackage{float}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{titlepic}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{etoolbox}
%\usepackage{cite}    
\usepackage{bm}
\usepackage{makecell}
\usepackage{placeins}

\usepackage{listings}
\lstset{ %
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=none,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=10pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=4,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}



\usetikzlibrary{arrows}

\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}

\renewcommand{\contentsname}{\'Indice General}
\renewcommand{\listfigurename}{\'Indice de Figuras}
\renewcommand{\listtablename}{\'Indice de Tablas}
\renewcommand{\lstlistingname}{Salida}
\renewcommand\tablename{Tabla}
\renewcommand{\figurename}{Figura}
\renewcommand\thesubfigure{(\alph{subfigure})}
\renewcommand{\baselinestretch}{1.2} 

\makeatletter
\def\verbatim{\small\@verbatim \frenchspacing\@vobeyspaces \@xverbatim}
\makeatother

\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\fancyhead[LE]{\nouppercase{\leftmark}}
\fancyhead[RO]{\nouppercase{\leftmark}}

\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0.4pt}



%\restylefloat{table}


\definecolor{TPcolor}{HTML}{A9D18E} % Verde claro para TP
\definecolor{TNcolor}{HTML}{D9EAD3} % Verde muy claro para TN
\definecolor{FPcolor}{HTML}{F4CCCC} % Rojo muy claro para FP
\definecolor{FNcolor}{HTML}{E06666} % Rojo claro para FN

\title{Comparación de Técnicas de Aprendizaje
	Automático Supervisado Aplicadas a Datos Cardiólogos}
\author{Autor: Nicolás Seivane \\ Tutora: Andrea Rey}
\date{\\ Universidad Nacional de Hurlingham (UNAHUR)}
\titlepic{\vspace{12cm}\includegraphics[width=0.15\textwidth]{logo.jpg}}

\usepackage{titlesec}

% Chapters justificados
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\justifying} % estilo del capítulo
{\chaptername\ \thechapter}{20pt}{\Huge}

% Sections justificados
\titleformat{\section}
{\normalfont\Large\bfseries\justifying} % estilo de la sección
{\thesection}{1em}{}

% Subsections justificados
\titleformat{\subsection}
{\normalfont\large\bfseries\justifying}
{\thesubsection}{1em}{}



\begin{document}



\maketitle


{\justifying
	\tableofcontents
	\listoffigures
	\listoftables
}



\chapter*{Resumen}

\chapter{Introducción}



Las enfermedades cardiovasculares son la causa número uno de muerte globalmente, con un estimado de 17.9 millones de vidas cada año, aproximadamente 31\% de todas las muertes globales. La idea central de este trabajo es encontrar la técnica de aprendizaje automático mas óptima para poder realizar predicciones de si un paciente tiene altas probabilidades de tener insuficiencia cardíaca.\\

\noindent
La cardiotocografía (CTG) es un registro continuo de la frecuencia cardíaca fetal que se obtiene mediante un transductor de ultrasonidos colocado en el abdomen materno. La CTG se utiliza ampliamente durante el embarazo como método para evaluar el bienestar fetal, sobre todo en embarazos con mayor riesgo de complicaciones.\\
	

\noindent
El objetivo general de este trabajo es comparar el rendimiento de diversas técnicas de Aprendizaje Automático Supervisado con el fin de recomendar aquella que presente el mejor desempeño al aplicarse sobre un conjunto de datos cardiológicos. Se expondrán las técnicas empleadas y las métricas utilizadas para medidas de calidad de un clasificador y, en consecuencia, resulta más adecuada para el problema planteado.

\section{Motivación}

El proceso de diagnóstico médico puede ser extenso, incluso contando con la mejor disposición del personal de salud, ya que con frecuencia requiere la recopilación y análisis de datos provenientes de distintos estudios. El propósito de este trabajo es contribuir a agilizar dicho proceso, identificando técnicas que puedan ser utilizadas por los profesionales médicos como herramientas complementarias para realizar diagnósticos. No solo resulta fundamental la posibilidad de obtener diagnósticos más ágiles, sino también la de reconocer qué atributos o características de los estudios resultan más significativos que otros para un diagnóstico determinado.

\section{Estado del Arte}


Las técnicas de Aprendizaje Automático se utilizan cada vez más en la investigación cardiovascular. El trabajo de Isaksen et al.~\cite{isaksen2025evaluating}(2025) presenta recomendaciones y orientaciones para la evaluación adecuada de modelos de aprendizaje automático supervisado en cardiología, destacando los problemas específicos asociados con estas técnicas, como la fuga de datos (\textit{data leakage}) y el desequilibrio de clases. Por su parte, el documento de Kumar y Kumar ~\cite{kumar2021machine}(2021) revisa las metodologías de aprendizaje automático para el diagnóstico de cardiopatías utilizando métodos no invasivos, un área crucial dada la alta mortalidad anual (17.9 millones de personas) asociada con los problemas cardíacos.

%% Algo sobre variables cariológicas en general, y sobre trabajos que usen alguno de los que yo use a cardiología.
 
\section{Conjuntos de datos Utilizados}

Para la realización de este trabajo se exploraron diversas plataformas en busca de conjuntos de datos reales que resultaran relevantes para el estudio mediante técnicas de aprendizaje automático. Durante esta búsqueda se identificaron múltiples \emph{datasets} de distinta naturaleza: algunos correspondientes a problemas de \textbf{clasificación binaria}, donde las observaciones se asocian a dos posibles clases, y otro \emph{datasets} de \textbf{clasificación multiclase}, con más de dos categorías posibles.\\

\noindent
Se observó, además, una marcada predominancia de conjuntos de datos provenientes del ámbito \textbf{médico}, dentro de los cuales se seleccionaron aquellos considerados más adecuados para las pruebas de los métodos de aprendizaje automático, abarcando tanto casos binarios como multiclase.\\

\noindent
Todos estos \emph{datasets} fueron procesados anteriormente a las pruebas realizadas, en donde se eliminaron tanto registros duplicados como con datos faltantes, tampoco se tuvieron en cuenta aquellos con datos atípicos a \textit{priori}, como un caso donde un paciente tiene colesterol 0.\\


\noindent
Se realizará una pequeña descripción de cada atributo utilizado en cada \emph{datasets}. Si el atributo es categórico, se informará la distribución de los valores que posee dicho atributo y en caso de resultar ser un atributo numérico, se informará la media de los valores de dicho atributo, junto al valor máximo y mínimo que posee. Se realiza lo anterior para contextualizar los atributos y ver los rangos de valores con que se trabajará. \\


\subsection{Dataset Binario: Insuficiencia  Cardíaca Predicción}

\noindent
Este \emph{dataset}~\cite{HeartFailure}, llamado \textit{HeartFailure} fue creado mediante la combinación de cinco \emph{datasets} independientes en 11 atributos comunes, logrando el\emph{datasets} más grande de información de enfermedades cardiovasculares utilizado para investigación. Los cinco \emph{datasets} utilizados son:

\begin{itemize}
	\item Cleveland: 303 observaciones
	\item Hungarian: 294 observaciones
	\item Switzerland: 123 observaciones
	\item Long Beach VA: 200 observaciones
	\item Stalog (Heart) Data Set: 270 observaciones
\end{itemize}

\noindent
En la  Tabla~\ref{tab:tablebinario} se muestran que tipo de datos son los atributos del\emph{datasets} que serán utilizados en este trabajo, considerando que estos mismos ya fueron previamente procesados para poder ser utilizados en las técnicas de aprendizaje automático. Luego la Tabla~\ref{tab:cant_datos_binarios} muestra la cantidad de registros y atributos que fueron utilizados, junto a que tipo de datos son.\\


\begin{table}[htb]%[H]
	\centering
	\caption{Cantidad de registros utilizados}
	\label{tab:cant_datos_binarios}
	\begin{tabular}{ll}
		\toprule
		\textbf{Cantidad de registros} & \textbf{918} \\
		\textbf{Cantidad de atributos} & \textbf{11} \\
		\textbf{Atributos Categóricos} & \textbf{5} \\
		\textbf{Atributos Numéricos} & \textbf{6} \\

		\bottomrule
	\end{tabular}
\end{table}

	
	
	\begin{table}[htb]
		\caption{Tipo de atributo del conjunto Binario.} 
		\centering
		\label{tab:tablebinario}
			\begin{tabular}{|l|c|c|c|}
			\hline
			\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{¿Esta codificado?} & \textbf{Unidad} \\
			\hline
			Age        & Numérico (int) & No & Años \\
			\hline
			Sex             & Categórico (string) & No & -\\
			\hline
			ChestPainType               & Categórico (string) & No & -\\
			\hline
			RestingBP     & Numérico (int) & No & mm Hg\\
			\hline
			Cholesterol    & Numérico (int) & No & mm/dl\\
			\hline
			FastingBS                 & Numérico (int) & Sí & mg/dl\\
			\hline
			RestingECG                      & Categórico (string) & No & -\\
			\hline
			MaxHR    & Numérico (int) & No & -\\
			\hline
			ExerciseAngina               & Categórico (string) & No & -\\
			\hline
			Oldpeak                 & Numérico (float) & No & ST en depresión\\
			\hline
			$ST\_Slope$                 & Categórico (string) & No & -\\
			\hline
			HeartDisease                 & Numérico (int)     & Sí & -\\
			\hline
			\hline
		\end{tabular}
				
	\end{table}

	
	
	
\subsubsection{\underline{Descripción de los atributos:}}

	
	\noindent
	\textbf{Age}: Este atributo refiera a la edad de los pacientes. Tiene media: 53 años, valor máximo: 77 y valor mínimo: 28, con proporciones de edad bastante bien distribuidas, siendo la menor de 0.11\% para algunas edades y la mayor de 4.14\% para otras edades, teniendo otras distribuciones entre estos dos rangos\\
	
	\noindent
	\textbf{Sex}: Refiere al Sexo de los pacientes; hay una distribución 78.98\% M (masculinos) y  hay 21.02\%  F (femeninos)\\
	
	\noindent
	\textbf{ChestPainType}: Tipo del dolor en el pecho, del caul hay varias clasificaciones; Tiene una distribución 18.85\% ATA, hay 22.11\% NAP, hay 54.03\% ASY, hay 5.01\% TA. [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\\
	
	\noindent
	\textbf{RestingBP}: Se esta describiendo la Presión sanguínea en reposo, donde hay una distribución de 51.09\% de mujeres, codificadas en 1 y 48.91\% de hombres, codificados en 0.\\
	
	%{0.2cm}
	\noindent
	\textbf{Cholesterol}: Este atributo es el Colesterol serico, la medida total de colesterol en sangre; tiene media: 199.02, valor máximo: 603.00 y valor mínimo: 0.00. Miligramos por decilitro \\
	
	%{0.2cm}
	\noindent
	\textbf{FastingBS}: Es la Glucosa en sangre en ayuno; hay 76.66\% Glucosa en sangre < 120 mg/dl codificado en 0 y hay 23.34\% Glucosa en sangre > 120 mg/dl codificado en 1\\
	
	%{0.2cm}
	\noindent
	\textbf{RestingECG}: Son los Resultados de electrocardiogramas en reposo; hay 60.09\% codificado en Normal, hay 19.41\% codificado en ST y hay 20.50\% codificado en LVH  [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria] \\
	
	%{0.2cm}
	\noindent
	\textbf{MaxHR}: Este atributo es el Máximo ritmo cardíaco registrado, tiene media: 136.79, valor máximo: 202.00 y valor mínimo: 60.00 \\
	%{0.2cm}
	
	\noindent
	\textbf{ExerciseAngina}: Es la Angina producido por ejercicio, dolor en el pecho; hay 59.54\% No codificado en N y hay 40.46\% Si codificado en Y \\
	
	%{0.2cm}
	\noindent
	\textbf{Oldpeak}: Valor máximo de depresión del segmento ST (en milímetros) registrado en todas las derivaciones contiguas durante una prueba de esfuerzo. Forma parte del cálculo del riesgo de un paciente de isquemia o infarto de miocardio; valores más altos indican un mayor riesgo de enfermedad coronaria; tiene media: 0.90, valor máximo: 6.20 y valor mínimo: -0.10 \\
	
	%{0.2cm}
	\noindent
	\textbf{ST\_Slope}: The slope of the peak exercise ST segment; hay 43.08\% Up, hay 50.05\% Flat y hay 6.87\% Down [Up: upsloping, Flat: flat, Down: downsloping]\\
	
	%{0.2cm}
	\noindent
	\textbf{HeartDisease}: Variable de salida de si posee una enfermedad cardíaca; hay 44.71\% No codificado en 0 y hay 55.29\% Si codificado en 1.
	Siendo esta la \textbf{variables objetivo}.


\subsection{Dataset Multiclase: \emph{Cardiotocografía Predicción}}
	

\noindent
	En el \emph{datasets}~\cite{Cardiotocografia} utilizado se procesaron automáticamente 2126 cardiotocogramas fetales y se midieron sus características diagnósticas. Tres obstetras expertos clasificaron los CTG y se les asignó una etiqueta de clasificación consensuada. La clasificación se realizó tanto con respecto al estado fetal (N: Normal, S: Sospechoso y P: Patológico).\\

    \noindent
En la Tabla~\ref{tab:tablaej} se muestran los tipos de las variables utilizadas en este trabajo. Luego la cantidad de registros y los tipos de atributos utilizados para este trabajo fueron los siguientes, considerando que estos registros ya fueron previamente procesados para poder ser utilizados en las técnicas de aprendizaje automático. Luego la Tabla~\ref{tab:cant_datos_multiclase} muestra la cantidad de registros y atributos que fueron utilizados, junto a que tipo de datos son.


\begin{table}[htb]%[H]
	\centering
	\caption{Cantidad de registros utilizados}
	\label{tab:cant_datos_multiclase}
	\begin{tabular}{ll}
		\toprule
		\textbf{Cantidad de registros} & \textbf{2115} \\
		\textbf{Cantidad de atributos} & \textbf{21} \\
		\textbf{Atributos Categóricos} & \textbf{0} \\
		\textbf{Atributos Numéricos} & \textbf{21} \\
		
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]
\caption{Tipo de atributo del conjunto Multiclae.}
\label{tab:tablaej}
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{Media} & \textbf{Máximo} & \textbf{Mínimo}\\
		\hline
		LB        & Numérico (int) & 133.301 &160.000 &106.000\\
		
		\hline
		AC             & Numérico & 0.003 & 0.019 & 0.000\\
		\hline
		FM               & Numérico (float)& 0.009 & 0.481 & 0.000\\
		\hline
		UC     & Numérico (float) & 0.004 &0.015 &0.000\\
		\hline
		DL    & Numérico (float)&0.001 & 0.015 & 0.000\\
		\hline
		DS                 & Numérico (float)&- &- &-\\
		\hline
		DP                      & Numérico (float)&- &- &-\\
		\hline
		ASTV    & Numérico (int) &46.977 &87.000 &12.000\\
		\hline
		MSTV               & Numérico (float) &1.335 & 7.000&0.200\\
		\hline
		ALTV                 & Numérico (int) &9.759 &91.000 &0.000\\
		\hline
		MLTV                 & Numérico (float)&8.170 &50.700 &0.000\\
		\hline
		Width                 & Numérico (int) &70.511 &180.000 &3.000\\
		\hline
		Min                 & Numérico (int) &93.574 &159.000 &50.000\\
		\hline
		Max                 & Numérico (int)  &164.085 &238.000 &122.000\\
		\hline
		Nmax                 & Numérico (int)  &4.075 &18.000 &0.000\\
		\hline
		Nzeros                 & Numérico (int)  &- &- &-\\
		\hline
		Mode                 & Numérico (int)  &137.448 &187.000 &60.000\\
		\hline
		Mean                 & Numérico (int)& 134.596& 182.000&73.000 \\
		\hline
		Median                 & Numérico (int) &138.084 &186.000 &77.000 \\
		\hline
		Variance                 & Numérico (int)&18.891 &269.000 &0.000 \\
		\hline
		Tendency                 & Numérico (int)&- &- &- \\
		\hline
		NSP                 & Categórico (string) &- &- &-\\
		\hline
	\end{tabular}
\end{table}




\textbf{\underline{Descripción de los atributos:}}

	%{0.2cm}
	\noindent
	\textbf{LB}: Frecuencia cardíaca fetal basal (latidos por minuto). Tiene media: 133.30, valor máximo: 160.00 y valor mínimo: 106.00\\
	
\noindent
	\textbf{AC}: Número de aceleraciones por segundo. Tiene media: 0.00, valor máximo: 0.02 y valor mínimo: 0.00 \\
	
\noindent
	\textbf{FM}: Número de movimientos fetales por segundo. Tiene media: 0.01, valor máximo: 0.48 y valor mínimo: 0.00 \\
	
\noindent
	\textbf{UC}: Número de contracciones uterinas por segundo.  Tiene media: 0.00, valor máximo: 0.01 y valor mínimo: 0.00\\
	
\noindent
	\textbf{DL}: Número de desaceleraciones leves por segundo. Tiene media: 0.00, valor máximo: 0.01 y valor mínimo: 0.00  \\
	
\noindent
	\textbf{DS}: Número de desaceleraciones severas por segundo. Hay un 99.67\% con valor 0.0 y un 0.33\% con un valor 0.001\\
	
\noindent
	\textbf{DP}: Número de desaceleraciones prolongadas por segundo.  Hay un 91.58\% con valor 0.0, 3.40\% con un valor 0.002, 1.13\% con un valor 0.003, 3.31\% con un valor 0.001, 0.43\% con un valor 0.004 y 0.14\% con un valor 0.005\\
	
\noindent
	\textbf{ASTV}: Porcentaje de tiempo con variabilidad anormal a corto plazo. Tiene media: 46.98, valor máximo: 87.00 y valor mínimo: 12.00 \\
	
\noindent	
	\textbf{MSTV}: Valor medio de la variabilidad a corto plazo. Tiene media: 1.34, valor máximo: 7.00 y valor mínimo: 0.20 \\
	
\noindent
	\textbf{ALTV}: Porcentaje de tiempo con variabilidad anormal a largo plazo. Tiene media: 9.79, valor máximo: 91.00 y valor mínimo: 0.00 \\
	
\noindent
	\textbf{MLTV}: Valor medio de la variabilidad a largo plazo. Tiene media: 8.17, valor máximo: 50.70 y valor mínimo: 0.00\\
	
\noindent
	\textbf{Width}: Ancho del histograma de TCG. Tiene media: 70.51, valor máximo: 180.00 y valor mínimo: 3.00\\

\noindent
	\textbf{Min}: Mínimo del histograma de TCG. Tiene media: 93.57, valor máximo: 159.00 y valor mínimo: 50.00\\
	
\noindent
	\textbf{Max}: Máximo del histograma de TCG. Tiene media: 164.09, valor máximo: 238.00 y valor mínimo: 122.00\\

\noindent
	\textbf{Nmax}: Número de picos del histograma. Tiene media: 4.08, valor máximo: 18.00 y valor mínimo: 0.00\\
	
\noindent
	\textbf{Nzeros}: Número de ceros del histograma.  Hay un 76.26\% con valor 0, 17.30\% con un valor 1, 0.99\% con un valor 3, 5.11\% con un valor 2, 0.09\% con un valor 4, 0.05\% con un valor 10, 0.09\% con un valor 5, 0.05\% con un valor 8, y  0.05\% con un valor 7. \\
	
\noindent
	\textbf{Mode}: Moda del histograma. Tiene media: 137.45, valor máximo: 187.00 y valor mínimo: 60.00\\
	
\noindent
	\textbf{Mean}: Promedio del histograma. Tiene media: 134.60, valor máximo: 182.00 y valor mínimo: 73.00\\
	
\noindent
	\textbf{Median}: Media del histograma. Tiene media: 138.08, valor máximo: 186.00 y valor mínimo: 77.00\\
	
\noindent
	\textbf{Variance}: Varianza del histograma. Tiene media: 18.89, valor máximo: 269.00 y valor mínimo: 0.00\\
	
\noindent
	\textbf{Tendency}: Tendencia del histograma.  Hay un 39.67\% con valor 1, 52.53\% con un valor 0 y 8.27\% con un valor -1\\
	
\noindent
	\textbf{CLASS}: Código de clasificación del estado fetal (N=normal; S=sospechoso; P=patológico).  Hay un 13.81\% con valor Sospechoso, 77.92\% con un valor Normal, 8.27\% con un valor Patológico.
	Siendo esta la \textbf{variables objetivo}.
	

\chapter{Métricas de Rendimiento Utilizadas}



El objetivo de este trabajo es evaluar el desempeño calificador de cada modelo de Aprendizaje Automático. Para alcanzarlo, se utilizarán \textbf{métricas de rendimiento} que permiten cuantificar la capacidad del algoritmo de clasificar, ergo son herramienta que utilizamos para saber que tan bien predice un modelo a una clase o que tan bien puede clasificar entre varias clases.\\

\noindent
\textbf{La importancia de las métricas} se ubica en que el objetivo central de estos algoritmos no es simplemente obtener un buen rendimiento en los datos utilizados para construir el modelo, sino en su \textbf{capacidad de generalización}, su habilidad para funcionar correctamente con entradas nuevas y previamente no observadas (no utilizadas en el entrenamiento). Esto se debe a que es perfectamente normal y sumamente esperable que el modelo funcione correctamente con el conjunto de datos que se utiliza para el entrenamiento del modelo, la idea fundamental es poder tener el mismo rendimiento o incluso uno mejor que con el conjunto de entrenamiento.\\

\noindent
Para la obtención de las métricas y entrenamiento de algoritmo se utilizara la estrategia de \textbf{Validación Cruzada $k$-fold}, donde el conjunto de datos se divide en $k$ grupos (o pliegues, en una traducción más fiel) del mismo tamaño, donde en cada iteración un grupo $k$ es utilizado para entrenar y el resto para evaluar, repitiéndose el proceso $k$ veces. Es importante señalar que un grupo $k_i$ es utilizado solo una vez para entrenar, el resto de veces será utilizado como parte del conjunto de prueba. El valor final estimado de la métrica, denotado por $\widehat{M}$,  es el promedio de los valores obtenidos de cada grupo, es decir,
\begin{equation}
%\label{eq:metrica}
\hat{M} = \frac{1}{k} \sum_{i=1}^{k} M_i,
\label{eq:kfolds}
\end{equation}
donde $M_i$ es el valor de la métrica de evaluación obtenido en el $i$-ésimo grupo utilizado como conjunto de prueba, para $i=1,2, \dots, k$.\\

\noindent
Dentro de este trabajo no sólo se evaluaran distintos modelos, sino que se utilizaran distintos \textit{datasets} para lograrlos. A continuación se señalaran las métricas que se utilizaran en cada caso y se podran apreciar algunas diferencias, leves, pero diferencias en si. 

\section{Métricas para caso Binario}

\textbf{Matriz de Confusión}

Una matriz de confusión, que se puede observar en la Tabla~\ref{tab:matrizconfusionbinario}, es una forma simple de saber de que forma esta clasificando el algoritmo, donde una clase es considerada \textbf{positiva $P$} y la otra \textbf{negativa $N$}. La matriz de confusión clasifica las predicciones en:

\begin{itemize}
	\item \textbf{Verdaderos Positivos (TP):} Casos positivos clasificados correctamente.\\
	\item \textbf{Verdaderos Negativos (TN):} Casos negativos clasificados correctamente.\\
	\item \textbf{Falsos Positivos (FP):} Casos negativos clasificados incorrectamente como positivos.\\
	\item \textbf{Falsos Negativos (FN):} Casos positivos clasificados incorrectamente como positivos. \\
\end{itemize}


\begin{table}[htb]
    \centering
    \begin{tabular}{|c|c|c|c|}
		\cline{3-4}
		\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{\textbf{Predicción}} \\
		\cline{3-4}
		\multicolumn{2}{c|}{} & \textbf{Positivo} & \textbf{Negativo} \\
		\hline
		\multirow{2}{*}{\textbf{Verdad}} & \textbf{Positivo} & \cellcolor{TPcolor} \textbf{Verdadero Positivo (TP)} & \cellcolor{FNcolor} \textbf{Falso Negativo (FN)} \\
		\cline{2-4}
		& \textbf{Negativo} & \cellcolor{FNcolor} \textbf{Falso Positivo (FP)} & \cellcolor{TPcolor} \textbf{Verdadero Negativo (TN)} \\
		\hline
	\end{tabular}
    \caption{Matriz de Confusion}
    \label{tab:matrizconfusionbinario}
\end{table}




\textbf{\textit{Accuracy}}

El \textit{Accuracy} es la proporción de instancias clasificadas correctamente, es una medida "ingenua" que puede ser engañosa si existe un gran desbalance entre clases, ergo se puede obtener un \textit{Accuracy} alto si predice una clase muy bien, que tiene una distribución mucho mayor que la otra, mientras que la de menor distribución casi no la predice.
\noindent
En términos de la Matriz de Confusión la formula seria la siguiente:
\begin{equation}
    \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN} = \frac{TP + TN}{\text{Total}},
    \label{eq:accurcybinario}
\end{equation}
\noindent
y en términos del conjunto de predicciones y valores verdaderos, se tiene que $n_{\text{samples}}$: representa la cantidad total de ejemplos en la muestra, mientras que $\hat{y}_i$ es el valor predicho del $i$-ésimo ejemplo, e $y_i$ es el valor verdadero correspondiente:

\begin{equation}
    \texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i),
    \label{eq:accuracybinarioformula}
\end{equation}
\noindent
por lo tanto, se puede simplificar como la siguiente formula:

\begin{equation}
    \text{Accuracy} = \frac{\text{Número de predicciones correctas}}{\text{Número total de muestras}}.
    \label{eq:accuracygeneral}
\end{equation}


\textbf{\textit{Precision}}

El \textit{Precision} mide la probabilidad de que la predicción positiva del clasificador sea correcta, en otras palabras, mide que tan bien predice las clases positivas el modelo.
\noindent
En términos de la Matriz de Confusión, se puede expresar lo anterior de la siguiente manera;
\begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}.
    \label{eq:precisionbinario}
\end{equation}


\textbf{\textit{Recall}}

El \textit{Recall} o también conocido como Sensibilidad o Tasa de Verdaderos Negativos (TPR). Mide la probabilidad de que el clasificador detecte un caso positivo cuando en verdad lo es.
\noindent
En términos de la Matriz de Confusión se puede entender a la \textit{Recall} de la siguiente manera:

\begin{equation}
     \text{Recall} = TPR = \frac{TP}{TP + FN} = \frac{TP}{P}.
     \label{eq:recallbinario}
\end{equation}


\textbf{\textit{F-measure}}

El \textit{F-measure} es la media armónica ponderada de \textit{precision} y \textit{recall}. La versión más común es el \textbf{F1-score}, donde el parámetro de ponderación $\beta$ es igual a 1. Un clasificador perfecto tiene un valor $F1 = 1$.
\noindent
Fórmula General ($F_{\beta}$):

\begin{equation}
    F_{\beta} = \frac{(1 + \beta^2) \text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}},
    \label{eq:formulageneral}
\end{equation}
\noindent
donde la fórmula del F1-score ($\beta=1$) en términos de Precision y Recall se puede notar de la siguiente manera: 

\begin{equation}
    F1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}},
    \label{eq:f1general}
\end{equation}
\noindent
o en términos de la Matriz de Confusión, de forma más simplificada:
\begin{equation}
     F1 = \frac{2TP}{2TP + FP + FN}.
        \label{eq:f1enmatriz}
\end{equation}

\textbf{\textit{Área Bajo la Curva ROC (ROC AUC)}}

La métrica \textit{ROC AUC} es un valor que resume la capacidad de un clasificador para distinguir entre clases, una métrica muy útil para comparar el desempeño entre modelos distintos o entre un mismo modelo con hipermetamorfosis distintos.\\

\noindent
\textbf{La Curva ROC} es un gráfico que ilustra el rendimiento de un clasificador binario a media que se varia su umbral de discriminación. Se crea graficando la \textbf{Tasa de Verdaderos Positivos (TPR)} versus la \textbf{Tasa de Falsos Positivos (FPR)} en varios umbrales. El \textbf{AUC} mide justamente el área debajo de la Curva ROC.\\

\noindent
Ejes utlizados para el gráfico:

• \textbf{Eje Y:} TPR

• \textbf{Eje X:} FPR\\

\noindent
Interpretación de valores: Un clasificador \textbf{ideal} se ubica en el punto $(0, 1)$, donde $TPR=1$ y $FPR=0$, lo que resulta en un \textbf{$AUC = 1$}. Un clasificador \textbf{aleatorio} se sitúa sobre la línea $TPR = FPR$, lo que resulta en un \textbf{$AUC = 0.5$}. Un clasificador se considera \textbf{razonable} si \textbf{ $0.5 < AUC \leq 1$}



\textbf{Métricas para caso Multiclase}

En este caso se utiliza el método \textit{"weighted"}, el cual computa o tiene en cuenta el desequilibrio de clases calculando el promedio de métricas binarias, en las que la puntuación o peso de cada clase se pondera según su presencia en la muestra de datos reales.\\

\noindent
La métrica ponderada por la presencia de la clase, $\text{M}_{\text{weighted}}$, se calcula como el promedio de la métrica por clase $\text{M}_l$, donde cada contribución es ponderada por el tamaño de la clase $|y_l|$, siendo $L$ es el conjunto de etiquetas o clases. Donde $\hat{M}_{\text{weighted}}$ es el valor estimado de la métrica promedio ponderada.

\begin{equation}
    \hat{M}_{\text{weighted}} = \frac{1}{\sum{l \in L} |y_l|} \sum_{l \in L} |y_l| \cdot \text{M}_l.
    \label{eq:metricasponderadas}
\end{equation}

\textbf{Matriz de Confusión (Multiclase)}

La matriz de confusión multiclase, que se puede ver en la Tabla~\ref{tab:matrizmulticlase}, es una matriz cuadrada de tamaño $L \times L$, donde $L$ es el número de clases. Cada celda $C_{ij}$ representa la cantidad de muestras verdaderamente pertenecientes a la clase $i$ que fueron clasificadas como clase $j$.\\

\noindent
Para cada clase $l$ se definen los valores que antes habíamos utilizados para la matriz de confusión del caso binario:
\begin{itemize}
\item $TP_l = C_{ll}$ 
\item $FP_l = \sum_{i \neq l} C_{il}$
\item $FN_l = \sum_{j \neq l} C_{lj}$
\item $TN_l = N - TP_l - FP_l - FN_l$
\end{itemize}


\begin{table}[htb]%[htb][]
    \centering
\begin{tabular}{|c|c|c|c|c|c|}
		\cline{3-6}
		\multicolumn{2}{c|}{} & \multicolumn{4}{c|}{\textbf{Predicción}} \\
		\cline{3-6}
		\multicolumn{2}{c|}{} & \textbf{Clase $C_1$} & \textbf{Clase $C_2$} & \textbf{$\cdots$} & \textbf{Clase $C_l$} \\
		\hline
		\multirow{4}{*}{\textbf{Verdad}} 
		& \textbf{Clase $C_1$} & \cellcolor{TPcolor} TN$_l$ & \cellcolor{TPcolor} $\cdots$ & \cellcolor{TPcolor} TN$_l$ & \cellcolor{FNcolor} FP$_l$ \\
		\cline{2-6}
		& \textbf{Clase $C_2$} & \cellcolor{TPcolor} TN$_l$ & \cellcolor{TPcolor} TN$_l$ & \cellcolor{TPcolor} $\cdots$ & \cellcolor{FNcolor} FP$_l$ \\
		\cline{2-6}
		& $\vdots$ & \cellcolor{TPcolor} $\vdots$ & \cellcolor{TPcolor} $\vdots$ & \cellcolor{TPcolor} $\ddots$ & \cellcolor{FNcolor} $\vdots$ \\
		\cline{2-6}
		& \textbf{Clase $C_l$} & \cellcolor{FNcolor} FN$_l$ & \cellcolor{FNcolor} $\cdots$ & \cellcolor{FNcolor} FN$_l$ & \cellcolor{TPcolor} TP$_l$ \\
		\hline
	\end{tabular}
    \caption{Matriz Confusion Multiclase}
    \label{tab:matrizmulticlase}
\end{table}



\textbf{\textit{Precision}}

La \textit{Precision} por clase $l$ mide la proporción de muestras clasificadas como positivas que realmente pertenecen a la clase $l$.

\noindent
En términos más simple, utilizando la Matriz de Confusión:

\begin{equation}
    \text{Precision}_l = \frac{TP_l}{TP_l + FP_l}.
    \label{eq:precisionmulticlase}
\end{equation}


\textbf{\textit{Recall}}

El \textit{Recall} por clase $l$ mide la proporción de muestras verdaderamente positivas de la clase $l$ que fueron correctamente identificadas.

\noindent
En términos más simple, utilizando la Matriz de Confusión:

\begin{equation}
    \text{Recall}_l = \frac{TP_l}{TP_l + FN_l}.
    \label{eq:recallmulticlase}
\end{equation}

\textbf{\textit{F-measure}}

El \textit{F-measure} es la media armónica ponderada de \textit{precision} y \textit{recall}. La versión más común es el \textbf{F1-score}, donde el parámetro de ponderación $\beta$ es igual a 1. Un clasificador perfecto tiene un valor $F1 = 1$.

\noindent
El valor global ponderado se obtiene aplicando la fórmula de $M_{\text{weighted}}$ sobre los $F_{1,l}$:

\begin{equation}
    F_{1,\text{weighted}} = \sum_{l \in L} w_l \, F_{1,l}, \quad 
\text{con } w_l = \frac{n_l}{\sum_{i \in L} n_i}.
    \label{eq:f1formula}
\end{equation}



\textbf{\textit{Área Bajo la Curva ROC (ROC AUC)}}

Para extender la métrica ROC AUC a clasificación multiclase se emplea el enfoque \textbf{One-vs-Rest (OVR)}. Para cada clase $l$, se considera la clase $l$ como positiva y el resto como negativas, luego se calcula el AUC correspondiente ($\text{AUC}_l$) sobre la curva ROC de esa clasificación binaria. Finalmente, se obtiene un promedio ponderado por el soporte de cada clase:

\begin{equation}
     \text{AUC}_{\text{OVR, weighted}} = \sum_{l \in L} w_l \, \text{AUC}_l.
     \label{eq:onevsrest}
\end{equation}
%{eq:onevsrest}{One-vs-Rest (OVR)}



\textbf{Importancia de la característica}

\noindent Sea un modelo predictivo $Modelo$ entrenado sobre un conjunto de datos tabular $X$ (ya sea de entrenamiento o validación), y sea $M$ la métrica de referencia del modelo sobre los datos originales. El procedimiento se detalla a continuación:

\begin{enumerate}
	\item Calcular el puntaje de referencia del modelo $M$ sobre $X$:
	\begin{equation}
		\text{M} = \text{calcular métrica}(Modelo, X).
		\label{eq:puntaje_referencia}
	\end{equation}

	\item Para cada atributo $j$ del conjunto de datos:
	\begin{enumerate}
		\item Repetir el siguiente proceso $K$ veces (para reducir la varianza de la estimación):
		\begin{enumerate}
			\item Generar una versión alterada del conjunto de datos, $X^{(k,j)}$, en la que se permuta aleatoriamente, se intercambian de orden los datos de la columna correspondiente a la característica $j$, manteniendo las demás columnas sin cambios, para ver si el modelo empeora o no en su rendimiento.
			
			\item Calcular el puntaje del modelo sobre los datos permutados:
			\begin{equation}
				M_{k,j} = \text{calcular métrica}(Modelo, X^{(k,j)}).
				\label{eq:puntaje_referencia_datos_permutados}
			\end{equation}

		\end{enumerate}
		\item Calcular la importancia de la característica $j$ como la disminución promedio en el puntaje del modelo respecto del puntaje de referencia:
		\begin{equation}
			I_j = M - \frac{1}{K} \sum_{k=1}^{K} M_{k,j}.
			\label{eq:puntaje_referencia_final}
		\end{equation}

	\end{enumerate}
\end{enumerate}


\noindent De esta manera, $I_j$ mide la pérdida de desempeño al romper la relación entre la característica $j$ y la variable objetivo. Valores más altos de $I_j$ indican características más relevantes para el modelo.\\



















\chapter{Descripción de los Métodos Utilizados}




\section{Regresión Logística}

El modelo de \textbf{Regresión Logística}~\cite{cramer2002origins} (LR, por su equivalente en inglés \emph{Logistic Regression}) es una técnica del análisis de datos utilizada para establecer relaciones entre las variables predictoras y la clase a la cual pertenece cada registro. Posteriormente, el modelo permite predecir la probabilidad de que un nuevo registro pertenezca a una clase determinada. Este tipo de modelo de regresión es justamente utilizado para los problemas no linealmente separables, como la mayoría de problemas.\\

\noindent
A diferencia de la regresión lineal múltiple, la regresión logística predice una probabilidad (valor entre 0 y 1). Ambos modelos son lineales en sus parámetros, pero difieren en la naturaleza de la variable dependiente. El objetivo es estimar los coeficientes de regresión que maximizan la verosimilitud de los datos observados, o encontrar los coeficientes que mejor funcionan para los datos de entrenamiento.\\

\noindent
El modelo busca modelar la probabilidad condicional de que una observación pertenezca a la clase objetivo $y_{i}$, siendo

\begin{equation}
	P(y_i = 1 \mid \bm{x}_i),
	\label{eq:probabilidad_condicional}
\end{equation}

donde $\bm{x}_i = (x_{i1}, x_{i2}, \dots, x_{ik})$ es el vector de características de la observación $i$, el cual también llamamos registro.

\textbf{Función de Probabilidad}

La función logística define la probabilidad de pertenencia de $x_{i}$ a la clase $1$ como:

\begin{equation}
	p(\bm{x}_i) = P(y_i = 1 \mid \bm{x}_i) =
	\frac{\exp(\beta_0 + \sum_{j=1}^{k} \beta_j x_{ij})}
	{1 + \exp(\beta_0 + \sum_{j=1}^{k} \beta_j x_{ij})},
	\label{eq:funcion_logistica}
\end{equation}

donde $\beta_0$ es el intercepto y $\beta_j$ los coeficientes asociados a cada predictor.

\textbf{Función Logit}

La función inversa de la función logística, denominada \textit{logit}, relaciona el logaritmo de las \emph{odds} con un modelo lineal, siendo \emph{odds} lo que se utiliza para analizar si la probabilidad de ocurrencia de un evento -caso/no caso- difiere o no en distintos grupos,

%\begin{equation}
%	\text{logit}(\textbf{antilogic}(x)) = x
%	\label{eq:logit}
%\end{equation}


%\begin{equation}
%	\text{odds} =
%	\frac{p}{1 - p} 
%	\label{eq:odds}
%\end{equation}


\begin{equation}
	\text{logit}(p(\bm{x}_i)) =
	\ln \left( \frac{p(\bm{x}_i)}{1 - p(\bm{x}_i)} \right)
	= \beta_0 + \sum_{j=1}^{k} \beta_j x_{ij}.
	\label{eq:funcion_logit}
\end{equation}

\noindent
Esta transformación asegura que las probabilidades estén acotadas entre 0 y 1, mientras que la combinación lineal de predictores puede tomar cualquier valor real, siendo este ultimo punto un factor que realiza el calculo de coeficientes muy difícil.

\textbf{Estimación por Máxima Verosimilitud}

Los coeficientes de regresión se estiman mediante el método de \textbf{Máxima Verosimilitud} (MLE), donde la función de log-verosimilitud a maximizar

\begin{equation}
	\ell(\beta_0, \beta_1, \dots, \beta_k) =
	\sum_{i=1}^{n} \left[
	y_i \ln(p(\bm{x}_i)) +
	(1 - y_i) \ln(1 - p(\bm{x}_i))
	\right],
	\label{eq:log_verosimilitud}
\end{equation}

\noindent
en donde la solución analítica no existe, por lo que se utilizan métodos numéricos iterativos para obtener los parámetros óptimos.

\textbf{Hiperparámetros}

\begin{itemize}
	\item \textbf{Parámetro de Regularización ($C$):} Controla la complejidad del modelo. Valores pequeños de $C$ implican mayor regularización (menor sobreajuste), mientras que valores grandes permiten mayor flexibilidad del modelo.
	
	\item \textbf{Penalización (\textit{penalty}):} Es un término regulador ($\Omega$) que se suma a la función de coste original ($J$) para formar una función ajustada $\tilde{J}$. Controla la capacidad del modelo y reduce el error de generalización.
	
	\begin{itemize}
	
	\item  Regularización L1 (Lasso).
	\begin{equation}
		\tilde{J}_{L1}(\boldsymbol{\beta}) =
		J(\boldsymbol{\beta}) +
		\alpha \sum_{j=1}^{k} |\beta_j|.
		\label{eq:l1_penalty}
	\end{equation}
	%{eq:l1_penalty}{}
	
	\item Regularización L2 (Ridge).
	\begin{equation}
		\tilde{J}_{L2}(\boldsymbol{\beta}) =
		J(\boldsymbol{\beta}) +
		\frac{1}{2}\alpha \sum_{j=1}^{k} \beta_j^2.
		\label{eq:l2_penalty}
	\end{equation}
	%{eq:l2_penalty}{}
	
	\item Regularización Elastic Net
	\begin{equation}
		\tilde{J}_{EN}(\boldsymbol{\beta}) =
		J(\boldsymbol{\beta}) +
		\alpha \left[
		\rho \sum_{j=1}^{k} |\beta_j| +
		\frac{1 - \rho}{2} \sum_{j=1}^{k} \beta_j^2
		\right].
		\label{eq:elasticnet_penalty}
	\end{equation}
	%{eq:elasticnet_penalty}{}
	
	\end{itemize}
	
	\item \textbf{Algoritmo de Optimización (\textit{solver}):}  
	El \textit{solver} es el algoritmo numérico encargado de minimizar la función de coste regularizada $\tilde{J}(\boldsymbol{\beta})$.
	
	\begin{itemize}
		\item Método Newton-CG (basado en segunda derivada).
		\begin{equation}
			\boldsymbol{\beta}^{(t+1)} \leftarrow
			\boldsymbol{\beta}^{(t)} -
			\bm{H}^{-1}
			\nabla_{\boldsymbol{\beta}} \tilde{J}(\boldsymbol{\beta}^{(t)}).
			\label{eq:newtoncg}
		\end{equation}
%		%{eq:newtoncg}{}

		\item Método BFGS (quasi-Newton).
		\begin{equation}
			\boldsymbol{\beta}^{(t+1)} \leftarrow
			\boldsymbol{\beta}^{(t)} -
			\bm{B}^{-1}
			\nabla_{\boldsymbol{\beta}} \tilde{J}(\boldsymbol{\beta}^{(t)}).
			\label{eq:bfgs}
		\end{equation}
%		%{eq:bfgs}{}
	\end{itemize}
	
	\item 
\textbf{Estrategia Multiclase (\textit{multi\_class}):}
	La regresión logística está diseñada originalmente para clasificación binaria. Para extenderla a múltiples clases se emplean estrategias como:
	\begin{itemize}
		\item \textit{one-vs-rest} (OvR): Entrena un clasificador por clase.
		\item \textit{multinomial}: Optimiza una única función de verosimilitud multinomial conjunta.
	\end{itemize}
\end{itemize}

\section{Árboles de Decisión}

El aprendizaje mediante \textbf{Árboles de Decisión} (RF, por su equivalente en inglés \emph{Random Forest}) es un método no paramétrico que utiliza divisiones jerárquicas sobre los atributos de los datos, construyendo reglas de decisión del tipo \textit{if-else} para predecir el valor de una variable objetivo.\\

\noindent  
El objetivo principal es encontrar las divisiones (particiones) que maximicen la pureza de los nodos hijos, es decir, que minimicen la impureza del nodo resultante. Es un método mucho mas sencillo de realizar, donde se crea un camino de decisión, por lo cual según los valores de los atributos de un registro podemos predecir que a que clase pertenecerían, donde el computo pesado se encuentra en la creación del propio camino. 

\textbf{Conceptos Fundamentales}

La probabilidad de que un ejemplo en el nodo $t$ pertenezca a la clase $C_k$ se define como

\begin{equation}
	p(k|t) = \frac{N_k(t)}{N(t)},
	\label{eq:probabilidad_clase_nodo}
\end{equation}
%%{eq:probabilidad_clase_nodo}{Probabilidad de pertenencia a la clase $C_k$ en el nodo $t$}
donde $N(t)$ es la cantidad total de ejemplos en el nodo $t$, y $N_k(t)$ la cantidad de ejemplos de la clase $C_k$. Es importante este fenómeno porque es un costo computacional muy barato el calcular esta probabilidad y se asemejan a las probabilidades equiprobables, donde la probabilidad de una clase en un nodo esta dada por la cantidad de clases que tienen en el mismo.

\subsubsection{Impureza del Nodo}

La impureza de un nodo $t$ se mide mediante una función $\phi$ que depende de las probabilidades de clase en dicho nodo, donde la función de impureza $i$ es un hiperparametro en sí mismo.

\begin{equation}
	i(t) = \phi \big( p(1|t), p(2|t), \ldots, p(K|t) \big).
	\label{eq:impureza_general}
\end{equation}
%%{eq:impureza_general}{Impureza general de un nodo $t$ en función de las probabilidades de clase}

\noindent
La impureza es máxima cuando las clases están perfectamente mezcladas y mínima (cero) cuando el nodo contiene solo una clase, en donde la impureza máxima seria que la probabilidad de cada clase sea aleatoria, ya que están perfectamente mezcladas.

\paragraph{Entropía de Shannon de un conjunto de datos $D$}

\begin{equation}
	H(D) = - \sum_{k=1}^K 
	\frac{N_k(D)}{N(D)} 
	\log_2 \left( \frac{N_k(D)}{N(D)} \right).
	\label{eq:entropia_shannon}
\end{equation}
%%{eq:entropia_shannon}{Entropía de Shannon de un conjunto de datos $D$}

%\paragraph{Ganancia de Información}

%\begin{equation}
%	IG(A) = H(D) - H(D|A)
%	= H(D) - \sum_{a \in \text{val}(A)}
%	\frac{N(D_a)}{N(D)} H(D_a)
%	\label{eq:ganancia_informacion}
%\end{equation}
%%{eq:ganancia_informacion}{Ganancia de información de un atributo $A$}

\paragraph{Índice de Gini de un nodo $t$}

\begin{equation}
	\text{Gini}(t) = 
	1 - \sum_{k=1}^K \big[p(k|t)\big]^2 =
	1 - \sum_{k=1}^K \left(\frac{N_k(t)}{N(t)}\right)^2.
	\label{eq:indice_gini}
\end{equation}
%%{eq:indice_gini}{Índice de Gini de un nodo $t$}

\subsubsection{Disminución de Impureza}

La reducción de impureza generada al dividir el nodo $t$ en dos nodos hijos $t_1$ y $t_2$ mediante una partición $s$ se calcula como

\begin{equation}
	\Delta i(s, t) = i(t) - q_1 i(t_1) - q_2 i(t_2),
	\label{eq:disminucion_impureza}
\end{equation}
%%{eq:disminucion_impureza}{Disminución de impureza asociada a una partición $s$ del nodo $t$}

donde $q_j = \frac{N(t_j)}{N(t)}$ para $j = 1, 2$.

\subsection{Bosques Aleatorios (Random Forest)}

El algoritmo de \textbf{Random Forest}~\cite{Breiman2001} (RF) combina múltiples árboles de decisión independientes construidos sobre subconjuntos aleatorios de los datos (muestreo con reemplazo o \emph{bootstrap}).  
Cada árbol se entrena sobre un subconjunto de atributos aleatorios en cada división, lo que introduce diversidad y reduce la varianza.

\noindent
La predicción final para clasificación se obtiene mediante el voto mayoritario de los árboles

\begin{equation}
	\hat{y} = 
	\underset{c \in \mathcal{C}}{\operatorname{argmax}}
	\sum_{m=1}^{M} \mathbb{I}
	\big( h_m(\bm{x}) = c \big) ,
	\label{eq:voto_random_forest}
\end{equation}
%%{eq:voto_random_forest}{Predicción final en Random Forest mediante voto mayoritario}

\noindent
donde $h_m(\bm{x})$ es la predicción del árbol $m$.

\textbf{Hiperparámetros}

\begin{itemize}
	\item \textbf{Criterio de Partición:} Función de impureza utilizada (e.g., Índice de Gini o Entropía de Shannon).
	\item \textbf{Algoritmo de Construcción:} 
	\textit{ID3} emplea la ganancia de información (entropía), mientras que \textit{CART} utiliza el índice de Gini y genera árboles binarios.
	\item \textbf{Número de Atributos Muestreados (RF):} 
	En Random Forest, típicamente se seleccionan $\sqrt{a}$ o $\ln(a)$ atributos por partición, donde $a$ es la cantidad total de atributos.
	\item \textbf{Número de Árboles (RF):} 
	Cantidad de árboles a construir en el bosque.
\end{itemize}

\section{Clasificador Naïve Bayes}

El \textbf{Clasificador Naïve Bayes}~\cite{hand2001idiot} (CNB) es un método supervisado probabilístico basado en el \textit{Teorema de Bayes}, que asume independencia condicional entre los atributos dado la clase, lo cual sabemos que no es posible que estas mismas variables sean independientes entre sí. La regla de clasificación, o la probabilidad de que se de una clase dado un registro,

\begin{equation}
	P(Y = C_k \mid X_1 = x_1, \ldots, X_d = x_d)
	= \frac{
		P(X_1 = x_1, \ldots, X_d = x_d \mid Y = C_k) \, P(Y = C_k)
	}{
		P(X_1 = x_1, \ldots, X_d = x_d)
	}.
	\label{eq:bayes_general}
\end{equation}
%%{eq:bayes_general}{Teorema de Bayes aplicado a clasificación}


\paragraph{Probabilidad Condicional:}



Bajo el supuesto de independencia condicional, la probabilidad condicional puede expresarse como

\begin{equation}
	P(X_1 = x_1, \ldots, X_d = x_d \mid Y = C_k)
	= \prod_{j=1}^{d} P(X_j = x_j \mid Y = C_k),
	\label{eq:independencia_condicional}
\end{equation}
%%{eq:independencia_condicional}{Factorización de la verosimilitud bajo independencia}

\noindent
en donde puede ser expresado para una clase $k$ cuando se presenta un registro $j$
\begin{equation}
	\hat{\theta}_{jmk} =
	P(X_j = x_{jm} \mid Y = C_k) =
	\frac{\#\{X_j = x_{jm} \wedge Y = C_k\}}{\#\{Y = C_k\}}.
	\label{eq:probabilidad_condicional}
\end{equation}
%%{eq:probabilidad_condicional}{Estimador de máxima verosimilitud para la probabilidad condicional}

\paragraph{Probabilidad a Priori:}



Otra estimación fundamental a calcular y expresar es la a priori, en la cual se asemeja a la probabilidad de que un nodo pertenezca a una clase $k$ en \textit{Random Forest}
\begin{equation}
	\hat{\pi}_k = P(Y = C_k) =
	\frac{\#\{Y = C_k\}}{N}.
	\label{eq:probabilidad_priori}
\end{equation}
%%{eq:probabilidad_priori}{Estimador de máxima verosimilitud para la probabilidad a priori}


%\noindent
%La clase predicha maximiza la probabilidad a posteriori:

%\begin{equation}
%	\hat{Y} = 
%	\underset{C_k}{\operatorname{argmax}}
%	\left[
%	P(Y = C_k)
%	\prod_{j=1}^{d} P(X_j = x_j \mid Y = C_k)
%	\right]
%	\label{eq:regla_clasificacion_nb}
%\end{equation}
%%{eq:regla_clasificacion_nb}{Regla de decisión de Naïve Bayes}


\textbf{Caso Continuo (Naïve Bayes Gaussiano)}

Cuando los atributos son continuos y se asume distribución normal, las verosimilitudes se estiman con la función de densidad Gaussiana:

\begin{equation}
	f(x) =
	\frac{1}{\sigma \sqrt{2\pi}}
	\exp \left[
	-\frac{1}{2} \left(
	\frac{x - \mu}{\sigma}
	\right)^2
	\right],
	\label{eq:densidad_gaussiana}
\end{equation}
%%{eq:densidad_gaussiana}{Función de densidad de probabilidad Gaussiana}

\noindent
por lo cual la decisión final se obtiene como

\begin{equation}
	\hat{Y} = 
	\underset{C_k}{\operatorname{argmax}}
	\left[
	\log P(Y = C_k) +
	\sum_{j=1}^{d}
	\log f(x_j \mid \mu_{jk}, \sigma_{jk}^2).
	\right]
	\label{eq:decision_gaussiana}
\end{equation}
%%{eq:decision_gaussiana}{Regla de decisión para Naïve Bayes Gaussiano}

\section{Máquinas de Soporte Vectorial (SVM)}

Las \textbf{Máquinas de Soporte Vectorial}~\cite{Cortes1995} (SVM, por sus siglas en inglés) constituyen una técnica de clasificación supervisada basada en la búsqueda de una \textbf{función de decisión} que permita predecir la clase de una observación a partir de sus atributos. Dado que este método opera sobre variables numéricas, los atributos categóricos deben codificarse previamente.\\

\noindent
El objetivo fundamental de una SVM es encontrar un \textbf{hiperplano de separación óptimo} que divida los datos en función de sus clases, \textbf{maximizando el margen} $M$, es decir, la distancia mínima entre el hiperplano y los puntos más cercanos de cada clase (denominados \textbf{vectores de soporte}). Dichos vectores determinan la posición y orientación del hiperplano, por lo que son los únicos puntos relevantes en el entrenamiento del modelo.\\

\noindent
Aunque existen infinitos hiperplanos que pueden separar los datos, el principio de las SVM consiste en seleccionar aquel que logre la \textbf{máxima separación posible entre las clases}, minimizando simultáneamente el riesgo de sobreajuste y aumentando la capacidad de generalización.\\

\textbf{Margen Rígido (Hard Margin)}

El caso de \textbf{margen rígido} supone que los datos son linealmente separables, es decir, existe un hiperplano que separa perfectamente las clases sin errores de clasificación. En este caso, el problema de optimización se formula como:

\begin{equation}
	\text{Minimizar }
	\frac{1}{2} |\bm{w}|^2
	\quad \text{sujeto a }
	y_i (\langle \bm{w}, \bm{x}_i \rangle + \beta) \ge 1 ,
	\label{eq:hard_margin}
\end{equation}

\noindent
donde $ \bm{w} $ es el vector normal al hiperplano, $\beta$ es el término de sesgo, y la restricción garantiza que todas las observaciones queden correctamente clasificadas, y a una distancia mínima de $\frac{1}{\lVert \bm{w} \rVert}$ del hiperplano.

\textbf{Margen Suave (Soft Margin)}

En la práctica, los datos rara vez son perfectamente separables. Por ello, se introduce el concepto de \textbf{margen suave}, que permite violaciones controladas de las restricciones mediante variables de holgura $ \theta_i \ge 0 $. El problema se redefine como:

\begin{equation}
	\text{Minimizar }
	\frac{1}{2} |\bm{w}|^2 +
	C \sum_{i=1}^n \xi_i
	\quad \text{sujeto a }
	\begin{cases}
		y_i(\langle \bm{w}, \bm{x}_i \rangle + \beta) \ge 1 - \xi_i
		\xi_i \ge 0,
	\end{cases}
	\label{eq:soft_margin}
\end{equation}

\noindent
donde el parámetro $C > 0$ actúa como un \textbf{control de regularización}: valores grandes de $C$ penalizan más fuertemente los errores, buscando una separación más estricta (a costa de menor margen), mientras que valores pequeños permiten más errores, favoreciendo márgenes amplios y mayor generalización.

\textbf{Formulación Dual y Kernel Trick}

La formulación dual del problema permite expresar la solución en términos de los productos internos entre las observaciones:

\begin{equation}
	\text{Maximizar }
	-\frac{1}{2} \sum_{i,\ell=1}^{n}
	\alpha_i \alpha_\ell y_i y_\ell
	K(\bm{x}i, \bm{x}\ell)
	+ \sum_{i=1}^{n} \alpha_i,
	\label{eq:dualsvm}
\end{equation}

\begin{equation}
	\text{sujeto a }
	0 \le \alpha_i \le C, \quad
	\sum_{i=1}^{n} \alpha_i y_i = 0.
\end{equation}

\noindent
Aquí, los \(\alpha_i\) son los multiplicadores de Lagrange asociados a las restricciones, y \(K(\bm{x}_i, \bm{x}_\ell)\) representa el producto interno entre las observaciones en un espacio transformado.\\


\noindent
En muchos casos, las clases no son separables linealmente en el espacio original de los datos. El \textbf{Kernel Trick} permite proyectar los datos a un espacio de mayor dimensión (posiblemente infinito) donde sí exista un hiperplano separador, \emph{sin necesidad de calcular explícitamente la transformación}.

\noindent
Esto se logra sustituyendo los productos internos \(\langle \bm{x}_i, \bm{x}_\ell \rangle\) por una función \textbf{kernel} \(K(\bm{x}_i, \bm{x}_\ell)\), que computa directamente el producto interno en el espacio transformado.
De esta forma, se mantiene la eficiencia computacional mientras se obtiene un modelo capaz de representar fronteras de decisión no lineales.

\subsubsection{Funciones Kernel Comunes}

\paragraph{Kernel Lineal:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\langle \bm{a}, \bm{b} \rangle.
	\label{eq:kernel_lineal}
\end{equation}

\paragraph{Kernel Radial (RBF o Gaussiano):}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\exp \left( -\gamma \|\bm{a} - \bm{b}\|^2 \right).
	\label{eq:kernel_rbf}
\end{equation}

\paragraph{Kernel Polinómico:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	(\langle \bm{a}, \bm{b} \rangle + r)^d.
	\label{eq:kernel_polinomico}
\end{equation}

\paragraph{Kernel Sigmoide:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\tanh(\gamma \langle \bm{a}, \bm{b} \rangle + r).
	\label{eq:kernel_sigmoid}
\end{equation}


\textbf{Hiperparámetros}

\begin{itemize}
	\item \textbf{Parámetro de Regularización (\(C\)):} Controla el equilibrio entre la maximización del margen y la penalización por errores de clasificación.
	
	\item \textbf{Tipo de Kernel:} Define la forma de la frontera de decisión (Lineal, Polinómico, Radial o Sigmoideo).
	
	\item \textbf{Parámetros del Kernel:} Por ejemplo, \(\gamma\) o \(\sigma^2\) en el kernel RBF; grado \(d\) y coeficiente \(r\) en el kernel polinómico.
\end{itemize}











\chapter{Resultados}
\label{ch:resultados}

En este capítulo se presentan los resultados obtenidos mediante la aplicación de los modelos de aprendizaje supervisado sobre los distintos conjuntos de datos. Se analizan las métricas de evaluación alcanzadas, las configuraciones óptimas halladas mediante búsqueda en malla (\emph{Grid Search}) y la importancia relativa de las características más influyentes en las predicciones.



En esta sección se presentan los resultados obtenidos tras la aplicación de distintos modelos de aprendizaje automático sobre los conjuntos de datos binario y multiclase seleccionados. Se buscó evaluar el rendimiento de cada modelo bajo diferentes configuraciones de hiperparámetros, a través de métricas como la precisión (\textit{Accuracy}), el F1-Score y el área bajo la curva ROC (ROC AUC), entre otras.\\

\noindent
Durante la fase experimental se exploraron distintas estrategias de balanceo de clases, dado que varios de los conjuntos presentaban desbalances significativos entre las clases. En particular, se probó la técnica de \textbf{undersampling}, reduciendo la cantidad de ejemplos de la clase mayoritaria para equilibrar el \emph{datasets}. Sin embargo, esta estrategia no arrojó resultados satisfactorios: los modelos tendieron a perder capacidad de generalización, mostrando un descenso notable en las métricas de validación, aunque se sostuvo la mejor configuración con mejor valor de métricas. Por este motivo, se optó finalmente por mantener la distribución original y aplicar técnicas de regularización y ajuste de hiperparámetros para mitigar el sesgo hacia la clase dominante.\\

\noindent
A continuación se presentan las métricas finales alcanzadas por cada modelo y los valores óptimos de los hiperparámetros encontrados mediante \textit{Grid Search}. Posteriormente, se analiza la importancia de las características más relevantes en la predicción de las clases.

\section{Métricas de Evaluación}

A continuación, se detallan las principales métricas obtenidas, junto con los hiperparámetros evaluados mediante \textbf{Grid Search} y las configuraciones óptimas seleccionadas.
\subsection{Dataset Binario}

\subsubsection{Regresión Logística}

La Regresión Logística obtuvo un desempeño correcto en la clasificación binaria.\\

\noindent
La mejor configuración se alcanzó utilizando un valor de regularización $C = 1$, sin penalización (\textit{Penalty = l1}), con el solver \textit{saga} y estrategia \textit{ovr} (one-vs-rest) para el tratamiento multiclase.\\

\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del modelo de Regresión Logística}
	\label{tab:rl_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Penalty = l1\\
			Solver = saga\\
			Multiclass = ovr
		} & 0.84 & 0.84 & 0.84 & 0.90 & 0.13 & 0.84 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rl_resultados_binario}. 
El modelo logra una precisión y un F1 Score de 0.84 y 0.84 respectivamente, junto con un AUC de 0.90, lo que refleja una buena capacidad discriminatoria, pero siendo datos médicos se requeriría incluso mejores valores.\\

\noindent 
El tiempo de entrenamiento fue de apenas 0.13 segundos, lo que lo convierte en una opción eficiente para este tipo de problema.


\begin{table}[htb]%[H]
	\centering
	\caption{Grid de hiperparámetros - Regresión Logística (binario)}
	\label{tab:grid_rl_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0, 0.1, 0.01] \\
		Penalty & [None, l1, l2, elasticnet] \\
		Solver & [lbfgs, saga, newton-s] \\
		Multiclass & [ovr, multinomial] \\
		\bottomrule
	\end{tabular}
\end{table}


\subsubsection{Máquinas de Soporte Vectorial (SVM)}

La Máquinas de Soporte Vectorial obtuvo un desempeño sólido en la clasificación binaria, mostrando un equilibrio adecuado entre precisión y generalización.\\

\noindent
La mejor configuración se alcanzó utilizando un valor de regularización $C = 1$, con kernel radial (\textit{Kernel = rbf},), con un gamma de 0.1 (\textit{Gamma = 0.1})\\


\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del SVM}
	\label{tab:svm_resultados_binario}
	\begin{tabular}{lcccccc}
	\toprule
	\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Kernel = rbf\\
			Gamma = 0.1\\
		} & 0.86 & 0.86 & 0.86 &0.92 &0.60 & 0.86 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:svm_resultados_binario}. 
El modelo logra una precisión y un F1 Score de 0.86 y 0.86 respectivamente, junto con un AUC de 0.92, lo que refleja una mejor capacidad discriminativa. 
El tiempo de entrenamiento fue de apenas 0.60 segundos, siendo un gran valor.


\begin{table}[htb]%[H]
	\centering
	\caption{Grid de hiperparámetros - SVM (binario)}
	\label{tab:grid_svm_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0.001, 0.01, 0.1, 1, 10, 15, 20, 25] \\
		Kernel & [linear, poly, rbf, sigmoid] \\
		Gamma & [scale, auto, 0.001, 0.01, 0.1, 1] \\
		Degree & [2--10] \\
		\bottomrule
	\end{tabular}
\end{table}


\subsubsection{Naive Bayes Gaussiano}

Naive Bayes Gaussiano obtuvo un buen desempeño en la clasificación multiclase, considerando su supuesto de independencia entre atributos. \\

\noindent
La mejor configuración se alcanzó con cualquier suavizado, no hubo diferencias\\


\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del Naive Bayes Gaussiano}
	\label{tab:nbg_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			var\_smoothing = Cualquiera\\
		} & 0.84 & 0.84 & 0.84 & 0.91 & 0.10 & 0.84 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:nbg_resultados_binario}. 
El modelo logra una precisión y un F1 Score de 0.84 y 0.84 respectivamente, junto con un AUC de 0.91, lo que refleja una buena capacidad discriminativa, bastante similar con los resultados de los modelos a los cuales se compara.\\

\noindent 
El tiempo de entrenamiento fue de apenas 0.10 segundos, siendo su mayor fortaleza, la rapidez de su entrenamiento.

\begin{table}[htb]%[H]
	\centering
	\caption{Grid de hiperparámetros - Naive Bayes Gaussiano (binario)}
	\label{tab:grid_nb_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Suavizado & Variaciones de suavizado \\
		\bottomrule
	\end{tabular}
\end{table}


\subsubsection{Random Forest}

Random Forest demostró un desempeño sobresaliente en la clasificación multiclase, mostrando alta capacidad de generalización para registros no vistos durante el entrenamiento.\\

\noindent 
La mejor configuración se obtuvo utilizando el criterio de \textit{Entropía de Shannon}, profundidad de 7 hojas los árboles, división mínima de 7 ejemplos por nodo, hoja mínima de 1 ejemplo y seleccionando la cantidad de atributos mediante la raíz cuadrada.\\


\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales del Random Forest}
	\label{tab:rf_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Criterion = entropy\\
			Max Depth = 7\\
			Min Samples Split = 5\\
			Min samples Leaf = 1\\
			Max Features = sqrt\\
		} & 0.87 & 0.87 & 0.87 & 0.93 & 1.38 & 0.87 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rf_resultados_binario}.\\

\noindent 
El modelo logra una precisión y un F1 Score de 0.87 y 0.87 respectivamente, junto con un AUC de 0.93, lo que refleja una buena excelente capacidad discriminatoria.\\

\noindent 
El tiempo de entrenamiento fue de apenas 1.38 segundos, siendo considerablemente el modelo en cual más se tarda en obtener los resultados.



\begin{table}[htb]%[H]
	\centering
	\caption{Grid de hiperparámetros - Random Forest (binario)}
	\label{tab:grid_rf_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Criterion & [gini, entropy] \\
		Max Depth & [None, 3, 5, 7, 9] \\
		Min Samples Split & [2, 5, 10] \\
		Min Samples Leaf & [1, 2, 4] \\
		Max Features & [None, sqrt, log2] \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{figure}[htb]%[H]
	\centering
	\includegraphics[width=1\textwidth]{../mejores_modelos_barras_binario}
	\caption{Comparación de desempeño de los mejores modelos (Binario)}
	\label{fig:mejores_modelos_binario}
\end{figure}

\noindent
En la Figura \ref{fig:mejores_modelos_binario} se observa que Random Forest obtiene la mayor puntuación en todas las métricas, seguido por la Regresión Logística y SVM. Esto indica que, para nuestro \emph{datasets}, el modelo de Random Forest presenta mejor capacidad de generalización, mientras que los demás modelos muestran un desempeño bastante competitivo.

\textbf{Importancia de las Características}
\noindent La importancia de las características se calculó mediante el método de \textit{Permutation Feature Importance}. Este método evalúa cuánto se degrada el desempeño del modelo cuando se altera aleatoriamente una característica, manteniendo fijas las demás. Cuanto mayor sea la disminución en la métrica de desempeño, mayor será la importancia atribuida a dicha característica.\\


\noindent Los resultados obtenidos se presentan en las Tablas~\ref{tab:rf_importancia},~\ref{tab:rl_importancia_},~\ref{tab:nb_importancia} y ~\ref{tab:svm_importancia} correspondientes a los modelos RF, RL, NB y SVM.


\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (RF)}
	\label{tab:rf_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ST\_Slope & 0.254265 \\
ChestPainType & 0.127319 \\
Oldpeak & 0.113156 \\
ExerciseAngina & 0.105952 \\
Cholesterol & 0.099872 \\
MaxHR & 0.088635 \\
Age & 0.065807 \\
RestingBP & 0.055053 \\
Sex & 0.040916 \\
FastingBS & 0.030069 \\
RestingECG & 0.018956 \\
		\bottomrule
	\end{tabular}
\end{table}



\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (RL)}
	\label{tab:rl_importancia_}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ST\_Slope & 0.072440 \\
ExerciseAngina & 0.026797 \\
ChestPainType   &  0.020806\\
Sex           &    0.011329\\
FastingBS      &   0.010240\\
Cholesterol    &   0.009150\\
Oldpeak        &   0.006100\\
Age            &   0.003922\\
MaxHR          &   0.003268\\
RestingBP      &   0.000545\\
RestingECG     &   0.000218\\
		\bottomrule
	\end{tabular}
\end{table}



\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (SVM)}
	\label{tab:svm_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ST\_Slope     &     0.106209\\
Cholesterol    &   0.031808\\
Oldpeak        &   0.024510\\
ChestPainType  &   0.023312\\
Sex            &   0.011438\\
MaxHR           &  0.008715\\
ExerciseAngina  &  0.008388\\
Age            &   0.007952\\
RestingBP       &  0.005773\\
RestingECG      &  0.004902\\
FastingBS      &   0.004575\\
		\bottomrule
	\end{tabular}
\end{table}




\begin{table}[H]
	\centering
	\caption{Importancia de las características según permutación (NB)}
	\label{tab:nb_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ST\_Slope & 0.027015 \\
ExerciseAngina & 0.023747 \\
Oldpeak & 0.018736 \\
ChestPainType & 0.018519 \\
Cholesterol & 0.014815 \\
Sex & 0.014270 \\
FastingBS & 0.004575 \\
RestingBP & 0.001852 \\
MaxHR & -0.000218 \\
RestingECG & -0.001198 \\
Age & -0.003595 \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_binario}
	\caption{Evolución de Accuracy (Binario)}
	\label{fig:evolucion_metricas_binario}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_binario_auc}
	\caption{Evolución de ROC AUC (Binario)}
	\label{fig:evolucion_metricas_binario_auc}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_binario_f1}
	\caption{Evolución de F1-Score (Binario)}
	\label{fig:evolucion_metricas_binario_f1}
\end{figure}



\noindent
La Figuras \ref{fig:evolucion_metricas_binario}, \ref{fig:evolucion_metricas_binario_auc} y \ref{fig:evolucion_metricas_binario_f1}  muestran cómo las métricas del modelo mejoran al incorporar las características más relevantes según su importancia. Se observa que inicialmente, con pocas características, el desempeño es limitado, siendo prudente incrementar la cantidad de características. En algunos casos es muy sorprendentemente, ya que se obtiene un buen resultado, y a medida que se agregan las variables de mayor relevancia, las métricas tienden a incrementarse hasta estabilizarse. Este comportamiento permite identificar el conjunto de características que maximiza el rendimiento sin necesidad de incluir todas las variables disponibles, optimizando tanto la complejidad del modelo como el tiempo de entrenamiento.\\

\noindent
Siendo los modelos \textbf{SVM} y \textbf{RF} los que mejor comportamiento tienen a lo largo del incremento de características, donde podemos ver en su gran mayoría un incremento del valor de las métricas a medida que se aumentan las caracterizaras. Ademas, como se puede observar, tienen valores aceptables para pocas características.


\subsection{Dataset Multiclase}

\textbf{Resultados del modelo de Regresión Logística}

La Regresión Logística obtuvo un desempeño sólido en la clasificación multiclase, mostrando un equilibrio adecuado entre precisión y generalización.\\

\noindent
La mejor configuración se alcanzó utilizando un valor de regularización $C = 1$, sin penalización (\textit{Penalty = None}), con el solver \textit{newton-cg} y estrategia \textit{ovr} (one-vs-rest) para el tratamiento multiclase.\\

\begin{table}[H]
	\centering
	\caption{Resultados finales del modelo de Regresión Logística}
	\label{tab:rl_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Penalty = None\\
			Solver = newton-cg\\
			Multiclass = ovr
		} & 0.89 & 0.89 & 0.89 & 0.96 & 0.30 & 0.89 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rl_resultados}. 
El modelo logra una precisión y un F1 Score de 0.897 y 0.896 respectivamente, junto con un AUC de 0.964, lo que refleja una buena capacidad discriminativa. 
El tiempo de entrenamiento fue de apenas 0.30 segundos, lo que lo convierte en una opción eficiente para este tipo de problema.
\begin{table}[H]
	\centering
	\caption{Grid de hiperparámetros - Regresión Logística (multiclase)}
	\label{tab:grid_rl_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0.01, 0.1, 1, 10] \\
		Penalty & [None, l2, elasticnet] \\
		Solver & [lbfgs, saga, newton-s] \\
		Multiclass & [ovr] \\
		\bottomrule
	\end{tabular}
\end{table}



\subsubsection{Máquinas de Soporte Vectorial (SVM)}

La Máquinas de Soporte Vectorial obtuvo un desempeño sólido en la clasificación multiclase, mostrando un equilibrio adecuado entre precisión y generalización. \\

\noindent
La mejor configuración se alcanzó utilizando un valor de regularización $C = 0.1$, con kernel polinomico de segundo grado (\textit{Kernel = Poly}, \textit{Degree = 2}), con un gamma de 1 (\textit{Gamma = 1})\\


\begin{table}[H]
	\centering
	\caption{Resultados finales del SVM}
	\label{tab:svm_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
		$C = 0.1$\\
			Kernel = poly\\
			Gamma = 1\\
			Degree = 2\\
		} & 0.90 & 0.89 & 0.89 & 0.96 & 1.45 & 0.89 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:svm_resultados}. 
El modelo logra una precisión y un F1 Score de 0.90 y 0.89 respectivamente, junto con un AUC de 0.96, lo que refleja una buena capacidad discriminativa. 
El tiempo de entrenamiento fue de apenas 1.42 segundos..



\begin{table}[H]
	\centering
	\caption{Grid de hiperparámetros - SVM (multiclase)}
	\label{tab:grid_svm_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & [0.001, 0.01, 0.1, 1, 10, 15, 20, 25] \\
		Kernel & [linear, poly, rbf, sigmoid] \\
		Gamma & [scale, auto, 0.001, 0.01, 0.1, 1] \\
		Degree & [2--10] \\
		\bottomrule
	\end{tabular}
\end{table}



\subsubsection{Naive Bayes Gaussiano}


Naive Bayes Gaussiano obtuvo un buen desempeño en la clasificación multiclase, considerando su supuesto de independencia entre atributos. \\

\noindent
La mejor configuración se alcanzó con cualquier suavizado, no hubo diferencias\\


\begin{table}[H]
	\centering
	\caption{Resultados finales del Naive Bayes Gaussiano}
	\label{tab:nbg_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			var\_smoothing = Cualquiera\\
		} & 0.82 & 0.82 & 0.83 & 0.91 & 0.11 & 0.87 \\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:nbg_resultados}. 
El modelo logra una precisión y un F1 Score de 0.82 y 0.83 respectivamente, junto con un AUC de 0.91, lo que refleja una buena capacidad discriminativa, pero lo suficientemente menor a los modelos con los cuales se compara. 
El tiempo de entrenamiento fue de apenas 0.11 segundos, siendo su mayor fortaleza, la rapidez de su entrenamiento.


\begin{table}[H]
	\centering
	\caption{Grid de hiperparámetros - Naive Bayes Gaussiano (multiclase)}
	\label{tab:grid_nb_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Suavizado & Variaciones de suavizado \\
		\bottomrule
	\end{tabular}
\end{table}


\subsubsection{Random Forest}


Random Forest demostró un desempeño sobresaliente en la clasificación multiclase, mostrando alta capacidad de generalización para registros no vistos durante el entrenamiento.\\

\noindent 
La mejor configuración se obtuvo utilizando el criterio de \textit{Gini}, profundidad ilimitada de los árboles, división mínima de 2 ejemplos por nodo, hoja mínima de 1 ejemplo y seleccionando la cantidad de atributos mediante la raíz cuadrada.


\begin{table}[H]
	\centering
	\caption{Resultados finales del Random Forest}
	\label{tab:rf_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{Precisión (Acc)} & \textbf{Recall} & \textbf{F1 Score} & \textbf{ROC AUC} & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Criterion = gini\\
			Max Depth = None\\
			Min Samples Split = 2\\
			Min samples Leaf = 1\\
			Max Features = sqrt\\
		} & 0.94 & 0.94 &0.94 & 0.98 & 2.24& 0.94\\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rf_resultados}. 
El modelo logra una precisión y un F1 Score de 0.94 y 0.94 respectivamente, junto con un AUC de 0.98, lo que refleja una buena excelente capacidad discriminativa. 
El tiempo de entrenamiento fue de apenas 2.24 segundos..


\begin{table}[htb]
	\centering
	\caption{Grid de hiperparámetros - Random Forest (multiclase)}
	\label{tab:grid_rf_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Criterion & [gini, entropy] \\
		Max Depth & [None, 3, 5, 7, 9] \\
		Min Samples Split & [2, 5, 10] \\
		Min Samples Leaf & [1, 2, 4] \\
		Max Features & [None, sqrt, log2] \\
		\bottomrule
	\end{tabular}
\end{table}

\FloatBarrier
\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../mejores_modelos_barras_multiclase}
	\caption{Comparación de desempeño de los mejores modelos (Multiclase)}
	\label{fig:mejores_modelos_multiclase}
\end{figure}
\FloatBarrier
\noindent
En la Figura \ref{fig:mejores_modelos_multiclase} se observa que Random Forest obtiene la mayor puntuación en todas las métricas, seguido por la Regresión Logística y SVM. Esto indica que, para nuestro \emph{datasets}, el modelo de Random Forest presenta mejor capacidad de generalización, mientras que los demás modelos muestran un desempeño bastante competitivo.



\textbf{Importancia de las Características}
\noindent La importancia de las características se calculó mediante el método de \textit{Permutation Feature Importance}. Este método evalúa cuánto se degrada el desempeño del modelo cuando se altera aleatoriamente una característica, manteniendo fijas las demás. Cuanto mayor sea la disminución en la métrica de desempeño, mayor será la importancia atribuida a dicha característica.\\




\noindent Los resultados obtenidos se presentan en las Tablas~\ref{tab:rf_importancia_multiclase},~\ref{tab:rl_importancia_multiclase},~\ref{tab:nb_importancia_multiclase} y ~\ref{tab:svm_importancia_multiclase} correspondientes a los modelos RF, RL, NB y SVM.

\begin{table}[htb]
	\centering
	\caption{Importancia de las características según permutación (RF)}
	\label{tab:rf_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
	ASTV      &  0.139807\\
	ALTV      &  0.109941\\
	MSTV      &  0.104823\\
	Mean      &  0.091579\\
	AC        &  0.063645\\
	Mode       & 0.061986\\
	Median      &0.060633\\
	DP        &  0.047945\\
	LB        &  0.045324\\
	MLTV      &  0.045132\\
	Variance  &  0.040531\\
	UC        &  0.039166\\
	Width     &  0.030551\\
	Min       &  0.030109\\
	Max       &  0.027147\\
	FM        &  0.020801\\
	Nmax      &  0.018407\\
	DL        &  0.011128\\
	Tendency  &  0.007652\\
	Nzeros    &  0.003405\\
	DS        &  0.000287\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[htb]
	\centering
	\caption{Importancia de las características según permutación (RL)}
	\label{tab:rl_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
Mean    &    0.098487\\
AC       &   0.084113\\
ASTV     &   0.057069\\
Median   &   0.031631\\
DP       &   0.029740\\
LB       &   0.023404\\
Variance &   0.022270\\
UC       &   0.022080\\
ALTV     &   0.019243\\
Max      &   0.018109\\
Nmax     &   0.014374\\
Mode     &   0.011348\\
Min      &   0.005910\\
MSTV     &   0.004208\\
FM       &   0.003830\\
Tendency &   0.003546\\
MLTV     &   0.002979\\
Nzeros   &   0.002837\\
DL       &   0.001655\\
Width    &   0.000189\\
DS       &   0.000000\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (SVM)}
	\label{tab:svm_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
ASTV     &   0.050355\\
ALTV     &   0.037069\\
UC       &   0.030638\\
AC       &   0.026903\\
DP       &   0.018345\\
Mean     &   0.015887\\
Mode     &   0.014988\\
Median   &   0.014043\\
Nmax     &   0.011915\\
MSTV     &   0.009125\\
DL       &   0.005059\\
Variance &   0.004775\\
Nzeros   &   0.004586\\
Min      &   0.004444\\
Max      &   0.004350\\
MLTV     &   0.003357\\
Tendency &   0.003026\\
FM       &   0.002459\\
Width    &   0.001418\\
DS       &   0.000000\\
LB       &  -0.000804\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características según permutación (Naive Bayes Gaussiano)}
	\label{tab:nb_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		AC       & 0.057163 \\
		DP       & 0.018676 \\
		ALTV     & 0.015461 \\
		ASTV     & 0.005106 \\
		DS       & 0.002695 \\
		UC       & 0.002364 \\
		FM       & 0.001371 \\
		Variance & 0.001087 \\
		Nzeros   & 0.001040 \\
		Nmax     & -0.000993 \\
		Tendency & -0.001040 \\
		Mode     & -0.001324 \\
		Max      & -0.001371 \\
		Min      & -0.001986 \\
		LB       & -0.001986 \\
		MLTV     & -0.002222 \\
		Width    & -0.002459 \\
		Median   & -0.003310 \\
		MSTV     & -0.004965 \\
		Mean     & -0.005768 \\
		DL       & -0.006809 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_multiclase}
	\caption{Evolución de Accuracy (Multiclase)}
	\label{fig:evolucion_metricas_multiclase}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_multiclase_auc}
	\caption{Evolución de ROC AUC (Multiclase)}
	\label{fig:evolucion_metricas_multiclase_auc}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=1\textwidth]{../evolucion_metrica_multiclase_F1}
	\caption{Evolución de F1-score (Multiclase)}
	\label{fig:evolucion_metricas_multiclase_f1}
\end{figure}


\noindent
Las Figuras \ref{fig:evolucion_metricas_multiclase}, \ref{fig:evolucion_metricas_multiclase_f1} y \ref{fig:evolucion_metricas_multiclase_auc} muestran cómo las métricas del modelo mejoran al incorporar las características más relevantes según su importancia. Se observa que inicialmente, con pocas características, el desempeño es menor al obtenido anteriormente, pero mucho mejor del esperable. En algunos casos, de forma muy sorprendentemente, se obtiene un buen resultado ya a los pocos atributos utilizados, y a medida que se agregan las variables de mayor relevancia, las métrica tienden a incrementarse hasta estabilizarse.\\

\noindent
Este comportamiento permite identificar el conjunto de características que maximiza el rendimiento sin necesidad de incluir todas las variables disponibles, optimizando tanto la complejidad del modelo como el tiempo de entrenamiento.





\chapter{Conclusiones}

\section{Análisis General e Inferencias}

Del análisis de los resultados obtenidos se puede observar que el modelo \textbf{Random Forest} alcanzó reiteradamente los mejores valores en todas las métricas, tanto en el problema binario como en el multiclase. Esto se debe a su capacidad de combinar múltiples árboles de decisión, lo que permite capturar relaciones no lineales y reducir el sobreajuste, sobretodo a la hora de hacer un voto mayoritario de estos mismos arboles que permiten una representación mejor.\\

\noindent
El modelo de \textbf{SVM} mostró también un rendimiento muy bueno, especialmente con el kernel \textbf{RBF} en el caso binario y el \textbf{polinómico} en el caso multiclase, destacándose su capacidad para definir fronteras de decisión complejas en espacios transformados.\\

\noindent
\textbf{Regresión Logística} presentó resultados positivos y de buena generalización, aunque con menor capacidad para capturar patrones que los otros modelos, no por ser malos resultados, sino que el resto tuvo mejores valores de métricas. Por su parte, el modelo \textbf{Naive Bayes} ofreció un rendimiento aceptable, siendo el más liviano computacionalmente, aunque con limitaciones inherentes a su supuesto de independencia de las variables. Esto no quita que aunque posea este supuesto, es el más liviano y rápido de los modelos obteniendo resultados sumamente buenos.\\

\noindent
En cuanto a la \textbf{importancia de las características}, se identificaron atributos dominantes en cada conjunto de datos. En el binario, variables como \textit{ST\_Slope}, \textit{ChestPainType} y \textit{Oldpeak} fueron recurrentemente relevantes; mientras que en el multiclase destacaron \textit{ASTV}, \textit{ALTV} y \textit{MSTV}.

\section{Mejoras Potenciales y Consideraciones}

Para optimizar aún más las métricas, podrían explorarse las siguientes estrategias:

\begin{itemize}
	\item \textbf{Ajuste más fino de hiperparámetros:} empleando \emph{Randomized Search} o \emph{Bayesian Optimization} para reducir tiempos de búsqueda, y luego utlizar un \emph{Grid Search} en los hiperparámetros encontrados.
	\item \textbf{Manipulación de características:} Reducción de dimensionalidad (PCA) o creación de variables sintéticas, para observar mejor las importancias de cada característica.
	\item \textbf{Validación cruzada más robusta:} utilizando más particiones para estimar mejor la generalización.
\end{itemize}

En conjunto, los modelos demostraron un desempeño satisfactorio, con un claro potencial de mejora mediante el refinamiento de hiperparámetros y una mejor comprensión de la estructura de los datos.

\bibliographystyle{plain}
\bibliography{Referencias}

\end{document}