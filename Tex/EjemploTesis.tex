\documentclass[a4paper,10pt]{book}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}
%\usepackage{mathtools}makeglossaries %.tex
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage[spanish, es-tabla]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{incgraph,tikz}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
%\usepackage{babel}
\usepackage{color}
\usepackage{listings}
\usepackage{float}
\usepackage{tikz}
\usepackage{adjustbox}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{titlepic}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{etoolbox}
%\usepackage{cite}    
\usepackage{bm}
\usepackage{makecell}
\usepackage{placeins}



\usepackage{listings}
\lstset{ %
	basicstyle=\footnotesize,       % the size of the fonts that are used for the code
	numbers=none,                   % where to put the line-numbers
	numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
	stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
	numbersep=10pt,                  % how far the line-numbers are from the code
	backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
	showspaces=false,               % show spaces adding particular underscores
	showstringspaces=false,         % underline spaces within strings
	showtabs=false,                 % show tabs within strings adding particular underscores
	frame=single,           % adds a frame around the code
	tabsize=4,          % sets default tabsize to 2 spaces
	captionpos=b,           % sets the caption-position to bottom
	breaklines=true,        % sets automatic line breaking
	breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
	escapeinside={\%*}{*)}          % if you want to add a comment within your code
}



\usetikzlibrary{arrows}

\captionsetup[subfigure]{subrefformat=simple,labelformat=simple}

\renewcommand{\contentsname}{\'Indice General}
\renewcommand{\listfigurename}{\'Indice de Figuras}
\renewcommand{\listtablename}{\'Indice de Tablas}
\renewcommand{\lstlistingname}{Salida}
\renewcommand{\tablename}{Tabla}
\renewcommand{\figurename}{Figura}
\renewcommand\thesubfigure{(\alph{subfigure})}
\renewcommand{\baselinestretch}{1.2} 

\makeatletter
\def\verbatim{\small\@verbatim \frenchspacing\@vobeyspaces \@xverbatim}
\makeatother

\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}

\fancyhead[LE]{\nouppercase{\leftmark}}
\fancyhead[RO]{\nouppercase{\leftmark}}

\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0.4pt}



%\restylefloat{table}


\definecolor{TPcolor}{HTML}{A9D18E} % Verde claro para TP
\definecolor{TNcolor}{HTML}{D9EAD3} % Verde muy claro para TN
\definecolor{FPcolor}{HTML}{F4CCCC} % Rojo muy claro para FP
\definecolor{FNcolor}{HTML}{E06666} % Rojo claro para FN

\title{Comparación de Técnicas de Aprendizaje
	Automático Supervisado Aplicadas a Datos Cardiólogos}
\author{Autor: Nicolás Seivane \\ Tutora: Andrea Rey}
\date{Universidad Nacional de Hurlingham (UNAHUR)}
\titlepic{\vspace{12cm}\includegraphics[width=0.15\textwidth]{logo.jpg}}

\usepackage{titlesec}

% Chapters justificados
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\justifying} % estilo del capítulo
{\chaptername\ \thechapter}{20pt}{\Huge}

% Sections justificados
\titleformat{\section}
{\normalfont\Large\bfseries\justifying} % estilo de la sección
{\thesection}{1em}{}

% Subsections justificados
\titleformat{\subsection}
{\normalfont\large\bfseries\justifying}
{\thesubsection}{1em}{}







\begin{document}
	
	
	\maketitle
	
	
	\tableofcontents
	
	\addcontentsline{toc}{chapter}{Índice de Figures}
	\renewcommand{\listfigurename}{Índice de Figuras}
	\listoffigures
	
	\addcontentsline{toc}{chapter}{Índice de Tablas}
	\renewcommand{\listtablename}{Índice de Tablas}
	\listoftables
	
	
	\chapter*{Glosario}
	\addcontentsline{toc}{chapter}{Glosario}
	
	\begin{longtable}{p{3cm} | p{11cm}}
		\toprule
		
		\multicolumn{2}{l}{\textbf{Notación general y datos}}\\
		\midrule
		$n$ & Número total de registros o ejemplos en el \emph{dataset}. \\
		$D$ & Cantidad de atributos de cada registro. \\
		$L$ & Número total de clases posibles en el problema de clasificación. \\
		$\bm{x}_i$ & Vector de características del registro $i$-ésimo, $\bm{x}_i = (x_{i1}, x_{i2},\dots, x_{iD})$. \\
		$X_j$ & $j$-ésimo atributo de un registro. \\
		$x_{ij}$ & Valor del atributo $X_j$ para el registro $i$-ésimo. \\
		$y_i$ & Clase real (etiqueta verdadera) del registro $i$-ésimo. \\
		$\widehat{y}_i$ & Clase predicha para el registro $i$-ésimo. \\
		$C_l$ & $l$-ésima clase del conjunto de clases $\{C_1, C_2,\dots, C_L\}$. \\
		$|C_l|$ & Cardinalidad o número de registros pertenecientes a la clase $l$-ésima. \\
		$\widehat{y}$ & Predicción final en un modelo de clasificación. \\
		
		\toprule
		\multicolumn{2}{l}{\textbf{Naïve Bayes (Gaussiano)}}\\
		\midrule
		$\widehat{\pi}_l$ & Estimación de la probabilidad \emph{a priori} de que un registro pertenezca a la clase $C_l$. \\
		$\widehat{\theta}_{jvl}$ & Estimación de la probabilidad estimada de que el atributo $X_j$ tome el valor $x_{jv}$ dado que la clase es $C_l$. \\
		%$f(x_j\mid \mu_{jl},\sigma_{jl}^2)$ & Densidad gaussiana condicional usada en Naïve Bayes para atributos continuos. \\
		$\mu_{jl}$ & Media del atributo $X_j$ en la clase $C_l$. \\
		$\sigma_{jl}^2$ & Varianza del atributo $X_j$ en la clase $C_l$. \\
		%$\text{var\_smoothing}$ & Hiperparámetro de suavizado numérico en Naïve Bayes gaussiano. \\
		\toprule
		\multicolumn{2}{l}{\textbf{Regresión logística}}\\
		\midrule
		$\bm{\beta}$ & Vector de coeficientes en regresión logística, $\bm{\beta}=(\beta_1,\dots,\beta_D)$. \\
		$\beta_0$ & Intercepto (sesgo) del modelo en regresión logística. \\
		$\beta$ & Término de sesgo o intercepto en regresión logística (cuando se usa sin subíndice). \\
		$p(\bm{x}_i)$ & Estimación de la probabilidad de la clase positiva para la observación $\bm{x}_i$ en regresión logística. \\
		$\tilde{J}(\bm{\beta})$ & Función de costo regularizada en regresión logística. \\
		$\alpha$ & Parámetro de regularización L1 (Lasso) o L2 (Ridge) en regresión. \\
		$\bm{H}$ & Matriz Hessiana de la función de costo en métodos Newton-CG. \\
		$\bm{B}$ & Aproximación de la matriz Hessiana en L-BFGS. \\
		$\mathcal{C}$ & Parámetro de regularización del modelo (controla el equilibrio entre ajuste y complejidad). \\
		\toprule
		\multicolumn{2}{l}{\textbf{SVM}}\\
		\midrule
		$\bm{w}$ & Vector normal al hiperplano óptimo. \\
		$\beta$ & Término de sesgo o intercepto en SVM (cuando se usa sin subíndice). \\
		$\xi_i$ & Variable de holgura (\emph{slack}) para el registro $i$-ésimo en  margen suave. \\
		$C$ & Parámetro de regularización en SVM que controla el equilibrio entre margen y errores. \\
		$K(\bm{a},\bm{b})$ & Función kernel que calcula la similitud entre los vectores $\bm{a}$ y $\bm{b}$. \\
		$\gamma$ & Parámetro de ancho en el kernel RBF (Radial Basis Function). \\
		$r$ & Coeficiente de desplazamiento en kernels polinómicos y sigmoides. \\
		$d$ & Grado del polinomio en el kernel polinómico. \\
		\toprule
		\multicolumn{2}{l}{\textbf{Árboles de decisión y Random Forest}}\\
		\midrule
		$t$ & Nodo de un árbol de decisión. \\
		$N(t)$ & Número de registros en el nodo $t$. \\
		$p(l\mid t)$ & Proporción/probabilidad de la clase $l$ en el nodo $t$. \\
		$H(\mathcal{D})$ & Entropía de Shannon del conjunto de datos $\mathcal{D}$. \\
		$\text{Gini}(t)$ & Índice de Gini del nodo $t$ en un árbol de decisión. \\
		$\widehat{y}_a(\bm{x})$ & Predicción del árbol $a$ en Random Forest para la observación $\bm{x}$. \\
		$A$ & Número total de árboles en un Random Forest. \\
		$\texttt{criterion}$ & Criterio de partición en árboles/RF (gini o entropy). \\
		$\texttt{max\_depth}$ & Profundidad máxima del árbol. \\
		$\texttt{min\_samples\_split}$ & Mínimo de muestras para dividir un nodo. \\
		$\texttt{min\_samples\_leaf}$ & Mínimo de muestras en una hoja. \\
		$\texttt{max\_features}$ & Número de atributos considerados por división. \\
		%	$\text{bootstrap}$ & Remuestreo con reemplazo para entrenar árboles en Random Forest. \\
		\toprule
		\multicolumn{2}{l}{\textbf{Métricas de evaluación}}\\
		\midrule
		$\text{TP}$ & Verdaderos positivos. \\
		$\text{TN}$ & Verdaderos negativos. \\
		$\text{FP}$ & Falsos positivos. \\
		$\text{FN}$ & Falsos negativos. \\
		$\emph{Accuracy}$ & Proporción global de predicciones correctas. \\
		$\emph{Precisión}$ & Proporción de positivos predichos correctos. \\, %$\frac{TP}{TP+FP}$. \\
		$\emph{Recall}$ & Proporción de positivos reales detectados. \\
		%, $\frac{TP}{TP+FN}$. \\
		$\text{F1-Score}$ & Media armónica entre \emph{Precisión} y \emph{Recall}. \\
		$F_{\beta_f}$ & F-\emph{measure} ponderada que balancear \emph{Recall} y \emph{Precisión}. \\
		% mediante $\beta_f$. \\
		$AUC$ & Área bajo la curva ROC. \\
		$M_l$ & Valor de una métrica (\emph{Accuracy}, F1-Score, etc.) para la clase $C_l$. \\
		$\widehat{M}_{\text{weighted}}$ & Métrica ponderada por el tamaño de la clase en problemas multiclase. \\
		\toprule
		\multicolumn{2}{l}{\textbf{Validación cruzada}}\\
		\midrule
		$K$ & Número de pliegues en validación cruzada $K$-fold. \\
		$G_i$ & $i$-ésimo pliegue o grupo en validación cruzada. \\
		
		\bottomrule
	\end{longtable}
	
	
	\chapter*{Resumen}
	
	\noindent
	En este trabajo se realiza un análisis comparativo de distintas técnicas de \textbf{Aprendizaje Automático Supervisado} aplicadas a datos cardiológicos, con el objetivo de evaluar su desempeño en la clasificación de pacientes según riesgo o presencia de enfermedad cardíaca.\\
	
	\noindent
	En primer lugar, se lleva a cabo una \textbf{preparación del \emph{dataset}}, que incluye limpieza, transformación de variables, selección de atributos y división en conjuntos de entrenamiento y prueba. Se definen las notaciones fundamentales utilizadas a lo largo del trabajo.
	%, tales como el número de muestras $n$, la cantidad de atributos $D$, el número de clases $L$, y los parámetros estimados para algunos modelos.\\
	
	\noindent
	Luego, se describen los principales algoritmos estudiados:
	
	\begin{itemize}
		\item \textbf{Naïve Bayes}, se resalta su fundamento probabilístico y el supuesto de independencia condicional.
		\item \textbf{Regresión Logística}, incluyendo la función logística, el hiperplano de decisión y el proceso de optimización.
		\item \textbf{Árboles de Decisión} y \textbf{Random Forest}, destacando la construcción jerárquica de reglas, los criterios de impureza y la reducción de varianza aportada por los métodos de ensamble.
		\item \textbf{Máquinas de Vectores de Soporte (SVM)}, considerando márgenes duros y suaves, los hiperparámetros clave, el rol del vector normal $\bm{w}$ y el uso de \emph{kernels}.
	\end{itemize}
	
	\noindent
	Para cada método, se explican los \textbf{hiperparámetros relevantes}, su interpretación y cómo afectan el ajuste del modelo. Se especifican los valores considerados o el procedimiento utilizado para seleccionarlos.\\
	
	\noindent
	Posteriormente, se detalla la \textbf{metodología de evaluación}, incluyendo las métricas utilizadas derivadas de la matriz de confusión: \emph{Accuracy}, precisión, \emph{Recall}, 
	%especificidad, sensibilidad 
	y F1-Score, así como el área bajo la cyrva ROC y el tiempo de ejecución.\\
	
	\noindent
	A continuación, se presentan y discuten los \textbf{resultados obtenidos} para cada modelo. Se comparan sus desempeños utilizando las métricas seleccionadas. El método con mejor desempeño en clasificación binaria y multiclase es \textbf{Random Forest}, el cual alcanza valores altos en las métricas utilizadas para evaluar, siendo claro el poder del clasificador al usar diferentes árboles de decisión para clasifican un mismo registro, considerando la clase más votada. Por el contrario, se nota cómo a mayor cantidad de datos y clases para clasificar, el método tarda más tiempo en tomar una decisión.\\
	
	
	\noindent
	Finalmente, se exponen las \textbf{conclusiones generales} del estudio, señalando qué métodos mostraron mejor rendimiento en el \emph{dataset} de cardiología, qué limitaciones presenta el estudio y cómo podrían extenderse o mejorarse los análisis en trabajos futuros mediante técnicas adicionales, mayor optimización o validación más exhaustiva.\\
	
	
	\chapter{Introducción}
	
	
	\noindent
	Las enfermedades cardiovasculares son la causa número uno de muerte globalmente, con un estimado de 17.9 millones de vidas cada año, aproximadamente el 31\% de todas las muertes globales. La idea central de este trabajo es encontrar una técnica de aprendizaje automático  óptima para poder realizar predicciones de si un paciente tiene altas probabilidades de tener insuficiencia cardíaca.\\
	
	\noindent
	Por otro lado, la cardiotocografía (CTG) es un registro continuo de la frecuencia cardíaca fetal que se obtiene mediante un transductor de ultrasonidos colocado en el abdomen materno. La CTG se utiliza ampliamente durante el embarazo como método para evaluar el bienestar fetal, sobre todo en embarazos con mayor riesgo de complicaciones.\\
	
	
	\noindent
	El objetivo general de este trabajo es comparar el rendimiento de diversas técnicas de Aprendizaje Automático Supervisado con el fin de recomendar aquella que presente el mejor desempeño al aplicarse sobre un conjunto de datos cardiológicos. Se expondrán las técnicas empleadas y las métricas utilizadas para medidas de calidad de un clasificador y, en consecuencia, mostrar cual técnica resulta más adecuada para el problema del pre-diagnóstico de enfermedades cardíacas.
	
	\section{Motivación}
	
	El proceso de diagnóstico médico puede ser extenso, incluso contando con la mejor disposición del personal de salud, ya que con frecuencia requiere la recopilación y análisis de datos provenientes de distintos estudios. El propósito de este trabajo es contribuir a agilizar dicho proceso, identificando técnicas que puedan ser utilizadas por los profesionales médicos como herramientas complementarias para realizar diagnósticos. No solo resulta fundamental la posibilidad de obtener diagnósticos más ágiles, sino también la de reconocer qué atributos o características de los estudios resultan más significativos que otros para un diagnóstico determinado.
	
	\section{Estado del Arte}
	
	
	Las técnicas de Aprendizaje Automático se utilizan cada vez más en la investigación cardiovascular. El trabajo de Isaksen et al.~\cite{isaksen2025evaluating}
	%(2025) 
	presenta recomendaciones y orientaciones para la evaluación adecuada de modelos de aprendizaje automático supervisado en cardiología, destacando los problemas específicos asociados con estas técnicas, como la fuga de datos (\textit{data leakage}) y el desequilibrio de clases. Por su parte, el documento de Kumar y Kumar~\cite{kumar2021machine}
	%(2021) 
	revisa las metodologías de aprendizaje automático para el diagnóstico de cardiopatías utilizando métodos no invasivos, un área crucial dada la alta mortalidad anual (17.9 millones de personas) asociada con los problemas cardíacos.\\
	
	
	\noindent
	En los estudios cardiológicos, las variables utilizadas pueden agruparse en tres grandes categorías principales: parámetros clínicos estructurados, señales cardíacas y datos provenientes de imágenes médicas. Los parámetros clínicos incluyen variables demográficas, antecedentes, síntomas y mediciones de laboratorio, que suelen encontrarse en bases de datos estructuradas como el conjunto de Cleveland, utilizado comúnmente en estudios de clasificación de enfermedades cardíacas. Las señales cardíacas, como los electrocardiogramas (ECG) y fonocardiogramas (PCG), aportan información temporal y frecuencial altamente relevante para la detección de arritmias, soplos o patrones eléctricos anómalos. Finalmente, los datos de imagen —que abarcan ecocardiografías, resonancias magnéticas cardíacas (CMR), tomografías computarizadas (CCT) o estudios SPECT— permiten evaluar morfología, función cardíaca y perfusión.\\
	
	\noindent
	En el ámbito de los métodos de aprendizaje automático aplicados a cardiología, varios trabajos han empleado los mismos algoritmos utilizados en este estudio. Por ejemplo, modelos basados en \textit{Support Vector Machines} (SVM) han demostrado un rendimiento competitivo en la clasificación de ECG y en la detección de enfermedades a partir de parámetros clínicos, mostrando en muchos casos un desempeño superior a métodos más simples. Los clasificadores \textit{k}-\emph{Nearest Neighbors} (\textit{k}NN) han sido utilizados para tareas como la detección de arritmias mediante características temporales del ECG o para la predicción de riesgo a partir de datos clínicos estructurados. Otros estudios han empleado Naïve Bayes o árboles de decisión para la clasificación de cardiopatías utilizando tanto datos tabulares como señales procesadas. Estas aplicaciones muestran que los modelos implementados en este trabajo están bien representados dentro de la literatura del área y cuentan con antecedentes sólidos en tareas diagnósticas dentro del dominio cardiológico.\\
	
	\noindent
	Además de los enfoques basados en aprendizaje automático, los profesionales de la salud emplean tradicionalmente una variedad de herramientas diagnósticas para la evaluación cardiovascular, las cuales constituyen la base clínica sobre la que se interpretan los resultados computacionales. Entre ellas se destacan el electrocardiograma (ECG), utilizado para detectar alteraciones eléctricas como isquemia, arritmias o hipertrofias; las pruebas de esfuerzo, que permiten evaluar la respuesta cardíaca ante actividad física y cambios en el segmento ST; los ecocardiogramas, que brindan información estructural y funcional sobre cavidades, válvulas y fracción de eyección; y los análisis de laboratorio, que incluyen marcadores como troponinas, colesterol y glucosa, esenciales para caracterizar el riesgo cardiovascular. Estas herramientas continúan siendo el estándar clínico que pueden complementarse con modelos computacionales, proporcionando un marco interpretativo que guía la toma de decisiones médicas.\\
	
	
	\section{Conjuntos de Datos %Utilizados
	}
	
	\noindent
	Para la realización de este trabajo se exploraron diversas plataformas en busca de conjuntos de datos reales que resulten relevantes para el estudio mediante técnicas de aprendizaje automático. Durante esta búsqueda se identificaron múltiples \emph{datasets} de distinta naturaleza: algunos correspondientes a problemas de \textbf{clasificación binaria}, donde las observaciones se asocian a dos posibles clases, y otro \emph{dataset} de \textbf{clasificación multiclase}, con más de dos categorías posibles.\\
	
	\noindent
	Se observa, además, una marcada predominancia de conjuntos de datos provenientes del ámbito \textbf{médico}, dentro de los cuales se seleccionan aquellos considerados más adecuados para las pruebas de los métodos de aprendizaje automático, abarcando tanto casos binarios como multiclase.\\
	
	\noindent
	Todos estos \emph{datasets} son procesados anteriormente a las pruebas realizadas, en donde se eliminan tanto registros duplicados como con datos faltantes, tampoco se tienen en cuenta aquellos registros con datos atípicos \textit{a priori}, como un caso donde un paciente tiene colesterol 0.\\
	
	
	\noindent
	Se realiza una pequeña descripción de cada atributo utilizado en cada \emph{dataset}. Si el atributo es categórico, se informa la distribución de los valores que posee dicho atributo y en caso de resultar ser un atributo numérico, se informa la media de los valores de dicho atributo, junto con el valor máximo y mínimo que posee. Se realiza lo anterior para contextualizar los atributos y ver los rangos de valores con que se trabajará. \\
	
	
	\subsection{Dataset Binario: \emph{Insuficiencia  Cardíaca Predicción}}
	
	\noindent
	El \emph{dataset} \textit{HeartFailure}~\cite{HeartFailure} fue creado mediante la combinación de cinco \emph{datasets} independientes con 11 atributos comunes, logrando el \emph{dataset} más grande de información de enfermedades cardiovasculares utilizado para investigación. Los cinco \emph{datasets} utilizados son:
	
	\begin{itemize}
		\item Cleveland: 303 observaciones,
		\item Hungarian: 294 observaciones,
		\item Switzerland: 123 observaciones,
		\item Long Beach VA: 200 observaciones,
		\item Stalog (Heart) Data Set: 270 observaciones.
	\end{itemize}
	
	\noindent
	En la  Tabla~\ref{tab:tablebinario} se muestra qué tipo de datos son los atributos del \emph{dataset} que son utilizados en este trabajo, considerando que estos mismos ya fueron previamente procesados para poder ser utilizados en las técnicas de aprendizaje automático. Luego, la Tabla~\ref{tab:cant_datos_binarios} muestra la cantidad de registros y atributos que son utilizados, junto al tipo de dato que son.\\
	
	
	\begin{table}[htb]%[H]
		\centering
		\caption{Cantidad de registros utilizados.}
		\label{tab:cant_datos_binarios}
		\begin{tabular}{lr}
			\toprule
			\textbf{Cantidad de registros} & \textbf{918} \\
			\textbf{Cantidad de atributos} & \textbf{11} \\
			\textbf{Atributos Categóricos} & \textbf{5} \\
			\textbf{Atributos Numéricos} & \textbf{6} \\
			
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\begin{table}[htb]
		\caption{Tipo de atributo del conjunto binario.} 
		\centering
		\label{tab:tablebinario}
		\begin{tabular}{llcc}
			\toprule
			\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{¿Está codificado?} & \textbf{Unidad} \\
			\hline
			Age        & Numérico (\texttt{int}) & No & Años \\
			\hline
			Sex             & Categórico (\texttt{string}) & No & -\\
			\hline
			ChestPainType               & Categórico (\texttt{string}) & No & -\\
			\hline
			RestingBP     & Numérico (\texttt{int}) & No & mm Hg\\
			\hline
			Cholesterol    & Numérico (\texttt{int}) & No & mm/dl\\
			\hline
			FastingBS                 & Numérico (\texttt{int}) & Sí & mg/dl\\
			\hline
			RestingECG                      & Categórico (\texttt{string}) & No & -\\
			\hline
			MaxHR    & Numérico (\texttt{int}) & No & -\\
			\hline
			ExerciseAngina               & Categórico (\texttt{string}) & No & -\\
			\hline
			Oldpeak                 & Numérico (\texttt{float}) & No & ST en depresión\\
			\hline
			ST\_Slope                 & Categórico (\texttt{string}) & No & -\\
			\hline
			HeartDisease                 & Numérico (\texttt{int})     & Sí & -\\
			\bottomrule
		\end{tabular}
		
	\end{table}
	
	
	%	\begin{table}[htb]
		%		\caption{Tipo de atributo del conjunto binario.} 
		%		\centering
		%		\label{tab:tablebinario}
		%			\begin{tabular}{l|c|c|c}
			%			\toprule
			%			\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{¿Está codificado?} & \textbf{Unidad} \\
			%			\hline
			%			Age        & Numérico (int) & No & Años \\
			%			\hline
			%			Sex             & Categórico (string) & No & -\\
			%			\hline
			%			ChestPainType               & Categórico (string) & No & -\\
			%			\hline
			%			RestingBP     & Numérico (int) & No & mm Hg\\
			%			\hline
			%			Cholesterol    & Numérico (int) & No & mm/dl\\
			%			\hline
			%			FastingBS                 & Numérico (int) & Sí & mg/dl\\
			%			\hline
			%			RestingECG                      & Categórico (string) & No & -\\
			%			\hline
			%			MaxHR    & Numérico (int) & No & -\\
			%			\hline
			%			ExerciseAngina               & Categórico (string) & No & -\\
			%			\hline
			%			Oldpeak                 & Numérico (float) & No & ST en depresión\\
			%			\hline
			%			ST\_Slope                 & Categórico (string) & No & -\\
			%			\hline
			%			HeartDisease                 & Numérico (int)     & Sí & -\\
			%			\bottomrule
			%		\end{tabular}
		%				
		%	\end{table}
	
	
	\subsubsection{\underline{Descripción de los atributos}}
	
	
	\noindent
	\textbf{Age}: Este atributo se refiere a la edad de los pacientes. Los pacientes tienen una media de edad de 53 años, con una edad máxima de 77 y de edad mínima  de 28, con proporciones de edad bastante bien distribuidas, siendo la menor de 0.11\% para algunas edades y la mayor de 4.14\% para otras edades, teniendo otras distribuciones entre estos dos rangos.\\
	
	\noindent
	\textbf{Sex}: Refiere al sexo de los pacientes; hay una distribución de sexo del 78.98\% masculinos y el 21.02\%  femeninos.\\
	
	\noindent
	\textbf{ChestPainType}: Tipo de dolor torácico causado por isquemia cardíaca. Tiene una distribución 18.85\% de angina atípica, luego un 22.11\% de dolor no anginoso, un 54.03\% asintomático y un 5.01\% de dolor de pecho anginoso típico.\\
	
	\noindent
	\textbf{RestingBP}: Describe la presión sanguínea en reposo, tiene un valor medio de 133.02, con un valor máximo de 200.00 y valor mínimo de 92.00.\\
	
	%{0.2cm}
	\noindent
	\textbf{Cholesterol}: Este atributo es el colesterol sérico, es decir, la medida total de colesterol en sangre; tiene un valor medio en los pacientes de 199.02, con un valor máximo de 603.00 y un valor mínimo de 85.00. Se encuentra medido en miligramos por decilitro. \\
	
	%{0.2cm}
	\noindent
	\textbf{FastingBS}: Es la Glucosa en sangre en ayuno; hay un 76.66\% de registros con  valores de glucosa en sangre menores a 120 mg/dl, codificado en 0, y un 23.34\% con valores mayores a 120 mg/dl, codificado en 1.\\
	
	%{0.2cm}
	\noindent
	\textbf{RestingECG}: Son los resultados de electrocardiogramas en reposo; hay un 60.09\% codificado en Normal, un 19.41\% codificado en ST (tiene una anormalidad en el estudio) y un 20.50\% codificado en LVH (probablemente una hipertrofia en el ventrículo izquierdo).\\
	
	%{0.2cm}
	\noindent
	\textbf{MaxHR}: Este atributo es el máximo ritmo cardíaco registrado, tiene una media en los pacientes de 136.79, un valor máximo de 202.00 y un valor mínimo de 60.00. \\
	%{0.2cm}
	
	\noindent
	\textbf{ExerciseAngina}: Es la angina (dolor en el pecho) producida por ejercicio, ; donde hay un 59.54\% de pacientes que no tenían dolor, codificado en N, y un 40.46\% que sí tenían dolor, codificado en Y. \\
	
	%{0.2cm}
	\noindent
	\textbf{Oldpeak}: Valor máximo de depresión del segmento ST (en milímetros) registrado en todas las derivaciones contiguas durante una prueba de esfuerzo. Forma parte del cálculo del riesgo de que un paciente sufra de isquemia o infarto de miocardio; donde valores altos indican un mayor riesgo de enfermedad coronaria; tiene una media de 0.90, ujn valor máximo de 6.20 y un valor mínimo de -0.10. \\
	
	%{0.2cm}
	\noindent \textbf{ST\_Slope}: La pendiente del segmento ST durante el ejercicio máximo; hay un 43.08\% de casos \emph{Up}, un 50.05\% de \emph{Flat} y un 6.87\% de \emph{Down} [\emph{Up}: pendiente ascendente, \emph{Flat}: pendiente plana, 
	\emph{Down}: pendiente descendente].\\
	
	
	%{0.2cm}
	\noindent
	\textbf{HeartDisease}: Variable de salida que indica si la persona posee una enfermedad cardíaca; donde un 44.71\% no tiene enfermedad cardíaca, codificado en 0, y un 55.29\% sí tiene enfermedad cardíaca, codificado en 1.
	Siendo ésta la \textbf{variable objetivo}.
	
	
	\subsection{\emph{Dataset} Multiclase: \emph{Cardiotocografía Predicción}}
	
	
	\noindent
	En el \emph{dataset}~\cite{Cardiotocografia} utilizado, se procesaron automáticamente 2126 cardiotocogramas fetales y se midieron sus características diagnósticas. Tres obstetras expertos clasificaron los CTG, asignando una etiqueta de clasificación consensuada. La clasificación se realizó con respecto al estado fetal, el cual puede ser normal, sospechoso o patológico.\\
	
	\noindent
	En la Tabla~\ref{tab:tablaej} se muestran los tipos de las variables utilizadas en este trabajo, junto con sus valores medio, máximo y mínimo. Luego, la Tabla~\ref{tab:cant_datos_multiclase} muestra la cantidad de registros y atributos que son utilizados, junto al tipo de dato que son.
	
	
	\begin{table}[htb]%[H]
		\centering
		\caption{Cantidad de registros utilizados.}
		\label{tab:cant_datos_multiclase}
		\begin{tabular}{lr}
			\toprule
			\textbf{Cantidad de registros} & \textbf{2115} \\
			\textbf{Cantidad de atributos} & \textbf{21} \\
			\textbf{Atributos Categóricos} & \textbf{0} \\
			\textbf{Atributos Numéricos} & \textbf{21} \\
			
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\begin{table}[htb]
		\caption{Tipo de atributo del conjunto multiclase.}
		\label{tab:tablaej}
		\centering
		\begin{tabular}{llrrr}
			\toprule
			\textbf{Atributo} & \textbf{Tipo de dato} & \textbf{Media} & \textbf{Máximo} & \textbf{Mínimo}\\
			\hline
			LB        & Numérico (\texttt{int}) & 133.301 &160.000 &106.000\\
			
			\hline
			AC             & Numérico (\texttt{float}) & 0.003 & 0.019 & 0.000\\
			\hline
			FM               & Numérico (\texttt{float})& 0.009 & 0.481 & 0.000\\
			\hline
			UC     & Numérico (\texttt{float}) & 0.004 &0.015 &0.000\\
			\hline
			DL    & Numérico (\texttt{float})&0.001 & 0.015 & 0.000\\
			\hline
			DS                 & Numérico (\texttt{float})&- &- &-\\
			\hline
			DP                      & Numérico (\texttt{float})&- &- &-\\
			\hline
			ASTV    & Numérico (\texttt{int}) &46.977 &87.000 &12.000\\
			\hline
			MSTV               & Numérico (\texttt{float}) &1.335 & 7.000&0.200\\
			\hline
			ALTV                 & Numérico (\texttt{int}) &9.759 &91.000 &0.000\\
			\hline
			MLTV                 & Numérico (\texttt{float})&8.170 &50.700 &0.000\\
			\hline
			Width                 & Numérico (\texttt{int}) &70.511 &180.000 &3.000\\
			\hline
			Min                 & Numérico (\texttt{int}) &93.574 &159.000 &50.000\\
			\hline
			Max                 & Numérico (\texttt{int})  &164.085 &238.000 &122.000\\
			\hline
			Nmax                 & Numérico (\texttt{int})  &4.075 &18.000 &0.000\\
			\hline
			Nzeros                 & Numérico (\texttt{int})  &- &- &-\\
			\hline
			Mode                 & Numérico (\texttt{int})  &137.448 &187.000 &60.000\\
			\hline
			Mean                 & Numérico (\texttt{int})& 134.596& 182.000&73.000 \\
			\hline
			Median                 & Numérico (\texttt{int}) &138.084 &186.000 &77.000 \\
			\hline
			Variance                 & Numérico (\texttt{int})&18.891 &269.000 &0.000 \\
			\hline
			Tendency                 & Numérico (\texttt{int})&- &- &- \\
			\hline
			NSP                 & Categórico (\texttt{string}) &- &- &-\\
			\bottomrule
		\end{tabular}
	\end{table}
	
	
	\subsubsection{\underline{Descripción de los atributos}}
	
	\noindent\textbf{LB}: Frecuencia cardíaca fetal basal (latidos por minuto). \\
	
	\noindent\textbf{AC}: Número de aceleraciones por segundo. \\
	
	\noindent\textbf{FM}: Número de movimientos fetales por segundo. \\
	
	\noindent\textbf{UC}: Número de contracciones uterinas por segundo. \\
	
	\noindent\textbf{DL}: Número de desaceleraciones leves por segundo. \\
	
	\noindent\textbf{DS}: Número de desaceleraciones severas por segundo. \\
	
	\noindent\textbf{DP}: Número de desaceleraciones prolongadas por segundo.  
	Hay un 91.58\% con valor de 0, un 3.40\% con valor 0.002, un 1.13\% con valor 0.003, 	un 3.31\% con valor 0.001, un 0.43\% con valor 0.004 y un 0.14\% con valor 0.005. \\
	
	\noindent\textbf{ASTV}: Porcentaje de tiempo con variabilidad anormal a corto plazo. \\
	
	\noindent\textbf{MSTV}: Valor medio de la variabilidad a corto plazo. \\
	
	\noindent\textbf{ALTV}: Porcentaje de tiempo con variabilidad anormal a largo plazo. \\
	
	\noindent\textbf{MLTV}: Valor medio de la variabilidad a largo plazo. \\
	
	\noindent\textbf{Width}: Ancho del histograma de CTG. \\
	
	\noindent\textbf{Min}: Mínimo del histograma de CTG. \\
	
	\noindent\textbf{Max}: Máximo del histograma de CTG. \\
	
	\noindent\textbf{Nmax}: Número de picos del histograma de CTG. \\
	
	\noindent\textbf{Nzeros}: Número de ceros del histograma de CTG.  
	Hay un 76.26\% con valor 0, un 17.30\% con valor 1, un 0.99\% con valor 3, 
	un 5.11\% con valor 2, un 0.09\% con valor 4, un 0.05\% con valor 10, 
	un 0.09\% con valor 5, un 0.05\% con valor 8 y un 0.05\% con valor 7. \\
	
	\noindent\textbf{Mode}: Moda del histograma de CTG. \\
	
	\noindent\textbf{Mean}: Promedio del histograma de CTG. \\
	
	\noindent\textbf{Median}: Mediana del histograma de CTG. \\
	
	\noindent\textbf{Variance}: Varianza del histograma de CTG. \\
	
	\noindent\textbf{Tendency}: Tendencia del histograma de CTG.  
	Hay un 39.67\% con valor 1, un 52.53\% con valor 0 y un 8.27\% con valor -1. \\
	
	\noindent\textbf{CLASS}: Código de clasificación del estado fetal  
	(N = normal, S = sospechoso y P = patológico).  
	Hay un 13.81\% de casos sospechosos, un 77.92\% de normales y un 8.27\% de patológicos.  
	Siendo ésta la \textbf{variable objetivo}.
	
	%{0.2cm}
	
	
	
	\chapter{Métodos de Clasificación }
	\label{Sec:Métodos}
	
	\noindent
	En este capítulo se presentan los métodos de aprendizaje automático supervisado que se emplearon en este trabajo, sde describen sus fundamentos teóricos, las fórmulas y notaciones utilizadas, así como los hiperparámetros y criterios de optimización más relevantes. 
	El objetivo es describir y explicar cómo funciona cada uno de los algoritmos, sus supuestos y la forma en que se aplican a los datos cardiólogos analizados. \\
	
	\noindent
	Los métodos requieren de  \textbf{hiperparámetros}, que son un parámetro cuyo valor se fija antes del proceso de entrenamiento de un modelo y que controla aspectos del aprendizaje, como la complejidad del modelo, la velocidad de convergencia o la regularización. 
	A diferencia de los parámetros internos del modelo (por ejemplo, los coeficientes en una regresión logística o en un hiperplano de separación), los hiperparámetros no se aprenden automáticamente a partir de los datos, sino que deben ser seleccionados mediante experimentación, validación cruzada u optimización de hiperparámetros, siendo un análisis en sí mismos. \\
	
	\noindent
	Los \textbf{hiperparámetros} utilizados en este trabajo corresponden a aquellos definidos por la librería \texttt{scikit-learn}~\cite{Pedregosa2011} del lenguaje de programación \emph{Python}, la cual ofrece implementaciones estandarizadas y bien documentadas de modelos clásicos de aprendizaje automático. En particular, la búsqueda en grilla se realiza con los valores posibles que determina la librería, junto con la combinación de valores posibles. \\
	% de combinar, como el grado del polinomio cuando utilizamos el kernel de polinomia en \textbf{SVM}. 
	
	\noindent
	En particular, se emplearon los valores por defecto sugeridos por la librería y, cuando fue necesario, se ajustaron manualmente algunos hiperparámetros relevantes %—como la profundidad máxima del árbol, el criterio de partición o la cantidad de vecinos en modelos basados en distancia— 
	con el fin de evaluar su influencia en el desempeño del modelo. 
	Este procedimiento permite garantizar consistencia en las comparaciones, reproducibilidad de los experimentos y una interpretación clara acerca del efecto de cada hiperparámetro sobre los resultados. \\
	
	\noindent 
	En lo que sigue, los datos poseen $D$ atributos denotados por $X_1,X_2, \dots, X_D$ y cada registro es representado como $\bm{x}_i=(x_{i1}, x_{i2}, \dots, x_{iD},y_i)$, para $i=1,2,\dots,n$, donde $y_i$ indica la etiqueta.
	Además, $C_1,C_2, \dots, C_L$ denotarán las clases de interés, siendo $L=2$ para el caso binario.
	
	\section{Clasificador Naïve Bayes}
	
	\noindent
	El \textbf{clasificador Naïve Bayes}~\cite{hand2001idiot} (NB) es un método supervisado probabilístico basado en el \textit{Teorema de Bayes}, que asume independencia condicional entre los atributos dado la clase de pertenencia. Si bien esta suposición rara vez se cumple estrictamente en la práctica —debido a que las variables suelen estar correlacionadas entre sí- el método continúa funcionando de manera efectiva en numerosos problemas reales. 
	La regla de clasificación, o la probabilidad de pertenencia de un registro a la clase $C_l$, para $l \in \{1, 2, \dots, L\}$ está dada por:
	%, donde cada registro tiene $D$ atributos, indexados por $d \in \{1, 2, \dots, D\}$.
	
	\begin{equation}
		P(Y = C_l \mid X_1 = x_1, X_2 = x_2,\dots, X_D = x_D)
		= \frac{
			P(X_1 = x_1, X_2 = x_2, \dots, X_D = x_D \mid Y = C_l) \, P(Y = C_l)
		}{
			P(X_1 = x_1, X_2 = x_2,\dots, X_D = x_D)
		},
		\label{eq:bayes_general}
	\end{equation}
	\noindent
	donde $P(Y = C_l)$ es la \textbf{probabilidad \emph{a priori}} de la clase $C_l$, es decir, qué tan frecuente es esta clase en el conjunto de datos. 
	Mientras que, $P(X_1 = x_1, X_2=x_2,\dots, X_D = x_D \mid Y = C_l)$ es la \textbf{verosimilitud}, es decir, cuán probable es observar las características de un registro si supiéramos que pertenece a la clase $C_l$.
	
	\paragraph{Probabilidad Condicional:}
	
	Bajo el supuesto de independencia, la probabilidad condicional puede expresarse como:
	
	\begin{equation}
		P(X_1 = x_1, X_2 = x_2,\dots, X_D = x_D \mid Y = C_l)
		= \prod_{j=1}^{d} P(X_j = x_j \mid Y = C_l).
		\label{eq:independencia_condicional}
	\end{equation}
	
	
	\noindent
	Por lo tanto, basta estimar individualmente, para cada atributo 
	$X_j$ con $j \in \{1,2,\dots,D\}$ y para cada clase 
	$C_l$ con $l \in \{1,2,\dots,L\}$, la probabilidad:
	
	\begin{equation}
		\widehat{\theta}_{jvl} =
		P(X_j = x_{jv} \mid Y = C_l) =
		\frac{\left| \{\,X_j = x_{jv} \,\wedge\, Y = C_l\,\} \right|}
		{|C_l|} ,
		\label{eq:probabilidad_condicional}
	\end{equation}
	\noindent
	donde $v \in \{1,2,\dots,V_j\}$ indexa los valores posibles del atributo $X_j$.
	Aquí, cada parámetro cumple una función específica:
	\begin{itemize}
		
		\item $\widehat{\theta}_{jvl}$: estimación de la probabilidad de que el atributo $X_j$ tome el valor $x_{jv}$ para registros de la clase $C_l$.  
		Representa qué tan típico es ese valor dentro de una clase.
		\item $\left| \{\,X_j = x_{jv} \wedge Y = C_l\,\} \right|$: cantidad de registros en la clase $C_l$ para los cuales el atributo $X_j$ toma el valor $x_{jv}$.
		\item $|C_l|$: número total de registros pertenecientes a la clase $C_l$.
	\end{itemize}
	
	
	
	\noindent
	Podemos pensar a $\theta_{jmk}$ como una especie de \textit{``frecuencia interna de la clase''}, una medida que indica qué características son más comunes dentro de cada grupo.\\
	
	\paragraph{Probabilidad \emph{a priori}:}
	
	Otra estimación fundamental a calcular es la de que un registro pertenezca a la clase $C_l$, dada por:
	%y expresar es la \emph{a priori}.
	%, en la cual se asemeja a la probabilidad de que un nodo pertenezca a una clase $k$ en \textit{Random Forest}
	\begin{equation}
		\widehat{\pi}_l = P(Y = C_l) =
		\frac{|C_l|}{n},
		\label{eq:probabilidad_priori}
	\end{equation}
	donde $n$ es el tamaño muestral.
	%\noindent
	%Los parámetros involucrados son:
	%
	%\begin{itemize}
	%	
	%	\item $\hat{\pi}_l$: probabilidad estimada de que un registro pertenezca a la clase $C_l$.
	%	Corresponde a la frecuencia relativa de esa clase en la muestra.	
	%	\item $|C_l|$: cantidad de registros cuya clase es $C_l$	
	%	\item $n$: tamaño muestral.
	%\end{itemize}
	
	\noindent
	La intuición es sencilla: antes de mirar ningún atributo, ¿qué tan probable es cada clase?\\
	
	
	
	%\noindent
	%La clase predicha maximiza la probabilidad a posteriori:
	
	%\begin{equation}
	%	\hat{Y} = 
	%	\underset{C_k}{\operatorname{argmax}}
	%	\left[
	%	P(Y = C_k)
	%	\prod_{j=1}^{d} P(X_j = x_j \mid Y = C_k)
	%	\right]
	%	\label{eq:regla_clasificacion_nb}
	%\end{equation}
	
	
	\noindent
	\textbf{Caso Continuo (Naïve Bayes Gaussiano)}
	
	\noindent
	Cuando los atributos son continuos y se asume distribución Normal, las verosimilitudes se estiman con la función de densidad Gaussiana, dada por:
	
	\begin{equation}
		f(x) =
		\frac{1}{\sigma \sqrt{2\pi}}
		\exp \left[
		-\frac{1}{2} \left(
		\frac{x - \mu}{\sigma}
		\right)^2
		\right],
		\label{eq:densidad_gaussiana}
	\end{equation}
	donde $x,\mu \in \mathbb{R}$ y $\sigma>0$.
	Por lo cual la decisión final se obtiene como:
	
	\begin{equation}
		\widehat{Y} = 
		\underset{C_l}{\operatorname{argmax}}
		\left[
		\log P(Y = C_l) +
		\sum_{j=1}^{D}
		\log f(x_j \mid \mu_{jl}, \sigma_{jl}^2)
		\right],
		\label{eq:decision_gaussiana}
	\end{equation}
	\noindent
	donde los parámetros de la distribución normal para cada atributo \(X_j\) y para cada clase \(C_l\) se estiman como:
	
	\begin{equation}
		\widehat{\mu}_{jl}
		= \frac{1}{|C_l|}
		\sum_{i : Y_i = C_l} x_{ij},
		\label{eq:media_gaussiana}
	\end{equation}
	
	\begin{equation}
		\widehat{\sigma}_{jl}^2
		= \frac{1}{|C_l|}
		\sum_{i : Y_i = C_l}
		\left( x_{ij} - \widehat{\mu}_{jl} \right)^2.
		\label{eq:varianza_gaussiana}
	\end{equation}
	
	
	
	\section{Regresión Logística}
	
	\noindent
	El modelo de \textbf{Regresión Logística}~\cite{cramer2002origins} (LR, por su equivalente en inglés \emph{Logistic Regression}) es una técnica de análisis de datos utilizada para establecer relaciones entre las variables predictoras y la clase a la cual pertenece cada registro. El modelo permite predecir la probabilidad de que un nuevo registro pertenezca a una clase determinada. Este tipo de modelo de regresión es justamente utilizado para los problemas no linealmente separables.
	%, como la mayoría de los problemas.
	\\
	
	\noindent
	A diferencia de la regresión lineal múltiple, la regresión logística predice una probabilidad (valor entre 0 y 1). Ambos modelos son lineales en sus parámetros, pero difieren en la naturaleza de la variable dependiente. El objetivo es estimar los coeficientes de regresión que maximizan la verosimilitud de los datos observados, o encontrar los coeficientes que mejor funcionan para los datos de entrenamiento.\\
	
	\noindent
	El modelo busca estimar la probabilidad condicional de que una observación pertenezca a la clase positiva (o clase objetivo). 
	%Sea $C_l$ el conjunto de clases posibles, con $l \in \{1,2,\dots,L\}$. 
	Para cada registro $i$ (con $i = 1,2,\dots,n$), su etiqueta verdadera se denota por $y_i$, donde se define:
	\[
	y_i =
	\begin{cases}
		1, & \text{si la observación pertenece a la clase positiva } C_1, \\
		0, & \text{si la observación pertenece a la clase negativa } C_2.
	\end{cases}
	\]
	
	%\begin{equation}
	%	P(l_i = 1 \mid \bm{x}_i),
	%	\label{eq:probabilidad_condicional}
	%\end{equation}
	%
	%\noindent
	%donde $\bm{x}_i = (x_{i1}, x_{i2}, \dots, x_{ik})$ es el vector de características de la observación $i$, el cual también llamamos registro, en el cual $D$ es la cantidad de atributos que posee la observación.\\
	
	\noindent
	La función logística define la \textbf{función de probabilidad} de pertenencia del registro $\bm x_{i}$ a la clase $C_1$ como:
	
	\begin{equation}
		p(\bm{x}_i) = P(y_i = 1 \mid \bm{x}_i) =
		\frac{\exp(\beta_0 + \sum_{j=1}^{D} \beta_j x_{ij})}
		{1 + \exp(\beta_0 + \sum_{j=1}^{D} \beta_j x_{ij})},
		\label{eq:funcion_logistica}
	\end{equation}
	
	\noindent
	donde $\beta_0$ es el intercepto y $\beta_j$ son los coeficientes asociados a cada predictor, para $j=1,2,\dots,D$.\\
	
	\noindent
	La función inversa de la función logística, denominada \textbf{\textit{función logit}}, relaciona el logaritmo de los \emph{odds} con un modelo lineal, siendo \emph{odds} lo que se utiliza para analizar si la probabilidad de ocurrencia de un evento -caso/no caso- difiere o no en distintos grupos. 
	En este caso,
	
	%\begin{equation}
	%	\text{logit}(\textbf{antilogic}(x)) = x
	%	\label{eq:logit}
	%\end{equation}
	
	
	%\begin{equation}
	%	\text{odds} =
	%	\frac{p}{1 - p} 
	%	\label{eq:odds}
	%\end{equation}
	
	
	\begin{equation}
		\text{logit}(p(\bm{x}_i)) =
		\ln \left( \frac{p(\bm{x}_i)}{1 - p(\bm{x}_i)} \right)
		= \beta_0 + \sum_{j=1}^{D} \beta_j x_{ij}.
		\label{eq:funcion_logit}
	\end{equation}
	
	\noindent
	Esta transformación asegura que los valores obtenidos estén acotados entre 0 y 1, mientras que la combinación lineal de predictores puede tomar cualquier valor real, siendo este último punto un factor que realiza el cálculo de coeficientes muy difícil.
	Para solucionar este obstáculo, el cálculo de los coeficientes de regresión se estiman mediante el método de \textbf{Máxima Verosimilitud} (MLE), donde la función de log-verosimilitud a maximizar es
	
	\begin{equation}
		\ell(\beta_0, \beta_1, \dots, \beta_k) =
		\sum_{i=1}^{n} \left[
		y_i \ln(p(\bm{x}_i)) +
		(1 - y_i) \ln(1 - p(\bm{x}_i))
		\right],
		\label{eq:log_verosimilitud}
	\end{equation}
	en donde la solución analítica no existe, por lo que se utilizan métodos numéricos iterativos para obtener los parámetros óptimos.\\
	
	\noindent
	\textbf{Hiperparámetros}
	\noindent
	\begin{itemize}
		\item \textbf{Parámetro de Regularización ($\mathcal{C}$):} Controla la complejidad del modelo. Valores pequeños de $\mathcal{C}$ implican mayor regularización (menor sobreajuste), mientras que valores grandes permiten mayor flexibilidad del modelo.
		
		\item \textbf{Penalización (\textit{penalty}):} Es un término regulador que se suma a la función de coste original $J$, para formar una función ajustada $\tilde{J}$. Controla la capacidad del modelo y reduce el error de generalización.
		
		\begin{itemize}
			
			\item  Regularización L1 (Lasso):
			\begin{equation}
				\tilde{J}_{L1}(\boldsymbol{\beta}) =
				J(\boldsymbol{\beta}) +
				\alpha \sum_{j=1}^{D} |\beta_j|,
				\label{eq:l1_penalty}
			\end{equation}
			donde
			\begin{itemize}
				\item \(\beta_j\) son los coeficientes del modelo, que representan la influencia de cada variable \(x_j\) en la predicción, para $j=1,2,\dots,D$;
				\item \(\alpha \ge 0\) es el parámetro de regularización que controla la intensidad de la penalización: valores más altos implican mayor castigo a los coeficientes. 
			\end{itemize}
			%{eq:l1_penalty}{}
			
			
			\item Regularización L2 (Ridge):
			\begin{equation}
				\tilde{J}_{L2}(\boldsymbol{\beta}) =
				J(\boldsymbol{\beta}) +
				\frac{1}{2}\alpha \sum_{j=1}^{D} \beta_j^2,
				\label{eq:l2_penalty}
			\end{equation}
			donde
			\begin{itemize}
				\item \(\beta_j\) controla cuánto influye cada característica sobre la salida;
				\item \(\alpha \geq 0  \) determina cuánto se penaliza el tamaño de los coeficientes, evitando que crezcan demasiado. 
			\end{itemize}
			
			%{eq:l2_penalty}{}
			
			\item Regularización Elastic Net:
			\begin{equation}
				\tilde{J}_{EN}(\boldsymbol{\beta}) =
				J(\boldsymbol{\beta}) +
				\alpha \left[
				\rho \sum_{j=1}^{D} |\beta_j| +
				\frac{1 - \rho}{2} \sum_{j=1}^{D} \beta_j^2
				\right],
				\label{eq:elasticnet_penalty}
			\end{equation}
			donde
			\begin{itemize}
				\item \(\beta_j\) son los coeficientes asociados a cada atributo;
				\item \(\alpha \geq 0\) controla el grado total de regularización; 
				\item \(\rho \geq 0\) balancea la combinación entre L1 y L2. En particular, si $\rho=0$ entonces se tiene L2 y si $\rho=1$ se tiene L1. 
			\end{itemize}
			%{eq:elasticnet_penalty}{}
			
		\end{itemize}
		
		\item \textbf{Algoritmo de Optimización (\textit{solver}):}  
		El \textit{solver} es el algoritmo numérico encargado de minimizar la función de coste regularizada $\tilde{J}(\boldsymbol{\beta})$.
		
		\begin{itemize}
			\item Método Newton-CG (basado en segunda derivada):
			\begin{equation}
				\boldsymbol{\beta}^{(t+1)} =
				\boldsymbol{\beta}^{(t)} -
				\bm{H}^{-1}
				\nabla_{\boldsymbol{\beta}} \tilde{J}(\boldsymbol{\beta}^{(t)}),
				\label{eq:newtoncg}
			\end{equation}
			donde \(\boldsymbol{\beta}\) es el vector de parámetros del modelo que se actualiza en cada iteración. El método utiliza el gradiente $\nabla_{\boldsymbol{\beta}}$ y la inversa de la matriz Hessiana $\bm{H}$ (o una aproximación de la misma mediante \emph{Conjugate Gradient}) para determinar la dirección de actualización del vector de parámetros.
			%		%{eq:newtoncg}{}
			
			\item Método L-BFGS (quasi-Newton):
			\begin{equation}
				\boldsymbol{\beta}^{(t+1)} =
				\boldsymbol{\beta}^{(t)} -
				\bm{B}^{-1}
				\nabla_{\boldsymbol{\beta}} \tilde{J}(\boldsymbol{\beta}^{(t)}).
				\label{eq:bfgs}
			\end{equation}
			
			En este método, \(\boldsymbol{\beta}\) también es el vector de parámetros a optimizar. La matriz \(\bm{B}\) aproxima al Hessiano, permitiendo actualizaciones sin calcular derivadas segundas reales. Soporta la penalidad L2.
			%		%{eq:bfgs}{}
			
			\item Método SAGA: 
			Algoritmo de tipo \emph{stochastic gradient descent} con soporte para regularización L1 y L2. Permite trabajar con grandes \emph{datasets} y converge de manera estable incluso con penalización L1.
			
			%		\item Método Liblinear.
			%		Optimiza la función de coste usando un método de descenso por coordenadas, eficiente para datasets grandes y modelos lineales. Soporta L1 y L2.
		\end{itemize}
		
		\item 
		\textbf{Estrategia Multiclase:} %(\textit{multi\_class}):}
	La regresión logística está diseñada originalmente para clasificación binaria. Para extenderla a múltiples clases se emplean estrategias como:
	\begin{itemize}
		\item \textit{one-vs-rest} (OvR): entrena un clasificador por clase;
		\item \textit{multinomial}: optimiza una única función de verosimilitud multinomial conjunta.
	\end{itemize}
\end{itemize}

% ----------------------------------------------------------------------------------------------------
\section{Árboles de Decisión}

\noindent
El aprendizaje mediante \textbf{Árboles de Decisión} es un método no paramétrico que utiliza divisiones jerárquicas sobre los atributos de los datos, construyendo reglas de decisión del tipo \textit{if-else} para predecir el valor de una variable objetivo.\\

\noindent  
El objetivo principal es encontrar las divisiones (particiones) de un nodo padre que maximicen la pureza de los nodos hijos, es decir, que minimicen la impureza de los nodos resultantes. Es un método mucho más sencillo de realizar, donde se crea un camino de decisión, por lo cual según los valores de los atributos de un registro podemos predecir a qué clase pertenecen, donde el cómputo pesado se encuentra en la creación del propio camino. \\

%% AR: puesto que las clases se usan en diversos contextos a lo largo del trabajo, usaría una notación común, que se puede introducir al comienzo de este capítulo (previo cambio con el capítulo de métricas), deciendo algo del estilo "En lo que resta del documento...".
%% NS: Ok, lo agrego.
%% AR: No lo encontré.
%% NS: Debo agregarlo si esta el glosario?
%% AR: Sí, el glosario no es obligatorio leerlo, es un apoyo al texto.
\noindent
En consecuencia, la probabilidad de que un registro en el nodo $t$ pertenezca a la clase $C_l$, para $l \in \{1,2,\dots,L\}$, se define como:

\begin{equation}
	p(l|t) = \frac{N_l(t)}{N(t)},
	\label{eq:probabilidad_clase_nodo}
\end{equation}


\noindent
donde $N(t)$ es la cantidad total de ejemplos en el nodo $t$ y $N_l(t)$ la cantidad de ejemplos en la clase $C_l$. 
%Es importante este fenómeno porque es un costo computacional muy barato el calcular esta probabilidad y se asemejan a las probabilidades equiprobables, donde la probabilidad de una clase en un nodo está dada por la cantidad de ejemplos de la clase que tienen en el mismo.\\

\noindent
La impureza de un nodo $t$ se mide mediante una función $\phi$ que depende de las probabilidades de las clases en dicho nodo y se calcula como:
\begin{equation}
	\iota(t) = \phi \big( p(1|t), p(2|t), \dots, p(L|t) \big).
	\label{eq:impureza_general}
\end{equation} 
La función de impureza $\iota$ es un hiperparámetro en sí mismo y debe cumplir las siguientes condiciones, para una función de densidad de probabilidad $\mathbf{p}=\{p_1,p_2, \dots, p_L\}$:

\begin{itemize}
	\item \textbf{No negatividad:}
	\[
	\phi(\mathbf{p}) \ge 0.
	\]
	
	\item \textbf{Impureza mínima en nodos puros:}
	\[
	\phi(\mathbf{p}) = 0 \quad \text{si existe } l\in \{1,2,\dots,L\} \text{ tal que } p_l = 1.
	\]
	
	\item \textbf{Máxima impureza con distribución uniforme:}
	\[
	\phi(\mathbf{p}) \text{ es máxima cuando } p_l = \frac{1}{L} \quad \forall l\in \{1,2,\dots,L\}.
	\]
\end{itemize}

%\noindent
%Tanto el índice de Gini como la entropía de Shannon satisfacen estas propiedades, por lo que son funciones de impureza válidas para la construcción del árbol.



\noindent
La impureza es máxima cuando las clases están perfectamente mezcladas (la probabilidad de cada clase es la misma)y es mínima (cero) cuando el nodo contiene solo una clase.\\

\noindent
Dentro de las funciones de impureza más utilizadas, se encuentran la entropía de Shannon y el índice de Gini. \\


\noindent
La \textbf{entropía de Shannon} mide el grado de desorden o incertidumbre en el conjunto de datos $\mathcal{D}$.  
Toma valores cercanos a cero cuando el nodo es puro y crece a medida que las clases se distribuyen de forma más uniforme, calculándose como:

\begin{equation}
	H(\mathcal{D}) = - \sum_{l=1}^L 
	\frac{N_l(t)}{N(t)} 
	\log_2 \left( \frac{N_l(t)}{N(t)} \right).
	\label{eq:entropia_shannon}
\end{equation}


\noindent
El \textbf{índice de Gini} cuantifica la probabilidad de clasificar incorrectamente una instancia y se calcula como:
%si se asigna aleatoriamente según la distribución de clases en $t$.  
\begin{equation}
	\text{Gini}(t) = 
	1 - \sum_{l=1}^L \big[p(l|t)\big]^2 =
	1 - \sum_{l=1}^L \left(\frac{N_l(t)}{N(t)}\right)^2.
	\label{eq:indice_gini}
\end{equation}
Al igual que la entropía, es mínimo cuando el nodo es puro y aumenta cuando las clases están mezcladas. \\



\noindent
La \textbf{reducción de impureza} generada al dividir el nodo $t$ en dos nodos hijos $t_1$ y $t_2$ mediante una partición $s$ se calcula como

\begin{equation}
	\Delta \iota(s, t) = \iota(t) - q_1 \iota(t_1) - q_2 \iota(t_2),
	\label{eq:disminucion_impureza}
\end{equation}
donde $q_j = \frac{N(t_j)}{N(t)}$ para $j = 1, 2$.

\subsection{Bosques Aleatorios (Random Forest)}

\noindent
El algoritmo de \textbf{Random Forest}~\cite{Breiman2001} (RF)
%%AR: No hay equivalente ya que así se presentó.
%% NS: Ok.
combina múltiples árboles de decisión independientes construidos sobre subconjuntos aleatorios de los datos (muestreo con reemplazo o \emph{bootstrap}).  
Cada árbol se entrena sobre un subconjunto de atributos aleatorios en cada división, lo que introduce diversidad y reduce la varianza.\\

\noindent
La predicción final para la clasificación se obtiene mediante el voto mayoritario de los árboles está dada por:

\begin{equation}
	\widehat{y} = 
	\underset{C_l}{\operatorname{argmax}}
	\sum_{a=1}^{A} \mathbb{I}
	\big( \widehat{y}_a(\bm{x}) = C_l \big) ,
	\label{eq:voto_random_forest}
\end{equation}


\noindent
donde $\widehat{y}_a(\bm{x})$ es la predicción del ejemplo $\bm x$ dada por árbol $a$, $A$ es la cantidad total de árboles en el bosque e $\mathbb{I}$ es la \textbf{función indicadora}, definida como:
\[
\mathbb{I}( \widehat{y}_a(\bm{x}) = C_l ) =
\begin{cases}
	1, & \text{si el árbol } a \text{ asigna la clase $C_l$ al ejemplo $\bm x$}, \\
	0, & \text{en caso contrario}.
\end{cases}
\]
Esta función contabiliza cuántos árboles votan por cada clase $C_l$, permitiendo que el modelo escoja aquella con mayor cantidad de votos.\\


\noindent
\textbf{Hiperparámetros}

\begin{itemize}
	
	\item \textbf{Criterio de Partición:} 
	Función de impureza utilizada para evaluar la calidad de una división. 
	En \texttt{scikit-learn}, las opciones disponibles son:
	\texttt{gini} (índice de Gini) y \texttt{entropy} (entropía de Shannon).
	
	\item \textbf{Algoritmo de Construcción:} 
	\textit{ID3}~\cite{breiman2017classification} emplea la ganancia de información (entropía), mientras que \textit{CART}~\cite{quinlan1986induction} utiliza el índice de Gini y construye árboles binarios.
	En \texttt{scikit-learn}, el árbol implementado corresponde a una versión optimizada de \textit{CART}.
	\item \textbf{Número de Atributos Muestreados (\texttt{max\_features}):} 
	Cantidad de atributos candidatos por división.  
	En Random Forest, las opciones típicas son:
	\[
	\texttt{sqrt} = \sqrt{D}, \qquad
	\texttt{log2} = \log_2(D),
	\]
	o bien un número entero, un porcentaje o \texttt{None} (usar todos los atributos).
	
	\item \textbf{Número de Árboles (\texttt{n\_estimators}):} 
	Cantidad total de árboles que componen el bosque en Random Forest. Valores altos reducen la varianza pero aumentan el costo computacional.
	
	\item \textbf{Profundidad Máxima (\texttt{max\_depth}):} 
	Límite superior para la altura del árbol. Un valor \texttt{None} permite crecer hasta que las hojas sean puras o no haya más divisiones posibles.
	
	\item \textbf{Tamaño Mínimo de Muestra para Dividir (\texttt{min\_samples\_split}):} 
	Número mínimo de muestras requerido para realizar una partición interna.
	
	\item \textbf{Tamaño Mínimo de Muestra en una Hoja (\texttt{min\_samples\_leaf}):}
	Cantidad mínima de muestras que debe contener una hoja para evitar sobreajuste.
	
	%	\item \textbf{Profundidad Mínima del Nodo Interno (\texttt{min\_impurity\_decrease}):}
	%	Umbral mínimo de reducción de impureza necesario para aceptar una división.
	%	
	%	\item \textbf{Bootstrap (\texttt{bootstrap}):}
	%	Indica si los árboles del bosque se entrenan con muestras generadas mediante remuestreo con reemplazo. Por defecto, \texttt{True} en Random Forest.
	
\end{itemize}




\section{Máquinas de Soporte Vectorial}

\noindent
Las \textbf{Máquinas de Soporte Vectorial}~\cite{Cortes1995} (SVM, por sus siglas en inglés) constituyen una técnica de clasificación supervisada basada en la búsqueda de una \textbf{función de decisión} que permita predecir la clase de una observación a partir de sus atributos. Dado que este método opera sobre variables numéricas, los atributos categóricos deben codificarse previamente.\\

\noindent
El objetivo fundamental de SVM es encontrar un \textbf{hiperplano de separación óptimo} que divida los datos en función de sus clases, \textbf{maximizando el margen}, es decir, la distancia mínima entre el hiperplano y los puntos más cercanos de cada clase (denominados \textbf{vectores de soporte}). Dichos vectores determinan la posición y orientación del hiperplano, por lo que son los únicos puntos relevantes en el entrenamiento del modelo.\\

\noindent
Aunque existen infinitos hiperplanos que pueden separar los datos, el principio de  SVM consiste en seleccionar aquel que logre la \textbf{máxima separación posible entre las clases}, minimizando simultáneamente el riesgo de sobreajuste y aumentando la capacidad de generalización.\\


\noindent
El caso de \textbf{Margen Rígido (\emph{Hard Margin})} separa perfectamente las clases sin errores de clasificación. En este caso, el problema de optimización se formula como:

\begin{equation}
	\text{Minimizar }
	\frac{1}{2} |\bm{w}|^2
	\quad \text{sujeto a }
	y_i (\langle \bm{w}, \bm{x}_i \rangle + \beta) \ge 1 ,
	\label{eq:hard_margin}
\end{equation}

\noindent
donde 
$\bm{w}$ es el vector normal al hiperplano, 
$\beta$ es el término de sesgo, 
$\bm{x}_i$ es el $i$-ésimo registro del conjunto de datos, 
$y_i \in \{-1, +1\}$ es la etiqueta asociada a dicho registro,
y la restricción garantiza que todas las observaciones queden correctamente clasificadas y a una distancia mínima de 
$\frac{1}{\lVert \bm{w} \rVert}$ del hiperplano.\\

\noindent
Pero hay un gran obstáculo, debido a que en la práctica los datos rara vez son perfectamente separables. Por ello, se introduce el concepto de \textbf{Margen Suave (\emph{Soft Margin})}, que permite violaciones controladas de las restricciones mediante variables de holgura $ \xi_i \ge 0 $.
El problema se redefine como:

\begin{equation}
	\text{Minimizar } 
	\frac{1}{2} \lVert \bm{w} \rVert^2 +
	C \sum_{i=1}^{n} \xi_i
	\quad \text{sujeto a }
	\begin{cases}
		y_i(\langle \bm{w}, \bm{x}_i \rangle + \beta) \ge 1 - \xi_i, \\
		\xi_i \ge 0,
	\end{cases}
	\label{eq:soft_margin}
\end{equation}
donde el parámetro $C > 0$ actúa como un \textbf{control de regularización}: valores grandes de $C$ penalizan más fuertemente los errores, buscando una separación más estricta (a costa de menor margen), mientras que valores pequeños permiten más errores, favoreciendo márgenes amplios y mayor generalización.\\


\noindent
La \textbf{Formulación Dual} del problema, en lo que respecta a los dos márgenes permite expresar la solución en términos de los productos internos entre las observaciones:

\begin{equation}
	\text{Maximizar }
	-\frac{1}{2} \sum_{i,\ell=1}^{n}
	\alpha_i \alpha_\ell y_i y_\ell
	K(\bm{x}_i, \bm{x}_\ell)
	+ \sum_{i=1}^{n} \alpha_i,
	\label{eq:dualsvm}
\end{equation}

\begin{equation}
	\text{sujeto a }
	0 \le \alpha_i \le C, \quad
	\sum_{i=1}^{n} \alpha_i y_i = 0.
\end{equation}

\noindent
Aquí, los \(\alpha_i\) son los multiplicadores de Lagrange asociados a las restricciones, y \(K(\bm{x}_i, \bm{x}_\ell)\) representa el producto interno entre las observaciones en un espacio transformado.\\


\noindent
En muchos casos, las clases no son separables linealmente en el espacio original de los datos. El \textbf{Kernel \emph{Trick}} permite proyectar los datos a un espacio de mayor dimensión (posiblemente infinito) donde sí exista un hiperplano separador, \emph{sin necesidad de calcular explícitamente la transformación}.\\

\noindent
Esto se logra sustituyendo los productos internos \(\langle \bm{x}_i, \bm{x}_\ell \rangle\) por una función \textbf{kernel} \(K(\bm{x}_i, \bm{x}_\ell)\), que computa directamente el producto interno en el espacio transformado.
De esta forma, se mantiene la eficiencia computacional mientras se obtiene un modelo capaz de representar fronteras de decisión no lineales.

\subsubsection{Funciones Kernel Comunes}

\noindent
En las siguientes definiciones, los vectores 
$\bm{a}, \bm{b} \in \mathbb{R}^D$ representan puntos en el 
espacio original de características o atributos. 

\paragraph{Kernel Lineal:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\langle \bm{a}, \bm{b} \rangle.
	\label{eq:kernel_lineal}
\end{equation}

\paragraph{Kernel Radial (RBF o Gaussiano):}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\exp \left( -\gamma \|\bm{a} - \bm{b}\|^2 \right).
	\label{eq:kernel_rbf}
\end{equation}

\paragraph{Kernel Polinómico:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	(\langle \bm{a}, \bm{b} \rangle + r)^\textbf{$d$}.
	\label{eq:kernel_polinomico}
\end{equation}

\paragraph{Kernel Sigmoide:}
\begin{equation}
	K(\bm{a}, \bm{b}) =
	\tanh(\gamma \langle \bm{a}, \bm{b} \rangle + r).
	\label{eq:kernel_sigmoid}
\end{equation}

\noindent
Cada kernel introduce parámetros que controlan la flexibilidad del modelo:
\begin{itemize}
	\item $\gamma > 0$: controla la influencia local de los puntos 
	(utilizado en el kernel RBF y el sigmoide).
	\item $r \in \mathbb{R}$: término independiente o sesgo 
	(usado en los kernels polinómico y sigmoide).
	\item $d \in \mathbb{N}$: grado del polinomio 
	en el kernel polinómico.
\end{itemize}

\noindent
\textbf{Hiperparámetros}

\begin{itemize}
	\item \textbf{Parámetro de Regularización (\(C\)):}
	Controla el equilibrio entre maximizar el margen y permitir errores de clasificación. 
	Valores altos priorizan clasificar correctamente los datos de entrenamiento; valores bajos generan un margen más amplio y toleran mayor error, donde $C > 0$.
	
	\item \textbf{Tipo de Kernel (\texttt{kernel}):}
	Determina la forma de la frontera de decisión. 
	Scikit-learn permite: \texttt{linear}, \texttt{poly}, \texttt{rbf} y \texttt{sigmoid}.
	% (y \texttt{precomputed}).
	
	\item \textbf{Parámetros del Kernel:}
	Dependiendo del tipo elegido, \texttt{scikit-learn} utiliza:
	\begin{itemize}
		\item \(\gamma\): controla la influencia de los puntos en kernels \texttt{rbf}, \texttt{poly} y \texttt{sigmoid}, $\gamma > 0 $.  
		Es equivalente a \( \tfrac{1}{2\sigma^2} \) en formulaciones gaussianas, aunque \texttt{scikit-learn} nunca usa \(\sigma\) explícitamente. Se consideran las variantes $\texttt{scaled}=(D \cdot \text{varianza de los datos de entrada})^{-1}$ y $\texttt{auto}=D ^{-1}$.
		\item \(d\) (\texttt{degree}): grado del polinomio cuando el kernel es \texttt{poly}, $d \ge 1$ .
		\item \(r\) (\texttt{coef0}): coeficiente independiente usado por los kernels \texttt{poly} y \texttt{sigmoid}.
	\end{itemize}
\end{itemize}



\chapter{Métricas de Rendimiento}


\noindent
El objetivo de este trabajo es evaluar el desempeño del calificador de cada modelo de Aprendizaje Automático introducidos en el Capítulo~\ref{Sec:Métodos}. Para alcanzarlo, se utilizan \textbf{métricas de rendimiento} que permiten cuantificar la capacidad del algoritmo, ergo son herramienta que se utilizan para saber qué tan bien predice un modelo a una clase o qué tan bien puede clasificar entre varias clases.\\

\noindent
\textbf{La importancia de las métricas} se ubica en que el objetivo central de estos algoritmos no es simplemente obtener un buen rendimiento en los datos utilizados para construir el modelo, sino en su \textbf{capacidad de generalización}, su habilidad para funcionar correctamente con entradas nuevas y previamente no observadas (no utilizadas en el entrenamiento). Esto se debe a que es perfectamente normal y sumamente esperable que el modelo funcione correctamente con el conjunto de datos que se utiliza para el entrenamiento del modelo, la idea fundamental es poder tener el mismo rendimiento o incluso uno mejor que con el conjunto de entrenamiento.\\

\noindent
Para obtener las métricas y entrenar el algoritmo se utiliza la estrategia de \textbf{Validación Cruzada $K$-fold}. En este método, el conjunto de datos se divide en $K$ grupos $\{G_1, G_2, \dots, G_K\}$ (o \textit{pliegues}) de igual tamaño. En cada iteración, un grupo $G_i$ ($1 \le i \le K$) se utiliza como conjunto de prueba, mientras que los $K-1$ restantes se emplean para el entrenamiento. Cada grupo actúa como conjunto de prueba exactamente una vez.
El valor final estimado de la métrica $M$, denotado por $\widehat{M}$, es el promedio de los valores obtenidos en cada iteración, es decir,
\begin{equation}
	\widehat{M} = \frac{1}{K} \sum_{i=1}^{K} M_i,
	\label{eq:kfolds}
\end{equation}
donde $M_i$ representa el valor de la métrica de evaluación cuando $G_i$ se utiliza como conjunto de prueba, para $i = 1, 2, \dots, K$.\\

\noindent
Dentro de este trabajo no solo se evalúan distintos modelos, sino que se utilizan distintos \emph{datasets} para lograrlos. A continuación se señalan las métricas que se utilizan en cada caso y se pueden apreciar algunas diferencias, leves, pero diferencias en sí. 

\section{Métricas para el Caso Binario}

%\noindent
%\subsection*{\textbf{Matriz de Confusión}}

\noindent
Una matriz de confusión, que se puede observar en la Tabla~\ref{tab:matrizconfusionbinario}, es una forma simple de saber de qué forma está clasificando el algoritmo, donde una clase es considerada \textbf{positiva $P$} y la otra \textbf{negativa $N$}. La matriz de confusión divide las predicciones en:

\begin{itemize}
	\item \textbf{Verdaderos Positivos (TP):} Casos positivos clasificados correctamente.
	\item \textbf{Verdaderos Negativos (TN):} Casos negativos clasificados correctamente.
	\item \textbf{Falsos Positivos (FP):} Casos negativos clasificados incorrectamente como positivos.
	\item \textbf{Falsos Negativos (FN):} Casos positivos clasificados incorrectamente como positivos. 
\end{itemize}


\begin{table}[htb]
	\centering
	\caption{Matriz de confusión.}
	\label{tab:matrizconfusionbinario}
	\begin{tabular}{|c|c|c|c|}
		\cline{3-4}
		\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{\textbf{Predicción}} \\
		\cline{3-4}
		\multicolumn{2}{c|}{} & \textbf{Positivo} & \textbf{Negativo} \\
		\hline
		\multirow{2}{*}{\textbf{Verdad}} & \textbf{Positivo} & \cellcolor{TPcolor} \textbf{Verdadero Positivo (TP)} & \cellcolor{FNcolor} \textbf{Falso Negativo (FN)} \\
		\cline{2-4}
		& \textbf{Negativo} & \cellcolor{FNcolor} \textbf{Falso Positivo (FP)} & \cellcolor{TPcolor} \textbf{Verdadero Negativo (TN)} \\
		\hline
	\end{tabular}
\end{table}

\noindent
A partir de la matriz de confusión se calculan varias métricas, algunas de las cuales se presentan a continuación.


\noindent
\subsection*{\textbf{\textit{Accuracy}}}


\noindent
El \textit{Accuracy} es la proporción de instancias clasificadas correctamente, es una medida ``ingenua'' que puede ser engañosa si existe un gran desbalance entre clases, ergo se puede obtener un \textit{Accuracy} alto si se predice una clase muy bien, que tiene una distribución mucho mayor que la otra, mientras que la de menor aparición casi no la predice. En términos de la matriz de confusión,
\begin{equation}
	\text{\textit{Accuracy}} = \frac{TP + TN}{TP + FP + TN + FN} = \frac{TP + TN}{\text{Total}}.
	\label{eq:accurcybinario}
\end{equation}
\noindent
En términos del conjunto de predicciones y valores verdaderos, se tiene que $n$ representa la cantidad total de ejemplos en el \emph{dataset}, mientras que $\widehat{y}_i$ es el valor predicho del $i$-ésimo ejemplo e $y_i$ es el valor verdadero correspondiente.
%, en cual caso si $\widehat{y}_l$ es igual al valor obtenido. La expresión $ 1(\widehat{y}_l = y_l)$ corresponde a la función indicadora, que toma el valor 1 si la predicción del ejemplo $l$ es correcta y 0 en caso contrario. Al sumar estos valores sobre todas las muestras, 
Se obtiene el número total de aciertos del modelo como:

\begin{equation}
	\textit{Accuracy}(y, \widehat{y}) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(\widehat{y}_i = y_i),
	\label{eq:accuracybinarioformula}
\end{equation}
\noindent
\noindent
donde la \textbf{función indicadora} se define como:
\[
\mathbb{I}(\widehat{y}_i = y_i) =
\begin{cases}
	1, & \text{si } \widehat{y}_i = y_i, \\
	0, & \text{en caso contrario}.
\end{cases}
\]\\
\noindent
Por lo tanto, se puede simplificar como la siguiente fórmula:

\begin{equation}
	\text{\textit{Accuracy}} = \frac{\text{Número de predicciones correctas}}{\text{Número total de muestras}}.
	\label{eq:accuracygeneral}
\end{equation}

\noindent
\subsection*{\textbf{\textit{Precisión}}}


\noindent
La \textit{Precisión} mide la probabilidad de que la predicción positiva del clasificador sea correcta, en otras palabras, mide qué tan bien predice las clases positivas el modelo.
En términos de la matriz de confusión, se puede expresar lo anterior de la siguiente manera:
\begin{equation}
	\text{\textit{Precisión}} = \frac{TP}{TP + FP}.
	\label{eq:precisionbinario}
\end{equation}

\noindent
\subsection*{\textbf{\textit{Recall}}}
\noindent
El \textit{Recall} o también conocido como Sensibilidad o Tasa de Verdaderos Positivos (TPR), mide la probabilidad de que el clasificador detecte un caso positivo cuando en verdad lo es.
\noindent
En términos de la matriz de confusión se puede entender al \textit{Recall} de la siguiente manera:

\begin{equation}
	Recall = \text{TPR} = \frac{TP}{TP + FN} = \frac{TP}{P}.
	\label{eq:recallbinario}
\end{equation}

\noindent
\subsection*{\textbf{\textit{F-measure}}}


\noindent
El \textit{F-measure} es la media armónica ponderada de \textit{Precisión} y \textit{Recall}, 
%La versión más común es el \textbf{F1-Score}, donde el parámetro de ponderación $\beta_f$ es igual a 1,. 
cuya fórmula general es:
\begin{equation}
	F_{\beta} = \frac{(1 + {\beta_f}^2) \, \text{\emph{Precisión}} \times Recall}{{\beta_f}^2 \text{\emph{Precisión}} + Recall},
	\label{eq:formulageneral}
\end{equation}

\noindent
donde $\beta_f > 0$ controla la importancia relativa entre la \emph{Precisión} y el \emph{Recall}.
En particular, la fórmula del F1-Score ($\beta_f=1$) en términos de \textit{Precisión} y \textit{Recall} se puede notar de la siguiente manera: 

\begin{equation}
	F1 = 2 \cdot \frac{Precisión \cdot Recall}{Precisión + Recall},
	\label{eq:f1general}
\end{equation}
\noindent
o en términos de la matriz de confusión, de forma más simplificada:
\begin{equation}
	F1 = \frac{2\,TP}{2\,TP + FP + FN}.
	\label{eq:f1enmatriz}
\end{equation}
Un clasificador perfecto tiene un valor de F1-Score igual a 1.

\noindent
\subsection*{\textbf{\textit{Área Bajo la Curva ROC ($AUC$)}}}


\noindent
La métrica \textit{$AUC$} es un valor que resume la capacidad de un clasificador para distinguir entre clases, siendo una métrica muy útil para comparar el desempeño entre modelos distintos o entre un mismo modelo con hiperparámetros distintos.\\

\noindent
La \textbf{Curva ROC} es un gráfico que ilustra el rendimiento de un clasificador binario a media que se varía su umbral de discriminación. Se crea graficando la \textbf{Tasa de Verdaderos Positivos (TPR)} versus la \textbf{Tasa de Falsos Positivos (FPR)} en varios umbrales. El $\bm{AUC}$ mide justamente el área debajo de la Curva ROC.\\

\noindent
Ejes utlizados para el gráfico:
\begin{itemize}
	\item[] \textbf{Eje Y:} TPR,
	\item[] \textbf{Eje X:} FPR.
\end{itemize}


\noindent
Un clasificador \textbf{ideal} se ubica en el punto $(0, 1)$, donde TPR=1 y FPR=0, lo que resulta en un \textbf{$AUC = 1$}. Un clasificador \textbf{aleatorio} se sitúa sobre la línea TPR = FPR, lo que resulta en un $AUC =$ 0.5. Un clasificador se considera \textbf{razonable} si 0.5 $< AUC \leq 1$.


\noindent
\section{Métricas para caso Multiclase}

\noindent
En este caso se utiliza el método \textit{``weighted''}, el cual tiene en cuenta el desequilibrio de clases calculando el promedio de las métricas binarias por clase, ponderadas según su frecuencia en el conjunto de datos reales.\\

\noindent
%Sea $\{1, 2, \dots, L\}$ el conjunto de etiquetas o clases. 
Para cada clase $C_l$, con $l \in \{1, 2, \dots, L\}$, se define $|C_l|$ como la cantidad de muestras reales pertenecientes a dicha clase.
% (las barras verticales representan la \textbf{cardinalidad} del conjunto de ejemplos con etiqueta $l$, es decir la cantidad de muestras que tiene esa clase). 
Asimismo, $M_l$ es el valor de la métrica binaria correspondiente a la clase $l$.\\

\noindent
La métrica ponderada, $\widehat{M}_{\text{weighted}}$, se define como:
\begin{equation}
	\widehat{M}_{\text{weighted}} =
	\frac{1}{\sum_{l =1}^L |C_l|}
	\sum_{l =1}^L |C_l| \cdot M_l.
	\label{eq:metricasponderadas}
\end{equation}

\noindent
De esta forma, las clases con mayor cantidad de muestras tienen una contribución proporcionalmente mayor en el valor final de la métrica evaluada.

\noindent
\subsection*{\textbf{Matriz de Confusión Multiclase}}


\noindent
La matriz de confusión multiclase, que se puede ver en la Tabla~\ref{tab:matrizmulticlase}, es una matriz cuadrada de tamaño $L \times L$, donde $L$ es el número de clases. Cada celda $C_{ij}$ representa la cantidad de muestras pertenecientes a la clase $C_i$ que fueron clasificadas como clase $C_j$ para $i,j \in \{ 1, 2, \dots, L\}$.\\

\noindent
Para cada clase $C_l$ se definen los valores que antes habíamos utilizados para la matriz de confusión del caso binario:
% en donde $Z$ es el total de instancias o la sumatoria de todas las celdas:
\begin{itemize}
	\item $TP_l = C_{ll}$; 
	\item $FP_l = \sum_{i=1,i \neq l}^{L} C_{il}$;
	\item $FN_l = \sum_{j=1, j \neq l}^{L}  C_{lj}$
	\item $TN_l = n - TP_l - FP_l - FN_l$.
\end{itemize}

\begin{table}[htb]%[htb][]
	\centering
	\caption{Matriz de confusión multiclase.}
	\label{tab:matrizmulticlase}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\cline{3-8}
		\multicolumn{2}{c|}{} & \multicolumn{6}{c|}{\textbf{Predicción}} \\
		\cline{3-8}
		\multicolumn{2}{c|}{} 
		& \textbf{Clase $C_1$} 
		& \textbf{Clase $C_2$} 
		& \textbf{$\cdots$} 
		& \textbf{Clase $C_l$}
		& \textbf{$\cdots$} 
		& \textbf{Clase $C_L$} \\
		\hline
		
		\multirow{5}{*}{\textbf{Verdad}} 
		
		% --- Fila C1 ---
		& \textbf{Clase $C_1$} 
		& \cellcolor{TPcolor} TN$_l$ 
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{FNcolor} FP$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{TPcolor} TN$_l$ \\ 
		\cline{2-8}
		
		% --- Fila C2 ---
		& \textbf{Clase $C_2$} 
		& \cellcolor{TPcolor} TN$_l$ 
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{FNcolor} FP$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{TPcolor} TN$_l$ \\
		\cline{2-8}
		
		% --- Filas intermedias ---
		& $\vdots$ 
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{FNcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$ \\
		\cline{2-8}
		
		% --- Fila Cl ---
		& \textbf{Clase $C_l$} 
		& \cellcolor{FNcolor} FN$_l$
		& \cellcolor{FNcolor} FN$_l$
		& \cellcolor{FNcolor} $\cdots$
		& \cellcolor{TPcolor} TP$_l$
		& \cellcolor{FNcolor} $\cdots$
		& \cellcolor{FNcolor} FN$_l$ \\
		\cline{2-8}
		
		% --- Filas intermedias ---
		& $\vdots$ 
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{FNcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$
		& \cellcolor{TPcolor} $\vdots$ \\
		\cline{2-8}
		
		% --- Fila CL ---
		& \textbf{Clase $C_L$}
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} TN$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{FNcolor} FP$_l$
		& \cellcolor{TPcolor} $\cdots$
		& \cellcolor{TPcolor} TN$_l$ \\
		\hline
	\end{tabular}
\end{table}


\noindent
\subsection*{\textbf{\textit{Precisión}}}

\noindent
La \textit{Precisión} para clase $C_l$ mide la proporción de muestras clasificadas como positivas que realmente pertenecen a la clase $C_l$, donde $l \in \{1, 2, \dots, L\}$. En términos más simple, utilizando la matriz de confusión:

\begin{equation}
	\text{\emph{Precisión}}_l = \frac{TP_l}{TP_l + FP_l}.
	\label{eq:precisionmulticlase}
\end{equation}

\noindent
\subsection*{\textit{Recall}}

\noindent
El \textit{Recall} para clase $C_l$, donde $l \in \{1, 2, \dots, L\}$, mide la proporción de muestras verdaderamente positivas de la clase $C_l$ que fueron correctamente identificadas. En términos más simple, utilizando la matriz de confusión:

\begin{equation}
	Recall_l = \frac{TP_l}{TP_l + FN_l}.
	\label{eq:recallmulticlase}
\end{equation}

\noindent
\subsection*{\textit{F1-measure}}

\noindent
El valor global ponderado se obtiene aplicando la fórmula de $M_{\text{weighted}}$ sobre los $F_{l}$, para $\beta = 1$:

\begin{equation}
	F_{\text{weighted}} = \sum_{l =1}^ L w_l \, F_{l}, \quad 
	\text{con } w_l = \frac{|C_l|}{\sum\limits_{l=1}^{L} |C_l|}.
	\label{eq:f1formula}
\end{equation}

\noindent
\subsection*{\textit{Área Bajo la Curva ROC ($AUC$)}}

\noindent
Para extender la métrica $AUC$ a clasificación multiclase se emplea el enfoque \textbf{One-vs-Rest (OVR)}. Para cada clase $C_l$ ($l \in \{1, 2, \dots, L\}$), se considera la clase $C_l$ como positiva y el resto como negativas; luego, se calcula el valor correspondiente $AUC_l$ a partir de la curva ROC del problema binario generado para esa clase. Finalmente, el $AUC$ multiclase se obtiene como un promedio ponderado por el soporte de cada clase.


\begin{equation}
	\text{$AUC$}_{\text{weighted}} = \sum_{l =1}^ L w_l \, \text{$AUC$}_l \quad 
	\text{con } w_l = \frac{|C_l|}{\sum\limits_{l=1}^{L} |C_l|}.
	\label{eq:onevsrest}
\end{equation}
%{eq:onevsrest}{One-vs-Rest (OVR)}


\noindent
\subsection*{Importancia de la característica}


\noindent Sea un modelo predictivo $\mathcal{M}$ entrenado sobre un conjunto de datos $\mathcal{D}$, y sea $M$ la métrica de referencia del modelo sobre los datos originales. El procedimiento se detalla a continuación:

\begin{enumerate}
	
	\item Para cada atributo $j$ del conjunto de datos:
	\begin{enumerate}
		\item Repetir el siguiente proceso $K$ veces (para reducir la varianza de la estimación):
		\begin{enumerate}
			\item Generar una versión alterada del conjunto de datos, $\mathcal{D}^{(k,j)}$, en la que se permuta aleatoriamente, se intercambian de orden los datos de la columna correspondiente al atributo $X_j$, manteniendo las demás columnas sin cambios, para ver si el modelo empeora o no en su rendimiento.
			
			\item Calcular la métrica $M$ del modelo sobre los datos permutados:
			\begin{equation}
				M_{k,j} = \text{valor de la métrica $M$ del modelo $\mathcal{M}$ con $\mathcal{D}^{(k,j)}$}.
				\label{eq:puntaje_referencia_datos_permutados}
			\end{equation}
			
		\end{enumerate}
		\item Calcular la importancia del atributo $X_j$ como la disminución promedio en el puntaje del modelo respecto del puntaje de referencia:
		\begin{equation}
			I_j = M - \frac{1}{K} \sum_{k=1}^{K} M_{k,j}.
			\label{eq:puntaje_referencia_final}
		\end{equation}
		
	\end{enumerate}
\end{enumerate}


\noindent De esta manera, $I_j$ mide la pérdida de desempeño al romper la relación entre el atributo $X_j$ y la variable objetivo. Valores más altos de $I_j$ indican características más relevantes para el modelo.\\





\chapter{Resultados}
\label{ch:resultados}

\noindent
En este capítulo se presentan los resultados obtenidos mediante la aplicación de los modelos de aprendizaje supervisado sobre los distintos conjuntos de datos. Se analizan las métricas de evaluación alcanzadas, las configuraciones óptimas halladas mediante búsqueda en malla (\emph{Grid Search}) y la importancia relativa de las características más influyentes en las predicciones.
Se busca evaluar el rendimiento de cada modelo bajo diferentes configuraciones de hiperparámetros, a través de métricas como la precisión (\textit{Accuracy}), el F1-Score y el área bajo la curva ROC ($AUC$), entre otras.\\

\noindent
Durante la fase experimental se exploraron distintas estrategias de balanceo de clases, dado que varios de los conjuntos presentan desbalances significativos entre las clases. En particular, se probó la técnica de \textbf{\emph{undersampling}}, reduciendo la cantidad de ejemplos de la clase mayoritaria para equilibrar el \emph{dataset}. Sin embargo, esta estrategia no arrojó resultados satisfactorios: los modelos tienden a perder capacidad de generalización, mostrando un descenso notable en las métricas de validación, aunque se sostiene la mejor configuración con mejor valor de métricas. Por este motivo, se opta finalmente por mantener la distribución original y aplicar técnicas de regularización y ajuste de hiperparámetros para mitigar el sesgo hacia la clase dominante.\\


\section{\emph{Dataset} Binario}

\noindent
En esta sección se presentan los resultados obtenidos al aplicar los distintos modelos de aprendizaje automático al \textbf{\emph{dataset} binario}. En este caso, la tarea de clasificación consiste en distinguir entre dos clases posibles, lo que típicamente permite a los algoritmos aprender fronteras de decisión más simples en comparación con escenarios multiclase.

\paragraph{Naïve Bayes}{\hspace{-0.2cm}obtuvo un desempeño sólido en la tarea de clasificación binaria, especialmente considerando su supuesto fuerte de independencia condicional entre atributos.Si bien este supuesto rara vez se cumple estrictamente en datos reales, el modelo suele mantener un rendimiento competitivo.
	En este trabajo, evaluamos su desempeño utilizando la métricas distitas métricas, y consideramos que un valor superior a 0.80 constituye un nivel de desempeño adecuado para el tipo de problema abordado.
	La mejor configuración se alcanzó con cualquier suavizado, no hubo diferencias. Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:nbg_resultados_binario}.  El modelo logra un \emph{Accuracy} y un F1-Score de 0.8420, junto con un $AUC$ de 0.9138, lo que refleja una muy buena capacidad clasificadora siendo un método tan eficaz, además de ser bastante similar con los resultados de los modelos a los cuales se compara.
	El tiempo de entrenamiento fue de apenas 0.10 segundos, siendo que la mayor fortaleza es la rapidez de su entrenamiento.}


\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros para NB Gaussiano en el caso binario.}
	\label{tab:grid_nb_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Suavizado & $10^{-9},\; 10^{-8},\; 10^{-7},\; 10^{-6},\; 10^{-5}$
		\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales de NB Gaussiano en el caso binario.}
	\label{tab:nbg_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Todas las consideradas
			%			var\_smoothing =\\
			%			 Cualquiera\\
		} & 0.8420 & 0.8420 & 0.8420 & 0.9138 & 0.10 & 0.8433 \\
		\bottomrule
	\end{tabular}
\end{table}


\paragraph{Regresión Logística}{\hspace{-0.2cm}obtuvo un correcto desempeño en la clasificación binaria, siempre tomando un buen resultado cuando cualquier métrica por lo menos supere el valor de 0.80.
	Los resultados cuantitativos se resumen en la Tabla~\ref{tab:rl_resultados_binario}, en la cual se muestra la mejor configuración obtenida de los hiperparámetros de la Tabla~\ref{tab:grid_rl_bin}. 
	El modelo logra un \emph{Accuracy}
	y un F1-Score de 0.8445,
	junto con un $AUC$ de 0.9050, lo que refleja una buena capacidad discriminatoria, pero siendo datos médicos se requieren incluso mejores valores.
	El tiempo de entrenamiento fue de apenas 0.13 segundos, lo que lo convierte en una opción eficiente para este tipo de problema, teniendo en cuenta el tamaño pequeño del \emph{dataset}.}

\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros para RL en el caso binario.}
	\label{tab:grid_rl_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & 0, 0.1, 0.01 \\
		Penalidad & Ninguna, Lasso, Ridge, Elastic Net \\
		\emph{Solver} & L-BFGS, SAGA, Newton-CG \\
		Multiclase & OvR, \emph{multinomial} \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[H]%[H]
	\centering
	\caption{Resultados finales del RL en el caso binario.}
	\label{tab:rl_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Penalidad = Lasso\\
			\emph{Solver} = SAGA \\
			Multiclase = OvR
		} & 0.8485 & 0.8485 & 0.8481 & 0.9050 & 0.13 & 0.8495 \\
		\bottomrule
	\end{tabular}
\end{table}


\paragraph{Random Forest}{\hspace{-0.2cm}demostró un desempeño bastante sólido en la clasificación binaria, mostrando alta capacidad de generalización para registros no vistos durante el entrenamiento.
	Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rf_resultados_binario}, donde el modelo logra un \emph{Accuracy} y un F1-Score de 0.8780, junto con un $AUC$ de 0.9315, lo que refleja una excelente capacidad de clasificación, pero evidentemente la tarea de creación de los árboles y del voto mayoritario lo vuelven un método bastante costoso aunque efectivo.
	El tiempo de entrenamiento fue de 1.38 segundos, siendo considerablemente el modelo en el cual más se tarda en obtener los resultados.}


\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros para RF en el caso binario.}
	\label{tab:grid_rf_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Criterio & \texttt{gini}, \texttt{entropy} \\
		\texttt{max\_depth} & Ninguna, 3, 5, 7, 9 \\
		\texttt{min\_samples\_split} & 2, 5, 10 \\
		\texttt{min\_samples\_leaf} & 1, 2, 4 \\
		\texttt{max\_features} & \texttt{None}, \texttt{sqrt}, \texttt{log2} \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Resultados finales de RF en el caso binario.}
	\label{tab:rf_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Criterio = \texttt{entropy}\\
			\texttt{max\_depth} = 7\\
			\texttt{min\_samples\_split} = 5\\
			\texttt{min\_samples\_leaf} = 1\\
			\texttt{max\_features} = \texttt{sqrt}\\
		} & 0.8780 & 0.8780 & 0.8775 & 0.9315 & 1.38 & 0.8731 \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Máquinas de Soporte Vectorial}{\hspace{-0.2cm}obtuvo un desempeño sólido en la clasificación binaria, mostrando un equilibrio adecuado entre precisión y generalización. Demostrando la solidez habitual con que es descripto este método.
	Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:svm_resultados_binario}. 
	El modelo logra un \emph{Accuracy} y un F1-Score de 0.8616, junto con un $AUC$ de 0.9232, lo que refleja la mejor capacidad de clasificación hasta ahora.
	El tiempo de entrenamiento fue de apenas 0.60 segundos, siendo un tiempo acotado, pero costoso si vemos que el tamaño del \emph{dataset} es bastante pequeño, refleja un costo de evaluación más grande que con RL.}

\begin{table}[htb]%[H]
	\centering
	\caption{Grilla de hiperparámetros para SVM en el caso binario.}
	\label{tab:grid_svm_bin}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & 0.001, 0.01, 0.1, 1, 10, 15, 20, 25 \\
		\texttt{kernel} & \texttt{linear}, \texttt{poly}, \texttt{rbf}, \texttt{sigmoid} \\
		$\gamma$ & \texttt{scale}, \texttt{auto}, 0.001, 0.01, 0.1, 1 \\
		\texttt{degree} & $2, 3, \dots, 10$ \\
		\bottomrule
	\end{tabular}
\end{table}



\begin{table}[H]%[H]
	\centering
	\caption{Resultados finales de SVM en el caso binario.}
	\label{tab:svm_resultados_binario}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			\texttt{kernel} = \texttt{rbf}\\
			$\gamma$ = 0.1\\
		} & 0.8616 & 0.8616 & 0.8609 &0.9232 &0.60 & 0.8628 \\
		\bottomrule
	\end{tabular}
\end{table}


\noindent
En la Figura \ref{fig:mejores_modelos_binario} se observa que Random Forest obtiene la mayor puntuación en todas las métricas, seguido por la Regresión Logística y SVM. Esto indica que, para nuestro \emph{dataset}, el modelo RF presenta mejor capacidad de generalización, mientras que los demás modelos muestran un desempeño bastante competitivo. Esto no quiere decir que sea el mejor modelo, sino que es el que mejor clasifica este \emph{dataset}, pero si se quieren resultados sólidos con menor tiempo de cálculo, entonces el NB es una buena solución. Podemos afirmar que todos los modelos tuvieron desempeños correctos.

\begin{figure}[htb]%[H]
	\centering
	\includegraphics[width=0.8\textwidth]{../mejores_modelos_barras_binario}
	\caption{Comparación del desempeño de los mejores modelos en el caso binario.}
	\label{fig:mejores_modelos_binario}
\end{figure}

\subsubsection*{Importancia de las Características}


\noindent La importancia de las características se calculó mediante el método de \textit{Permutation Feature Importance}. Este método evalúa cuánto se degrada el desempeño del modelo cuando se altera aleatoriamente una característica, manteniendo fijas las demás. Cuanto mayor sea la disminución en la métrica de desempeño, mayor será la importancia atribuida a dicha característica.\\


\noindent Los resultados obtenidos se presentan en las Tablas~\ref{tab:nb_importancia},~\ref{tab:rl_importancia_},\ref{tab:rf_importancia} y~\ref{tab:svm_importancia}, correspondientes a los modelos  NB, RL, RF y SVM, respectivamente.

\begin{table}[htb]
	\centering
	\caption{Importancia de las características para NB en el caso binario.}
	\label{tab:nb_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ST\_Slope & 0.027015 \\
		ExerciseAngina & 0.023747 \\
		Oldpeak & 0.018736 \\
		ChestPainType & 0.018519 \\
		Cholesterol & 0.014815 \\
		Sex & 0.014270 \\
		FastingBS & 0.004575 \\
		RestingBP & 0.001852 \\
		MaxHR & -0.000218 \\
		RestingECG & -0.001198 \\
		Age & -0.003595 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características para RL.}
	\label{tab:rl_importancia_}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ST\_Slope & 0.072440 \\
		ExerciseAngina & 0.026797 \\
		ChestPainType   &  0.020806\\
		Sex           &    0.011329\\
		FastingBS      &   0.010240\\
		Cholesterol    &   0.009150\\
		Oldpeak        &   0.006100\\
		Age            &   0.003922\\
		MaxHR          &   0.003268\\
		RestingBP      &   0.000545\\
		RestingECG     &   0.000218\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características para RF en el caso binario.}
	\label{tab:rf_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ST\_Slope & 0.254265 \\
		ChestPainType & 0.127319 \\
		Oldpeak & 0.113156 \\
		ExerciseAngina & 0.105952 \\
		Cholesterol & 0.099872 \\
		MaxHR & 0.088635 \\
		Age & 0.065807 \\
		RestingBP & 0.055053 \\
		Sex & 0.040916 \\
		FastingBS & 0.030069 \\
		RestingECG & 0.018956 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[H]%[H]
	\centering
	\caption{Importancia de las características para SVM en el caso binario.}
	\label{tab:svm_importancia}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ST\_Slope     &     0.106209\\
		Cholesterol    &   0.031808\\
		Oldpeak        &   0.024510\\
		ChestPainType  &   0.023312\\
		Sex            &   0.011438\\
		MaxHR           &  0.008715\\
		ExerciseAngina  &  0.008388\\
		Age            &   0.007952\\
		RestingBP       &  0.005773\\
		RestingECG      &  0.004902\\
		FastingBS      &   0.004575\\
		\bottomrule
	\end{tabular}
\end{table}

\noindent
Las Figuras \ref{fig:evolucion_metricas_binario}, \ref{fig:evolucion_metricas_binario_auc} y \ref{fig:evolucion_metricas_binario_f1}  muestran cómo las métricas del modelo mejoran al incorporar las características más relevantes según su importancia. Se observa que inicialmente, con pocas características, el desempeño es limitado, siendo prudente incrementar la cantidad de características. En algunos casos es muy sorprendentemente, ya que se obtiene un buen resultado, y a medida que se agregan las variables de mayor relevancia, las métricas tienden a incrementarse hasta estabilizarse. Este comportamiento permite identificar el conjunto de características que maximiza el rendimiento sin necesidad de incluir todas las variables disponibles, optimizando tanto la complejidad del modelo como el tiempo de entrenamiento.\\

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{../evolucion_metrica_binario}
	\caption{Valores de \textit{Accuracy} según la cantidad de características en el caso binario.}
	\label{fig:evolucion_metricas_binario}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{../evolucion_metrica_binario_auc}
	\caption{Valores de $AUC$ según la cantidad de características en el caso binario.}
	\label{fig:evolucion_metricas_binario_auc}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{../evolucion_metrica_binario_f1}
	\caption{Valores de F1-Score según la cantidad de características en el caso binario.}
	\label{fig:evolucion_metricas_binario_f1}
\end{figure}





\noindent
Siendo los modelos \textbf{SVM} y \textbf{RF} los que mejor comportamiento tienen a lo largo del incremento de características, donde podemos ver en su gran mayoría un incremento del valor de las métricas a medida que se aumentan las caracterizaras. Además, como se puede observar, tienen valores aceptables para pocas características.


\section{\emph{Dataset} Multiclase}


\noindent
En esta sección se presentan los resultados obtenidos al aplicar los distintos modelos de aprendizaje automático al \textbf{\emph{dataset} multiclase}. A diferencia del problema binario, este conjunto de datos requiere que los algoritmos distingan entre varias categorías posibles, lo cual introduce desafíos adicionales como una separación más compleja entre clases, mayor variabilidad interna y posibles desbalances entre categorías. 

\paragraph{Naïve Bayes}{\hspace{-0.2cm}obtuvo un buen desempeño en la clasificación multiclase, considerando su supuesto de independencia entre atributos. 
	Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:nbg_resultados}. 
	El modelo logra un \emph{Accuracy} y un F1-Score de 0.8208, junto con un $AUC$ de 0.91, lo que refleja una buena capacidad clasificadora, pero lo suficientemente menor a los modelos con los cuales se compara, para decidir que no es útil en cuanto a tener buenos valores de métricas a la hora de resolver este problema. Es evidente que se mantuvo en valores similares al caso binario, sin tener caídas aún cuando se incrementa el tamaño del \emph{dataset} o se dificulta el mismo con un caso multiclase, pero tampoco muestra mejora como el resto de modelos.
	El tiempo de entrenamiento fue de apenas 0.11 segundos, siendo su mayor fortaleza, incluso con más datos el modelo se mantiene en valores parecidos de tiempo de ejecución.}

\begin{table}[H]
	\centering
	\caption{Grilla de hiperparámetros para NB Gaussiano en el caso multiclase.}
	\label{tab:grid_nb_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Suavizado & $10^{-9},\; 10^{-8},\; 10^{-7},\; 10^{-6},\; 10^{-5}$\\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[H]
	\centering
	\caption{Resultados finales de NB Gaussiano en el caso multiclase.}
	\label{tab:nbg_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Todas las consideradas
			%			var\_smoothing =\\
			%			 Cualquiera\\
		} & 0.8208 & 0.8208 & 0.8366 & 0.9112 & 0.11 & 0.8747 \\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Regresión Logística}{\hspace{-0.2cm}obtuvo un muy buen desempeño en la clasificación multiclase, mostrando un equilibrio adecuado entre precisión y generalización.
	Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rl_resultados}, junto con la configuración de hiperparámetros que logro esos resultados. 
	El modelo logra un \emph{Accuracy} y un F1-Score de 0.8978 respectivamente, junto con un $AUC$ de 0.9644, lo que refleja una buena capacidad discriminatoria, logrando un buen resultado al incremento de datos y de dificultad al existir más de dos clases.
	El tiempo de entrenamiento fue de apenas 0.30 segundos, lo que lo convierte en una opción eficiente para este tipo de problema, teniendo en cuanta que se mantiene en rangos acotados, incluso con más datos que en el caso binario.}


\begin{table}[H]
	\centering
	\caption{Grilla de hiperparámetros para RL en el caso multiclase.}
	\label{tab:grid_rl_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & 0.01, 0.1, 1, 10 \\
		Penalidad & Ninguna, Ridge, Elastic Net \\
		\emph{Solver} & L-BFGS, SAGA, Newton-CG \\
		Multiclase & OvR \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption{Resultados finales de RL en el caso multiclase.}
	\label{tab:rl_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 1$\\
			Penalidad = Ninguna\\
			\emph{Solver} = Newton-CG\\
			Multiclase = OvR
		} & 0.8978 & 0.8978 & 0.8953 & 0.9644 & 0.30 & 0.8945 \\
		\bottomrule
	\end{tabular}
\end{table}





\paragraph{Random Forest}{\hspace{-0.2cm}demostró un desempeño claramente superador en la clasificación multiclase, mostrando alta capacidad de generalización incluso para un \emph{dataset} más grande y con caso multiclase.
	Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:rf_resultados}, junto con la mejor configuración con la cual se obtuvieron estos resultados. 
	El modelo logra un \emph{Accuracy} y un F1-Score de 0.9432, junto con un $AUC$ de 0.9868, lo que refleja una buena excelente capacidad clasificadora. Se entienden que los árboles de decisión no se ven afectados por casos multiclase y que la presencia de más cantidad de registros los enriquecen para realizar mejor clasificaciones. 
	El tiempo de entrenamiento fue de 2.24 segundos, mostrando como estos modelos incrementan su tiempo de ejecución a mayores datos requieren procesar.}


\begin{table}[htb]
	\centering
	\caption{Grilla de hiperparámetros para RF en el caso multiclase.}
	\label{tab:grid_rf_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		Criterio & \texttt{gini}, \texttt{entropy} \\
		\texttt{max\_depth} & Ninguna, 3, 5, 7, 9 \\
		\texttt{min\_samples\_split} & 2, 5, 10 \\
		\texttt{min\_samples\_leaf} & 1, 2, 4 \\
		\texttt{max\_features} & \texttt{None}, \texttt{sqrt}, \texttt{log2} \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[H]
	\centering
	\caption{Resultados finales de RF en el caso multiclase.}
	\label{tab:rf_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			Criterio = \texttt{gini}\\
			\texttt{max\_depth} = Ninguna\\
			\texttt{min\_samples\_split} = 2\\
			\texttt{min\_samples\_leaf} = 1\\
			\texttt{max\_features} = \texttt{sqrt}\\
		} & 0.9432 & 0.9432 &0.9412 & 0.9868 & 2.24& 0.9417\\
		\bottomrule
	\end{tabular}
\end{table}

\paragraph{Máquinas de Soporte Vecotorial}{\hspace{-0.2cm}obtuvo un desempeño bastante sólido en la clasificación multiclase, mostrando unos grandes valores en sus métricas. 
	Los resultados cuantitativos finales se resumen en la Tabla~\ref{tab:svm_resultados}, junto con la mejor configuración que logro las mejoras métricas. 
	El modelo logra un \emph{Accuracy} y un F1-Score de 0.9082, junto con un $AUC$ de 0.9556, lo que refleja una muy buena capacidad de clasificación, logrando ser un método útil para \emph{datasets} acotados y grandes, como a casos binarios y multiclase. 
	El tiempo de entrenamiento fue de apenas 1.42 segundos, mostrando una clara demora a cuantos más datos deben de procesar, si comparamos como sube el tiempo de ejecución con el caso binario.}


\begin{table}[H]
	\centering
	\caption{Grilla de hiperparámetros para SVM en el caso multiclase.}
	\label{tab:grid_svm_multi}
	\begin{tabular}{ll}
		\toprule
		\textbf{Hiperparámetro} & \textbf{Valores evaluados} \\
		\midrule
		$C$ & 0.001, 0.01, 0.1, 1, 10, 15, 20, 25 \\
		\texttt{kernel} & \texttt{linear}, \texttt{poly}, \texttt{rbf}, \texttt{sigmoid} \\
		$\gamma$ & \texttt{scale}, \texttt{auto}, 0.001, 0.01, 0.1, 1 \\
		\texttt{degree} & $2,3, \dots,10$ \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{table}[H]
	\centering
	\caption{Resultados finales de SVM en el caso multiclase.}
	\label{tab:svm_resultados}
	\begin{tabular}{lcccccc}
		\toprule
		\textbf{Configuración} & \textbf{\emph{Accuracy}} & \textbf{Recall} & \textbf{F1-Score} & $\bm{AUC}$ & \textbf{Tiempo (s)} & \textbf{Precisión} \\
		\midrule
		\makecell[l]{
			$C = 0.1$\\
			\texttt{kernel} = \texttt{poly}\\
			$\gamma = 1$\\
			\texttt{degree} = 2\\
		} & 0.9082 & 0.9082 & 0.9064 & 0.9556 & 1.45 & 0.9057 \\
		\bottomrule
	\end{tabular}
\end{table}


En la Figura \ref{fig:mejores_modelos_multiclase} se observa que Random Forest obtiene la mayor puntuación en todas las métricas, seguido por RL y SVM. Esto indica que, para nuestro \emph{datasets}, el modelo RF presenta mejor capacidad de generalización, mientras que los demás modelos muestran un desempeño bastante competitivo.
\FloatBarrier
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{../mejores_modelos_barras_multiclase}
	\caption{Comparación del desempeño de los mejores modelos en el caso multiclase.}
	\label{fig:mejores_modelos_multiclase}
\end{figure}
\FloatBarrier
\noindent



\subsubsection*{Importancia de las Características}

\noindent Los resultados obtenidos se presentan en las Tablas~\ref{tab:nb_importancia_multiclase},~\ref{tab:rl_importancia_multiclase}~\ref{tab:rf_importancia_multiclase} y ~\ref{tab:svm_importancia_multiclase} correspondientes a los modelos NB, RL, RF y SVM, respectivamente.
Las Figuras \ref{fig:evolucion_metricas_multiclase}, \ref{fig:evolucion_metricas_multiclase_f1} y \ref{fig:evolucion_metricas_multiclase_auc} muestran cómo las métricas del modelo mejoran al incorporar las características más relevantes según su importancia. Se observa que inicialmente, con pocas características, el desempeño es menor al obtenido anteriormente, pero mucho mejor del esperable. En algunos casos, de forma muy sorprendentemente, se obtiene un buen resultado ya a los pocos atributos utilizados, y a medida que se agregan las variables de mayor relevancia, las métrica tienden a incrementarse hasta estabilizarse.\\

\noindent
Este comportamiento permite identificar el conjunto de características que maximiza el rendimiento sin necesidad de incluir todas las variables disponibles, optimizando tanto la complejidad del modelo como el tiempo de entrenamiento. Se ven como \textbf{RF} muestra resultados muy prometedores incluso con pocas métricas, siendo claramente superior en los valores de métricas incluso con pocos atributos.



\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características para NB Gaussiano en el caso multiclase.}
	\label{tab:nb_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		AC       & 0.057163 \\
		DP       & 0.018676 \\
		ALTV     & 0.015461 \\
		ASTV     & 0.005106 \\
		DS       & 0.002695 \\
		UC       & 0.002364 \\
		FM       & 0.001371 \\
		Variance & 0.001087 \\
		Nzeros   & 0.001040 \\
		Nmax     & -0.000993 \\
		Tendency & -0.001040 \\
		Mode     & -0.001324 \\
		Max      & -0.001371 \\
		Min      & -0.001986 \\
		LB       & -0.001986 \\
		MLTV     & -0.002222 \\
		Width    & -0.002459 \\
		Median   & -0.003310 \\
		MSTV     & -0.004965 \\
		Mean     & -0.005768 \\
		DL       & -0.006809 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htb]%[htb]
	\centering
	\caption{Importancia de las características para RL en el caso multiclase.}
	\label{tab:rl_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		Mean    &    0.098487\\
		AC       &   0.084113\\
		ASTV     &   0.057069\\
		Median   &   0.031631\\
		DP       &   0.029740\\
		LB       &   0.023404\\
		Variance &   0.022270\\
		UC       &   0.022080\\
		ALTV     &   0.019243\\
		Max      &   0.018109\\
		Nmax     &   0.014374\\
		Mode     &   0.011348\\
		Min      &   0.005910\\
		MSTV     &   0.004208\\
		FM       &   0.003830\\
		Tendency &   0.003546\\
		MLTV     &   0.002979\\
		Nzeros   &   0.002837\\
		DL       &   0.001655\\
		Width    &   0.000189\\
		DS       &   0.000000\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htb]
	\centering
	\caption{Importancia de las características para RF en el caso multiclase.}
	\label{tab:rf_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ASTV      &  0.139807\\
		ALTV      &  0.109941\\
		MSTV      &  0.104823\\
		Mean      &  0.091579\\
		AC        &  0.063645\\
		Mode       & 0.061986\\
		Median      &0.060633\\
		DP        &  0.047945\\
		LB        &  0.045324\\
		MLTV      &  0.045132\\
		Variance  &  0.040531\\
		UC        &  0.039166\\
		Width     &  0.030551\\
		Min       &  0.030109\\
		Max       &  0.027147\\
		FM        &  0.020801\\
		Nmax      &  0.018407\\
		DL        &  0.011128\\
		Tendency  &  0.007652\\
		Nzeros    &  0.003405\\
		DS        &  0.000287\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[htb]%[H]
	\centering
	\caption{Importancia de las características para SVM en el caso multiclase.}
	\label{tab:svm_importancia_multiclase}
	\begin{tabular}{lc}
		\toprule
		\textbf{Característica} & \textbf{Importancia (Permutación)} \\
		\midrule
		ASTV     &   0.050355\\
		ALTV     &   0.037069\\
		UC       &   0.030638\\
		AC       &   0.026903\\
		DP       &   0.018345\\
		Mean     &   0.015887\\
		Mode     &   0.014988\\
		Median   &   0.014043\\
		Nmax     &   0.011915\\
		MSTV     &   0.009125\\
		DL       &   0.005059\\
		Variance &   0.004775\\
		Nzeros   &   0.004586\\
		Min      &   0.004444\\
		Max      &   0.004350\\
		MLTV     &   0.003357\\
		Tendency &   0.003026\\
		FM       &   0.002459\\
		Width    &   0.001418\\
		DS       &   0.000000\\
		LB       &  -0.000804\\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{../evolucion_metrica_multiclase}
	\caption{Valores de \textit{Accuracy} según la cantidad de características en el caso multiclase.}
	\label{fig:evolucion_metricas_multiclase}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{../evolucion_metrica_multiclase_auc}
	\caption{Valores de $AUC$ según la cantidad de características en el caso multiclase.}
	\label{fig:evolucion_metricas_multiclase_auc}
\end{figure}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{../evolucion_metrica_multiclase_F1}
	\caption{Valores de F1-Score según la cantidad de características en el caso multiclase..}
	\label{fig:evolucion_metricas_multiclase_f1}
\end{figure}



\chapter{Conclusiones}

\section{Análisis General e Inferencias}

\noindent
Del análisis de los resultados obtenidos se puede observar que el modelo \textbf{Random Forest} alcanzó reiteradamente los mejores valores en todas las métricas, tanto en el problema binario como en el multiclase. Esto se debe a su capacidad de combinar múltiples árboles de decisión, lo que permite capturar relaciones no lineales y reducir el sobreajuste, sobretodo a la hora de hacer un voto mayoritario de estos mismos árboles que permiten una representación mejor.\\

\noindent
El modelo de \textbf{Máquinas de Soporte Vectorial} mostró también un rendimiento muy bueno, especialmente con el kernel \textbf{RBF} en el caso binario y el \textbf{polinómico} en el caso multiclase, destacándose su capacidad para definir fronteras de decisión complejas en espacios transformados.\\

\noindent
\textbf{Regresión Logística} presentó resultados positivos y de buena generalización, aunque con menor capacidad para capturar patrones que los otros modelos, no por ser malos resultados, sino que el resto tuvo mejores valores de métricas. Por su parte, el modelo \textbf{Naïve Bayes} ofreció un rendimiento aceptable, siendo el más liviano computacionalmente, aunque con limitaciones inherentes a su supuesto de independencia de las variables. Esto no quita que aunque posea este supuesto, es el más liviano y rápido de los modelos obteniendo resultados sumamente buenos.\\

\noindent
En cuanto a la \textbf{importancia de las características}, se identificaron atributos dominantes en cada conjunto de datos:\\

\noindent
En el caso binario, variables como \textit{ST\_Slope}: este segmento del ECG se analiza porque cambia cuando hay isquemia, es decir, cuando el corazón recibe menos sangre de la necesaria; \textit{ChestPainType}: el tipo de dolor en el pecho, asociado a isquemia cardíaca; y \textit{Oldpeak}: componente utilizado para estimar el riesgo de isquemia o infarto de miocardio, fueron recurrentemente relevantes.\\

\noindent
En el caso multiclase destacaron \textit{ASTV}: porcentaje de tiempo con variabilidad anormal a corto plazo; \textit{ALTV}: porcentaje de tiempo con variabilidad anormal a largo plazo; y \textit{MSTV}: valor medio de la variabilidad a largo plazo.


\section{Mejoras Potenciales y Consideraciones}

\noindent
Para optimizar aún más las métricas, podrían explorarse las siguientes estrategias:

\begin{itemize}
	\item \textbf{Ajuste más fino de hiperparámetros:} empleando \emph{Randomized Search} o \emph{Bayesian Optimization} para reducir tiempos de búsqueda, y luego utlizar un \emph{Grid Search} en los hiperparámetros encontrados.
	\item \textbf{Manipulación de características:} Reducción de dimensionalidad (PCA) o creación de variables sintéticas, para observar mejor las importancias de cada característica.
	\item \textbf{Validación cruzada más robusta:} utilizando más particiones para estimar mejor la generalización.
\end{itemize}

En conjunto, los modelos demostraron un desempeño satisfactorio, con un claro potencial de mejora mediante el refinamiento de hiperparámetros y una mejor comprensión de la estructura de los datos.

\bibliographystyle{plain}
\bibliography{Referencias}
\addcontentsline{toc}{chapter}{Bibliografía}

\end{document}