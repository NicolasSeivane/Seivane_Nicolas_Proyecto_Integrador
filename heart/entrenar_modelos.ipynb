{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58caa5f9",
   "metadata": {},
   "source": [
    "# **Evaluacion de Data set**\n",
    "Tecnicatura en inteligencia artificial\n",
    "\n",
    "Universidad Nacional de Hurlingham\n",
    "\n",
    "Docente: Andrea Rey\n",
    "\n",
    "Alumno : Nicolas Seivane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf875c79",
   "metadata": {},
   "source": [
    "Librerias que voy a utilizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54015c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install scikit-learn seaborn matplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     accuracy_score,\n\u001b[0;32m      6\u001b[0m     precision_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     cohen_kappa_score\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install scikit-learn seaborn matplotlib\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    balanced_accuracy_score,\n",
    "    log_loss,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold# .head()\n",
    "# .info()\n",
    "# .isnull().any()\n",
    "# df.column.value_counts()\n",
    "# .max()\n",
    "# .min()\n",
    "# .describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2566f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos = pd.read_csv(\"C:\\\\Users\\\\User\\\\Documents\\\\Proyecto integrador\\\\heart\\\\heart.csv\")\n",
    "\n",
    "datos.drop_duplicates(inplace=True)\n",
    "datos.dropna(inplace=True)\n",
    "#datos.drop(columns=[\"CLASS\"], inplace=True)\n",
    "#datos['NSP'] = datos['NSP'].replace({1: 'Normal', 2: 'Sospechoso', 3: 'Patologico'})\n",
    "indices = datos[datos['Cholesterol'] == 0].index\n",
    "datos.drop(indices, inplace=True)\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e266065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "datos.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49913c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>918.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>918.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>53.510893</td>\n",
       "      <td>132.396514</td>\n",
       "      <td>198.799564</td>\n",
       "      <td>0.233115</td>\n",
       "      <td>136.809368</td>\n",
       "      <td>0.887364</td>\n",
       "      <td>0.553377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.432617</td>\n",
       "      <td>18.514154</td>\n",
       "      <td>109.384145</td>\n",
       "      <td>0.423046</td>\n",
       "      <td>25.460334</td>\n",
       "      <td>1.066570</td>\n",
       "      <td>0.497414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>-2.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>173.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>54.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>267.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>603.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age   RestingBP  Cholesterol   FastingBS       MaxHR  \\\n",
       "count  918.000000  918.000000   918.000000  918.000000  918.000000   \n",
       "mean    53.510893  132.396514   198.799564    0.233115  136.809368   \n",
       "std      9.432617   18.514154   109.384145    0.423046   25.460334   \n",
       "min     28.000000    0.000000     0.000000    0.000000   60.000000   \n",
       "25%     47.000000  120.000000   173.250000    0.000000  120.000000   \n",
       "50%     54.000000  130.000000   223.000000    0.000000  138.000000   \n",
       "75%     60.000000  140.000000   267.000000    0.000000  156.000000   \n",
       "max     77.000000  200.000000   603.000000    1.000000  202.000000   \n",
       "\n",
       "          Oldpeak  HeartDisease  \n",
       "count  918.000000    918.000000  \n",
       "mean     0.887364      0.553377  \n",
       "std      1.066570      0.497414  \n",
       "min     -2.600000      0.000000  \n",
       "25%      0.000000      0.000000  \n",
       "50%      0.600000      1.000000  \n",
       "75%      1.500000      1.000000  \n",
       "max      6.200000      1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02599b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columna 'Age' tiene media: 52.8820, valor maximo: 77.0000 y valor minimo: 28.0000\n",
      "\n",
      "Columna 'Sex' (candidata categórica): 2 valores únicos\n",
      "   Hay 75.60% codificado en M\n",
      "   Hay 24.40% codificado en F\n",
      "\n",
      "Columna 'ChestPainType' (candidata categórica): 4 valores únicos\n",
      "   Hay 22.25% codificado en ATA\n",
      "   Hay 22.65% codificado en NAP\n",
      "   Hay 49.60% codificado en ASY\n",
      "   Hay 5.50% codificado en TA\n",
      "\n",
      "Columna 'RestingBP' tiene media: 133.0228, valor maximo: 200.0000 y valor minimo: 92.0000\n",
      "\n",
      "Columna 'Cholesterol' tiene media: 244.6354, valor maximo: 603.0000 y valor minimo: 85.0000\n",
      "\n",
      "Columna 'FastingBS' (candidata categórica): 2 valores únicos\n",
      "   Hay 83.24% codificado en 0\n",
      "   Hay 16.76% codificado en 1\n",
      "\n",
      "Columna 'RestingECG' (candidata categórica): 3 valores únicos\n",
      "   Hay 59.65% codificado en Normal\n",
      "   Hay 16.76% codificado en ST\n",
      "   Hay 23.59% codificado en LVH\n",
      "\n",
      "Columna 'MaxHR' tiene media: 140.2265, valor maximo: 202.0000 y valor minimo: 69.0000\n",
      "\n",
      "Columna 'ExerciseAngina' (candidata categórica): 2 valores únicos\n",
      "   Hay 61.53% codificado en N\n",
      "   Hay 38.47% codificado en Y\n",
      "\n",
      "Columna 'Oldpeak' tiene media: 0.9016, valor maximo: 6.2000 y valor minimo: -0.1000\n",
      "\n",
      "Columna 'ST_Slope' (candidata categórica): 3 valores únicos\n",
      "   Hay 46.78% codificado en Up\n",
      "   Hay 47.45% codificado en Flat\n",
      "   Hay 5.76% codificado en Down\n",
      "\n",
      "Columna 'HeartDisease' (candidata categórica): 2 valores únicos\n",
      "   Hay 52.28% codificado en 0\n",
      "   Hay 47.72% codificado en 1\n"
     ]
    }
   ],
   "source": [
    "n_filas = len(datos)\n",
    "\n",
    "for column in datos.columns:\n",
    "    unicos = datos[column].unique()\n",
    "    n_unicos = len(unicos)\n",
    "    \n",
    "    # Si es categórica (tipo object/category) o es numérica con pocos únicos\n",
    "    if (datos[column].dtype == 'object') or (len(unicos) <= 10):\n",
    "        print(f\"\\nColumna '{column}' (candidata categórica): {n_unicos} valores únicos\")\n",
    "        \n",
    "        \n",
    "        for unico in unicos:\n",
    "                count = len(datos[datos[column] == unico])\n",
    "                normalizado = count / n_filas\n",
    "                print(f\"   Hay {normalizado:.2%} codificado en {unico}\")\n",
    "    else:\n",
    "        print(f\"\\nColumna '{column}' tiene media: {datos[column].mean():.4f}, valor maximo: {datos[column].max():.4f} y valor minimo: {datos[column].min():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in datos.columns:\n",
    "    if len(datos[column].unique()) <= 10 and datos[column].dtype == 'object':\n",
    "        print(column, datos[column].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in datos.columns:\n",
    "    if len(datos[column].unique()) <= 10 and datos[column].dtype == 'object':\n",
    "        unicos = datos[column].unique()\n",
    "        for i in range(len(unicos)):\n",
    "            datos[column] = datos[column].replace({unicos[i]: i})\n",
    "        print(column, datos[column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff355da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in datos.columns:\n",
    "    if len(datos[column].unique()) > 4:\n",
    "        datos[column] = (datos[column] - datos[column].mean()) / datos[column].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos = datos.columns[:-1]\n",
    "concepto = datos.columns[-1]\n",
    "\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion_cruzada(conjunto, atributos, concepto, k=5, random=False, agregar_unos=False, undersample=False, oversample=False):\n",
    "  \n",
    "  if undersample:\n",
    "    cantidad_menor_concepto = conjunto[concepto].value_counts().min()\n",
    "    conjunto = pd.concat([conjunto[conjunto[concepto] == valor].sample(n=cantidad_menor_concepto, random_state=42) for valor in conjunto[concepto].unique()])\n",
    "\n",
    "  if oversample:\n",
    "    cantidad_mayor_concepto = conjunto[concepto].value_counts().max()\n",
    "    conjunto = pd.concat([conjunto[conjunto[concepto] == valor].sample(n=cantidad_mayor_concepto, replace=True, random_state=42) for valor in conjunto[concepto].unique()])\n",
    "\n",
    "  if random: conjunto = conjunto.sample(frac=1, random_state=11).reset_index(drop=True)\n",
    "\n",
    "  x = conjunto[atributos]\n",
    "  if agregar_unos: x.insert(0, \"Unos\", 1)\n",
    "\n",
    "  y = conjunto[concepto]\n",
    "\n",
    "  resultados = []\n",
    "  skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "  for train_index, test_index in skf.split(x, y):\n",
    "        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        resultados.append((x_train, y_train, x_test, y_test))\n",
    "\n",
    "  return resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f583c6",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier(\n",
    "    n_estimators=100,      # número de árboles en el bosque\n",
    "    criterion=\"gini\",      # función de calidad de la división (\"gini\" o \"entropy\")\n",
    "    max_depth=None,        # profundidad máxima de cada árbol\n",
    "    min_samples_split=2,   # min muestras para dividir un nodo\n",
    "    min_samples_leaf=1,    # min muestras en una hoja\n",
    "    max_features=\"sqrt\",   # nº de features a considerar en cada split (\"sqrt\", \"log2\", None)\n",
    "    bootstrap=True,        # si usar bootstrap samples para entrenar cada árbol\n",
    "    random_state=42,       # semilla\n",
    "    n_jobs=-1              # usa todos los cores\n",
    ")\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "GaussianNB(\n",
    "    var_smoothing=1e-9     # suavizado para evitar divisiones por cero\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVC(\n",
    "    C=1.0,                 # penalización del error (mayor C = menos margen, más sobreajuste)\n",
    "    kernel=\"rbf\",          # kernel: \"linear\", \"poly\", \"rbf\", \"sigmoid\"\n",
    "    degree=3,              # grado del polinomio (si kernel=\"poly\")\n",
    "    gamma=\"scale\",         # controla la influencia de un solo ejemplo (\"scale\", \"auto\" o un valor numérico)\n",
    "    probability=True,     # si calcular probabilidades (más lento)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression(\n",
    "    penalty=\"l2\",          # regularización: \"l1\", \"l2\", \"elasticnet\", None\n",
    "    C=1.0,                 # inverso de la fuerza de regularización (menor C = más regularización)\n",
    "    solver=\"lbfgs\",        # optimizador (\"lbfgs\", \"saga\", \"liblinear\", \"newton-cg\")\n",
    "    max_iter=1000,         # iteraciones máximas (importante en datasets grandes)\n",
    "    multi_class=\"multinomial\",    # \"ovr\" (uno-vs-rest) o \"multinomial\"\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    balanced_accuracy_score,\n",
    "    log_loss,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "\n",
    "    y_true_total.extend(y_test)\n",
    "    y_pred_total.extend(y_pred)\n",
    "\n",
    "print(\"Matriz de confusión total:\\n\", confusion_matrix(y_true_total, y_pred_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641bd291",
   "metadata": {},
   "source": [
    "LogisticRegression(\n",
    "    penalty=\"l2\",          # regularización: \"l1\", \"l2\", \"elasticnet\", None\n",
    "    C=1.0,                 # inverso de la fuerza de regularización (menor C = más regularización)\n",
    "    solver=\"lbfgs\",        # optimizador (\"lbfgs\", \"saga\", \"liblinear\", \"newton-cg\")\n",
    "    max_iter=1000,         # iteraciones máximas (importante en datasets grandes)\n",
    "    multi_class=\"multinomial\",    # \"ovr\" (uno-vs-rest) o \"multinomial\"\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18a94f",
   "metadata": {},
   "source": [
    "# *Regresion Logistica*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb045fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l1\",\"l2\",\"elasticnet\", None],  # 'l1' si usas solver='liblinear' o 'saga'\n",
    "    \"solver\": [\"lbfgs\",\"saga\", \"newton-cg\"],# según el penalty que uses, liblinear no soporta None, newton-cg no soporta l1 ni elasticnet\n",
    "    \"multi_class\": [\"ovr\", \"multinomial\"] # si es multiclass\n",
    "}\n",
    " \n",
    "## cohen_kappa_score,     accuracy_score,    precision_score,     recall_score,    f1_score,    roc_auc_score,\n",
    "\n",
    "\n",
    "#with open(\"regresion_logistica_resultados.txt\", \"w\") as f:\n",
    "with open(\"regresion_logistica_resultados_corazon.txt\", \"w\") as f:\n",
    "    f.write(\"Resultados de Regresion Logistica con validacion cruzada (5 folds):\\n\")\n",
    "    \n",
    "    for C in grid[\"C\"]:\n",
    "\n",
    "        for penalty in grid[\"penalty\"]:\n",
    "            for multi_class in grid[\"multi_class\"]:\n",
    "                for solver in grid[\"solver\"]:\n",
    "                    # chequeo de compatibilidad\n",
    "                    if penalty == \"l1\" and solver not in [\"liblinear\", \"saga\"]:\n",
    "                        continue\n",
    "                    if penalty == \"elasticnet\" and solver != \"saga\":\n",
    "                        continue\n",
    "                    if penalty is None and solver not in [\"lbfgs\",\"newton-cg\",\"sag\",\"saga\"]:\n",
    "                        continue\n",
    "                    if penalty is None and solver not in [\"lbfgs\",\"newton-cg\",\"sag\",\"saga\"]:\n",
    "                        continue\n",
    "                    scoring_funcs = {\n",
    "                                \"cohen_kappa\": 0,\n",
    "                                \"accuracy\": 0,\n",
    "                                \"precision\": 0,\n",
    "                                \"recall\": 0,\n",
    "                                \"f1\": 0,\n",
    "                                \"roc_auc\": 0\n",
    "                            }\n",
    "                    try:\n",
    "                        inicio = time.time()\n",
    "                        modelo = LogisticRegression(\n",
    "                            C=C,\n",
    "                            penalty=penalty,\n",
    "                            solver=solver,\n",
    "                            l1_ratio=0.5 if penalty==\"elasticnet\" else None,\n",
    "                            multi_class=multi_class,\n",
    "                            max_iter=1000,\n",
    "                            random_state=42\n",
    "                        )\n",
    "\n",
    "                        folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True)\n",
    "                        \n",
    "                        for x_train, y_train, x_test, y_test in folds:\n",
    "                                modelo.fit(x_train, y_train)\n",
    "                                pred = modelo.predict(x_test)\n",
    "                                scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                probs = modelo.predict_proba(x_test)\n",
    "                                if len(set(y_test)) == 2:\n",
    "                                        # binaria → prob de la clase positiva\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                else:\n",
    "                                        # multiclase → usar esquema OVR\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "                                \n",
    "                        fin = time.time()\n",
    "                        tiempo_total = fin - inicio\n",
    "                        for key in scoring_funcs:\n",
    "                            scoring_funcs[key] /= len(folds) \n",
    "                        f.write(f\"\\nC={C}, penalty={penalty}, solver={solver}, multi_class={multi_class} --> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        f.write(f\"\\nC={C}, penalty={penalty}, solver={solver}, multi_class={multi_class} --> ERROR: {str(e)}\\n\")\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"------------------------------------------------------\\n\\n\")\n",
    "    f.write(\"Resultados de Regresion Logistica con validacion cruzada (5 folds) undersample:\\n\")\n",
    "    \n",
    "    for C in grid[\"C\"]:\n",
    "\n",
    "        for penalty in grid[\"penalty\"]:\n",
    "            for solver in grid[\"solver\"]:\n",
    "                for multi_class in grid[\"multi_class\"]:\n",
    "                    # chequeo de compatibilidad\n",
    "                    if penalty == \"l1\" and solver not in [\"liblinear\", \"saga\"]:\n",
    "                        continue\n",
    "                    if penalty == \"elasticnet\" and solver != \"saga\":\n",
    "                        continue\n",
    "                    if penalty is None and solver not in [\"lbfgs\",\"newton-cg\",\"sag\",\"saga\"]:\n",
    "                        continue\n",
    "                    scoring_funcs = {\n",
    "                                \"cohen_kappa\": 0,\n",
    "                                \"accuracy\": 0,\n",
    "                                \"precision\": 0,\n",
    "                                \"recall\": 0,\n",
    "                                \"f1\": 0,\n",
    "                                \"roc_auc\": 0\n",
    "                            }\n",
    "                    try:\n",
    "                        tiempo_inicio = time.time()\n",
    "                        modelo = LogisticRegression(\n",
    "                            C=C,\n",
    "                            penalty=penalty,\n",
    "                            solver=solver,\n",
    "                            l1_ratio=0.5 if penalty==\"elasticnet\" else None,\n",
    "                            multi_class=multi_class,\n",
    "                            max_iter=1000,\n",
    "                            random_state=42\n",
    "                        )\n",
    "\n",
    "                        folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True, undersample=True)\n",
    "\n",
    "                        for x_train, y_train, x_test, y_test in folds:\n",
    "                                modelo.fit(x_train, y_train)\n",
    "                                pred = modelo.predict(x_test)\n",
    "                                scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                probs = modelo.predict_proba(x_test)\n",
    "                                if len(set(y_test)) == 2:\n",
    "                                        # binaria → prob de la clase positiva\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                else:\n",
    "                                        # multiclase → usar esquema OVR\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "\n",
    "                        fin = time.time()\n",
    "                        tiempo_total = fin - tiempo_inicio\n",
    "                        for key in scoring_funcs:\n",
    "                            scoring_funcs[key] /= len(folds)  # Promediar las métricas\n",
    "\n",
    "                        f.write(f\"\\nC={C}, penalty={penalty}, solver={solver}, multi_class={multi_class} --> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        f.write(f\"\\nC={C}, penalty={penalty}, solver={solver}, multi_class={multi_class} --> ERROR: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0002c1",
   "metadata": {},
   "source": [
    "RandomForestClassifier(\n",
    "    n_estimators=100,      # número de árboles en el bosque\n",
    "    criterion=\"gini\",      # función de calidad de la división (\"gini\" o \"entropy\")\n",
    "    max_depth=None,        # profundidad máxima de cada árbol\n",
    "    min_samples_split=2,   # min muestras para dividir un nodo\n",
    "    min_samples_leaf=1,    # min muestras en una hoja\n",
    "    max_features=\"sqrt\",   # nº de features a considerar en cada split (\"sqrt\", \"log2\", None)\n",
    "    bootstrap=True,        # si usar bootstrap samples para entrenar cada árbol\n",
    "    random_state=42,       # semilla\n",
    "    n_jobs=-1              # usa todos los cores\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e697b",
   "metadata": {},
   "source": [
    "# *Random Forest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebf028",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"n_estimators\": [50, 100, 200, 500, 1000],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": [None, 3, 5, 7, 9],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#with open(\"Random_forest.txt\", \"w\") as f:\n",
    "with open(\"Random_forest_corazon.txt\", \"w\") as f:\n",
    "    f.write(\"Resultados de Random Forest con validacion cruzada (5 folds):\\n\")\n",
    "\n",
    "    for criterion in grid[\"criterion\"]:\n",
    "        for max_depth in grid[\"max_depth\"]:\n",
    "            for min_samples_split in grid[\"min_samples_split\"]:\n",
    "                for min_samples_leaf in grid[\"min_samples_leaf\"]:\n",
    "                    for max_features in grid[\"max_features\"]:\n",
    "                        scoring_funcs = {\n",
    "                                    \"cohen_kappa\": 0,\n",
    "                                    \"accuracy\": 0,\n",
    "                                    \"precision\": 0,\n",
    "                                    \"recall\": 0,\n",
    "                                    \"f1\": 0,\n",
    "                                    \"roc_auc\": 0\n",
    "                                }\n",
    "                        try:\n",
    "                            inicio = time.time()\n",
    "                            modelo = RandomForestClassifier(\n",
    "                                criterion=criterion,\n",
    "                                max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split,\n",
    "                                min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features,\n",
    "                                random_state=42\n",
    "                            )\n",
    "\n",
    "                            folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True)\n",
    "\n",
    "                            for x_train, y_train, x_test, y_test in folds:\n",
    "                                    modelo.fit(x_train, y_train)\n",
    "                                    pred = modelo.predict(x_test)\n",
    "                                    scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                    scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                    scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                    probs = modelo.predict_proba(x_test)\n",
    "                                    if len(set(y_test)) == 2:\n",
    "                                        # binaria → prob de la clase positiva\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                    else:\n",
    "                                        # multiclase → usar esquema OVR\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "                                    \n",
    "                            fin = time.time()\n",
    "                            tiempo_total = fin - inicio\n",
    "                            for key in scoring_funcs:\n",
    "                                scoring_funcs[key] /= len(folds) \n",
    "                            f.write(f\"\\ncriterion={criterion}, max_depth={max_depth}, min_samples_split={min_samples_split}, \"\n",
    "                                    f\"min_samples_leaf={min_samples_leaf}, max_features={max_features} \"\n",
    "                                    f\"--> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "                        except Exception as e:\n",
    "                            f.write(f\"\\ncriterion={criterion}, max_depth={max_depth}, min_samples_split={min_samples_split}, \"\n",
    "                                    f\"min_samples_leaf={min_samples_leaf}, max_features={max_features} --> ERROR: {str(e)}\\n\")\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"------------------------------------------------------\\n\\n\")\n",
    "    f.write(\"Resultados de Random Forest con validacion cruzada (5 folds) undersample:\\n\")\n",
    "    \n",
    "    for criterion in grid[\"criterion\"]:\n",
    "        for max_depth in grid[\"max_depth\"]:\n",
    "            for min_samples_split in grid[\"min_samples_split\"]:\n",
    "                for min_samples_leaf in grid[\"min_samples_leaf\"]:\n",
    "                    for max_features in grid[\"max_features\"]:\n",
    "                        scoring_funcs = {\n",
    "                                    \"cohen_kappa\": 0,\n",
    "                                    \"accuracy\": 0,\n",
    "                                    \"precision\": 0,\n",
    "                                    \"recall\": 0,\n",
    "                                    \"f1\": 0,\n",
    "                                    \"roc_auc\": 0\n",
    "                                }\n",
    "                        try:\n",
    "                            inicio = time.time()\n",
    "                            modelo = RandomForestClassifier(\n",
    "                                criterion=criterion,\n",
    "                                max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split,\n",
    "                                min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features,\n",
    "                                random_state=42\n",
    "                            )\n",
    "\n",
    "                            folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True, undersample=True)\n",
    "\n",
    "                            for x_train, y_train, x_test, y_test in folds:\n",
    "                                    modelo.fit(x_train, y_train)\n",
    "                                    pred = modelo.predict(x_test)\n",
    "                                    scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                    scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                    scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                    probs = modelo.predict_proba(x_test)\n",
    "                                    if len(set(y_test)) == 2:\n",
    "                                        # binaria → prob de la clase positiva\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                    else:\n",
    "                                        # multiclase → usar esquema OVR\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "                                    \n",
    "                            fin = time.time()\n",
    "                            tiempo_total = fin - inicio\n",
    "                            for key in scoring_funcs:\n",
    "                                scoring_funcs[key] /= len(folds) \n",
    "                            f.write(f\"\\ncriterion={criterion}, max_depth={max_depth}, min_samples_split={min_samples_split}, \"\n",
    "                                    f\"min_samples_leaf={min_samples_leaf}, max_features={max_features} \"\n",
    "                                    f\"--> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "                        except Exception as e:\n",
    "                            f.write(f\"\\ncriterion={criterion}, max_depth={max_depth}, min_samples_split={min_samples_split}, \"\n",
    "                                    f\"min_samples_leaf={min_samples_leaf}, max_features={max_features} --> ERROR: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09398c",
   "metadata": {},
   "source": [
    "GaussianNB(\n",
    "    var_smoothing=1e-9     # suavizado para evitar divisiones por cero\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5e7f4",
   "metadata": {},
   "source": [
    "# *Naive Bayes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee672f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]  \n",
    "}\n",
    "\n",
    "#with open(\"Naiva_Bayes.txt\", \"w\") as f:\n",
    "with open(\"Naiva_Bayes_corazon.txt\", \"w\") as f:\n",
    "    f.write(\"Resultados de Naive Bayes con validacion cruzada (5 folds):\\n\")\n",
    "\n",
    "    for var_smoothing in grid[\"var_smoothing\"]:\n",
    "                        scoring_funcs = {\n",
    "                                    \"cohen_kappa\": 0,\n",
    "                                    \"accuracy\": 0,\n",
    "                                    \"precision\": 0,\n",
    "                                    \"recall\": 0,\n",
    "                                    \"f1\": 0,\n",
    "                                    \"roc_auc\": 0\n",
    "                                }\n",
    "                        try:\n",
    "                            inicio = time.time()\n",
    "                            modelo = GaussianNB(var_smoothing=var_smoothing)\n",
    "\n",
    "                            folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True)\n",
    "\n",
    "                            for x_train, y_train, x_test, y_test in folds:\n",
    "                                    modelo.fit(x_train, y_train)\n",
    "                                    pred = modelo.predict(x_test)\n",
    "                                    scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                    scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                    scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                    probs = modelo.predict_proba(x_test)\n",
    "                                    if len(set(y_test)) == 2:\n",
    "                                        # binaria → prob de la clase positiva\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                    else:\n",
    "                                        # multiclase\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "                                    \n",
    "                            fin = time.time()\n",
    "                            tiempo_total = fin - inicio\n",
    "                            for key in scoring_funcs:\n",
    "                                scoring_funcs[key] /= len(folds) \n",
    "                            f.write(f\"\\n var_smoothing={var_smoothing}--> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            f.write(f\"\\n var_smoothing={var_smoothing}--> ERROR: {str(e)}\\n\")\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"------------------------------------------------------\\n\\n\")\n",
    "    f.write(\"Resultados de Naive Bayes con validacion cruzada (5 folds) undersample:\\n\")\n",
    "\n",
    "    for var_smoothing in grid[\"var_smoothing\"]:\n",
    "                        scoring_funcs = {\n",
    "                                    \"cohen_kappa\": 0,\n",
    "                                    \"accuracy\": 0,\n",
    "                                    \"precision\": 0,\n",
    "                                    \"recall\": 0,\n",
    "                                    \"f1\": 0,\n",
    "                                    \"roc_auc\": 0\n",
    "                                }\n",
    "                        try:\n",
    "                            inicio = time.time()\n",
    "                            modelo = GaussianNB(var_smoothing=var_smoothing)\n",
    "                            folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True, undersample=True)\n",
    "\n",
    "                            for x_train, y_train, x_test, y_test in folds:\n",
    "                                    modelo.fit(x_train, y_train)\n",
    "                                    pred = modelo.predict(x_test)\n",
    "                                    scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                    scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                    scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                    probs = modelo.predict_proba(x_test)\n",
    "                                    if len(set(y_test)) == 2:\n",
    "                                        # binaria → prob de la clase positiva\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                    else:\n",
    "                                        # multiclase\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "                                    \n",
    "                            fin = time.time()\n",
    "                            tiempo_total = fin - inicio\n",
    "                            for key in scoring_funcs:\n",
    "                                scoring_funcs[key] /= len(folds) \n",
    "                            f.write(f\"\\n var_smoothing={var_smoothing}--> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            f.write(f\"\\n var_smoothing={var_smoothing}--> ERROR: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892db97d",
   "metadata": {},
   "source": [
    "SVC(\n",
    "    C=1.0,                 # penalización del error (mayor C = menos margen, más sobreajuste)\n",
    "    kernel=\"rbf\",          # kernel: \"linear\", \"poly\", \"rbf\", \"sigmoid\"\n",
    "    degree=3,              # grado del polinomio (si kernel=\"poly\")\n",
    "    gamma=\"scale\",         # controla la influencia de un solo ejemplo (\"scale\", \"auto\" o un valor numérico)\n",
    "    probability=True,     # si calcular probabilidades (más lento)\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01183e",
   "metadata": {},
   "source": [
    "# *SVM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cc57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 15, 20, 25],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\", 0.001, 0.01, 0.1, 1],\n",
    "    \"degree\": [2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "#with open(\"SVM.txt\", \"w\") as f:\n",
    "with open(\"SVM_corazon.txt\", \"w\") as f:\n",
    "    f.write(\"Resultados de SVM con validacion cruzada (5 folds):\\n\")\n",
    "\n",
    "    for C in grid[\"C\"]:\n",
    "        for kernel in grid[\"kernel\"]:\n",
    "            for gamma in grid[\"gamma\"]:\n",
    "                degrees_to_use = grid[\"degree\"] if kernel == \"poly\" else [3]\n",
    "                for degree in degrees_to_use:\n",
    "                        scoring_funcs = {\n",
    "                                \"cohen_kappa\": 0,\n",
    "                                 \"accuracy\": 0,\n",
    "                                    \"precision\": 0,\n",
    "                                    \"recall\": 0,\n",
    "                                    \"f1\": 0,\n",
    "                                    \"roc_auc\": 0\n",
    "                                }\n",
    "                        try:\n",
    "                            inicio = time.time()\n",
    "                            modelo = SVC(\n",
    "                                  C=C,\n",
    "                                    kernel=kernel,\n",
    "                                    gamma=gamma,\n",
    "                                    degree=degree,\n",
    "                                    max_iter=1000,\n",
    "                                    random_state=42,\n",
    "                                    probability=True\n",
    "                            )\n",
    "\n",
    "                            folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True)\n",
    "\n",
    "                            for x_train, y_train, x_test, y_test in folds:\n",
    "                                    modelo.fit(x_train, y_train)\n",
    "                                    pred = modelo.predict(x_test)\n",
    "                                    scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                    scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                    scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                    probs = modelo.predict_proba(x_test)\n",
    "\n",
    "                                    if len(set(y_test)) == 2:\n",
    "                                        # binaria\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                    else:\n",
    "                                        # multiclase\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "                                    \n",
    "                            fin = time.time()\n",
    "                            tiempo_total = fin - inicio\n",
    "                            for key in scoring_funcs:\n",
    "                                scoring_funcs[key] /= len(folds) \n",
    "                            f.write(f\"\\nC={C}, kernel={kernel}, gamma={gamma}, degree={degree} --> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            f.write(f\"\\nC={C}, kernel={kernel}, gamma={gamma}, degree={degree} --> ERROR: {str(e)}\\n\")\n",
    "    f.write(\"\\n\\n\")\n",
    "    f.write(\"------------------------------------------------------\\n\\n\")\n",
    "    f.write(\"Resultados de SVM con validacion cruzada (5 folds) undersample:\\n\")\n",
    "\n",
    "    for C in grid[\"C\"]:\n",
    "        for kernel in grid[\"kernel\"]:\n",
    "            for gamma in grid[\"gamma\"]:\n",
    "                if kernel != \"poly\" and \"degree\" in grid:\n",
    "                    continue\n",
    "                for degree in grid[\"degree\"]:\n",
    "                        scoring_funcs = {\n",
    "                                    \"cohen_kappa\": 0,\n",
    "                                    \"accuracy\": 0,\n",
    "                                    \"precision\": 0,\n",
    "                                    \"recall\": 0,\n",
    "                                    \"f1\": 0,\n",
    "                                    \"roc_auc\": 0\n",
    "                                }\n",
    "                        try:\n",
    "                            inicio = time.time()\n",
    "                            modelo = SVC(\n",
    "                                  C=C,\n",
    "                                    kernel=kernel,\n",
    "                                    gamma=gamma,\n",
    "                                    degree=degree if kernel==\"poly\" else None,\n",
    "                                    max_iter=1000,\n",
    "                                    random_state=42,\n",
    "                                    probability=True\n",
    "                            )\n",
    "                            folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True, undersample=True)\n",
    "\n",
    "                            for x_train, y_train, x_test, y_test in folds:\n",
    "                                    modelo.fit(x_train, y_train)\n",
    "                                    pred = modelo.predict(x_test)\n",
    "                                    scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                                    scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                                    scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                                    scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                                    probs = modelo.predict_proba(x_test)\n",
    "\n",
    "                                    if len(set(y_test)) == 2:\n",
    "                                        # binaria\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                                    else:\n",
    "                                        # multiclase\n",
    "                                        scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "                                    \n",
    "                            fin = time.time()\n",
    "                            tiempo_total = fin - inicio\n",
    "                            for key in scoring_funcs:\n",
    "                                scoring_funcs[key] /= len(folds) \n",
    "                            f.write(f\"\\nC={C}, C={C}, gamma={gamma}, degree={degree} --> {scoring_funcs}, tiempo_total={tiempo_total:.2f} segundos\\n\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            f.write(f\"\\nC={C}, C={C}, gamma={gamma}, degree={degree} --> ERROR: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ef41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "best_lr_model = LogisticRegression(C=1.0, penalty='l1', solver='saga', multi_class='ovr', max_iter=1000, random_state=42)\n",
    "best_svm_model = SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42, probability=True)\n",
    "best_gnb_model = GaussianNB(var_smoothing=1e-09)\n",
    "best_rf_model = RandomForestClassifier(criterion='entropy', max_depth=7, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', random_state=42)\n",
    "\n",
    "\n",
    "best_lr_model.fit(datos[atributos], datos[concepto])\n",
    "best_svm_model.fit(datos[atributos], datos[concepto])\n",
    "best_gnb_model.fit(datos[atributos], datos[concepto])\n",
    "best_rf_model.fit(datos[atributos], datos[concepto])\n",
    "\n",
    "with open(\"Importancia_corazon.txt\", \"w\") as f:\n",
    "    f.write(\"La importancia de las features:\\n\")\n",
    "\n",
    "    f.write(\"Random Forest Importancia atributos:\")\n",
    "    feature_importances_rf = pd.Series(best_rf_model.feature_importances_, index=atributos)\n",
    "    f.write(str(feature_importances_rf.sort_values(ascending=False)))\n",
    "    f.write(\"-\" * 30)\n",
    "\n",
    "\n",
    "    f.write(\"Logistic Regression Importancia Permutacion:\")\n",
    "    result_lr = permutation_importance(best_lr_model, datos[atributos], datos[concepto], n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    feature_importances_lr = pd.Series(result_lr.importances_mean, index=atributos)\n",
    "    f.write(str(feature_importances_lr.sort_values(ascending=False)))\n",
    "    f.write(\"-\" * 30)\n",
    "\n",
    "    f.write(\"SVM Importancia Permutacion:\")\n",
    "    result_svm = permutation_importance(best_svm_model, datos[atributos], datos[concepto], n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    feature_importances_svm = pd.Series(result_svm.importances_mean, index=atributos)\n",
    "    f.write(str(feature_importances_svm.sort_values(ascending=False)))\n",
    "    f.write(\"-\" * 30)\n",
    "\n",
    "    f.write(\"Gaussiano Naive Bayes Importancia Permutacion:\")\n",
    "    result_gnb = permutation_importance(best_gnb_model, datos[atributos], datos[concepto], n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    feature_importances_gnb = pd.Series(result_gnb.importances_mean, index=atributos)\n",
    "    f.write(str(feature_importances_gnb.sort_values(ascending=False)))\n",
    "    f.write(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25283b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "best_lr_model = LogisticRegression(C=1.0, penalty='l1', solver='saga', multi_class='ovr', max_iter=1000, random_state=42)\n",
    "best_svm_model = SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42, probability=True)\n",
    "best_gnb_model = GaussianNB(var_smoothing=1e-09)\n",
    "best_rf_model = RandomForestClassifier(criterion='entropy', max_depth=7, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', random_state=42)\n",
    "\n",
    "\n",
    "best_lr_model.fit(datos[atributos], datos[concepto])\n",
    "best_svm_model.fit(datos[atributos], datos[concepto])\n",
    "best_gnb_model.fit(datos[atributos], datos[concepto])\n",
    "best_rf_model.fit(datos[atributos], datos[concepto])\n",
    "\n",
    "feature_importances_rf = pd.Series(best_rf_model.feature_importances_, index=atributos)\n",
    "\n",
    "result_lr = permutation_importance(best_lr_model, datos[atributos], datos[concepto], n_repeats=10, random_state=42, n_jobs=-1)\n",
    "feature_importances_lr = pd.Series(result_lr.importances_mean, index=atributos)\n",
    "\n",
    "result_svm = permutation_importance(best_svm_model, datos[atributos], datos[concepto], n_repeats=10, random_state=42, n_jobs=-1)\n",
    "feature_importances_svm = pd.Series(result_svm.importances_mean, index=atributos)\n",
    "\n",
    "result_gnb = permutation_importance(best_gnb_model, datos[atributos], datos[concepto], n_repeats=10, random_state=42, n_jobs=-1)\n",
    "feature_importances_gnb = pd.Series(result_gnb.importances_mean, index=atributos)\n",
    "\n",
    "\n",
    "\n",
    "print(feature_importances_lr.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd98cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "\n",
    "concepto = datos.columns[-1]\n",
    "\n",
    "atributos = []\n",
    "\n",
    "# The original code was trying to iterate over a pandas Series as if it were a list of strings.\n",
    "# We need to iterate over the index of the Series to get the attribute names.\n",
    "with open(\"Logistic_Regression_resultados_Corazon_atributos.txt\", \"w\") as f:\n",
    "    f.write(\"Resultados de Regresion Logistica con validacion cruzada (5 folds) con seleccion de atributos:\\n\")\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    acc = []\n",
    "    for atributo in feature_importances_lr.index:\n",
    "      atributos += [atributo]\n",
    "\n",
    "      scoring_funcs = {\n",
    "                  \"cohen_kappa\": 0,\n",
    "                  \"accuracy\": 0,\n",
    "                  \"precision\": 0,\n",
    "                  \"recall\": 0,\n",
    "                  \"f1\": 0,\n",
    "                  \"roc_auc\": 0\n",
    "              }\n",
    "      inicio = time.time()\n",
    "\n",
    "      folds = validacion_cruzada(datos, atributos, concepto, k=5, random=True)\n",
    "\n",
    "      for x_train, y_train, x_test, y_test in folds:\n",
    "                  best_lr_model.fit(x_train, y_train)\n",
    "                  pred = best_lr_model.predict(x_test)\n",
    "                  scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                  scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                  scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                  scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                  scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                  probs = best_lr_model.predict_proba(x_test)\n",
    "                  if len(set(y_test)) == 2:\n",
    "                      # binaria\n",
    "                      scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                  else:\n",
    "                      # multiclase\n",
    "                      scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "      fin = time.time()\n",
    "      tiempo_total = fin - inicio\n",
    "      for key in scoring_funcs:\n",
    "              scoring_funcs[key] /= len(folds)\n",
    "\n",
    "      print(f\"  Average Metrics with attributes {atributos}: {scoring_funcs}\")\n",
    "      print(f\"  Total Time: {tiempo_total:.2f} seconds\")\n",
    "      f.write(f\"  Average Metrics with attributes {atributos}: {scoring_funcs}, Total Time: {tiempo_total:.2f} seconds\\n\")\n",
    "      f1.append(scoring_funcs['f1'])\n",
    "      f.write(f\"F1 = {f1}\\n\")\n",
    "      auc.append(scoring_funcs['roc_auc'])\n",
    "      f.write(f\"AUC = {auc}\\n\")\n",
    "      acc.append(scoring_funcs['accuracy'])\n",
    "      f.write(f\"Accuracy = {acc}\\n\")\n",
    "      f.write(f\"Total Time: {tiempo_total:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f98e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "concepto = datos.columns[-1]\n",
    "\n",
    "atributos_svm = []\n",
    "\n",
    "# Using feature importances from SVM permutation importance\n",
    "with open(\"SVM_resultados_Corazon_atributos.txt\", \"w\") as f:\n",
    "    f.write(\"Resultados de SVM con validacion cruzada (5 folds) con seleccion de atributos:\\n\")\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    acc = []\n",
    "    for atributo in feature_importances_svm.sort_values(ascending=False).index:\n",
    "      atributos_svm += [atributo]\n",
    "\n",
    "      scoring_funcs = {\n",
    "                  \"cohen_kappa\": 0,\n",
    "                  \"accuracy\": 0,\n",
    "                  \"precision\": 0,\n",
    "                  \"recall\": 0,\n",
    "                  \"f1\": 0,\n",
    "                  \"roc_auc\": 0\n",
    "              }\n",
    "      inicio = time.time()\n",
    "\n",
    "      folds = validacion_cruzada(datos, atributos_svm, concepto, k=5, random=True)\n",
    "\n",
    "      for x_train, y_train, x_test, y_test in folds:\n",
    "              best_svm_model.fit(x_train, y_train)\n",
    "              pred = best_svm_model.predict(x_test)\n",
    "              scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "              scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "              scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "              scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "              scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "              probs = best_svm_model.predict_proba(x_test)\n",
    "              if len(set(y_test)) == 2:\n",
    "                  # binaria\n",
    "                  scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "              else:\n",
    "                  # multiclase\n",
    "                  scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "      fin = time.time()\n",
    "      tiempo_total = fin - inicio\n",
    "      for key in scoring_funcs:\n",
    "              scoring_funcs[key] /= len(folds)\n",
    "\n",
    "      print(f\"  Average Metrics with attributes {atributos_svm}: {scoring_funcs}\")\n",
    "      print(f\"  Total Time: {tiempo_total:.2f} seconds\")\n",
    "      f.write(f\"  Average Metrics with attributes {atributos_svm}: {scoring_funcs}, Total Time: {tiempo_total:.2f} seconds\\n\")\n",
    "      f1.append(scoring_funcs['f1'])\n",
    "      f.write(f\"F1 = {f1}\\n\")\n",
    "      auc.append(scoring_funcs['roc_auc'])\n",
    "      f.write(f\"AUC = {auc}\\n\")\n",
    "      acc.append(scoring_funcs['accuracy'])\n",
    "      f.write(f\"Accuracy = {acc}\\n\")\n",
    "      f.write(f\"Total Time: {tiempo_total:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce26dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "concepto = datos.columns[-1]\n",
    "\n",
    "atributos_gb = [] # Changed variable name to reflect Random Forest\n",
    "\n",
    "# Using feature importances from Random Forest feature importance\n",
    "with open(\"Gauss_Bayes_Corazon_atributos.txt\", \"w\") as f: # Added file writing\n",
    "    f.write(\"Resultados de Naive Bayes con validacion cruzada (5 folds) con seleccion de atributos:\\n\") # Added header\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    acc = []\n",
    "    for atributo in feature_importances_gnb.sort_values(ascending=False).index:\n",
    "      atributos_gb += [atributo] # Changed variable name to reflect Random Forest\n",
    "\n",
    "      scoring_funcs = {\n",
    "                  \"cohen_kappa\": 0,\n",
    "                  \"accuracy\": 0,\n",
    "                  \"precision\": 0,\n",
    "                  \"recall\": 0,\n",
    "                  \"f1\": 0,\n",
    "                  \"roc_auc\": 0\n",
    "              }\n",
    "      inicio = time.time()\n",
    "\n",
    "      # Use datos_multinomial for MultinomialNB\n",
    "      folds = validacion_cruzada(datos, atributos_gb, concepto, k=5, random=True) # Changed datos_multinomial to datos and atributos_gnb to atributos_rf\n",
    "\n",
    "      for x_train, y_train, x_test, y_test in folds:\n",
    "                  best_gnb_model.fit(x_train, y_train)\n",
    "                  pred = best_gnb_model.predict(x_test)\n",
    "                  scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                  scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                  scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                  scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                  scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                  probs = best_gnb_model.predict_proba(x_test)\n",
    "                  if len(set(y_test)) == 2:\n",
    "                      # binaria\n",
    "                      scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                  else:\n",
    "                      # multiclase\n",
    "                      scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "      fin = time.time()\n",
    "      tiempo_total = fin - inicio\n",
    "      for key in scoring_funcs:\n",
    "              scoring_funcs[key] /= len(folds)\n",
    "\n",
    "      print(f\"  Average Metrics with attributes {atributos_gb}: {scoring_funcs}\")\n",
    "      print(f\"  Total Time: {tiempo_total:.2f} seconds\")\n",
    "      f.write(f\"  Average Metrics with attributes {atributos_gb}: {scoring_funcs}, Total Time: {tiempo_total:.2f} seconds\\n\") # Added writing to file\n",
    "      f1.append(scoring_funcs['f1'])\n",
    "      f.write(f\"F1 = {f1}\\n\")\n",
    "      auc.append(scoring_funcs['roc_auc'])\n",
    "      f.write(f\"AUC = {auc}\\n\")\n",
    "      acc.append(scoring_funcs['accuracy'])\n",
    "      f.write(f\"Accuracy = {acc}\\n\")\n",
    "      f.write(f\"Total Time: {tiempo_total:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088edb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "concepto = datos.columns[-1]\n",
    "\n",
    "atributos_rf = [] # Changed variable name to reflect Random Forest\n",
    "\n",
    "# Using feature importances from Random Forest feature importance\n",
    "with open(\"Random_Forest_resultados_Corazon_atributos.txt\", \"w\") as f: # Added file writing\n",
    "    f.write(\"Resultados de Random Forest con validacion cruzada (5 folds) con seleccion de atributos:\\n\") # Added header\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    acc = []\n",
    "    for atributo in feature_importances_rf.sort_values(ascending=False).index:\n",
    "      atributos_rf += [atributo] # Changed variable name to reflect Random Forest\n",
    "\n",
    "      scoring_funcs = {\n",
    "                  \"cohen_kappa\": 0,\n",
    "                  \"accuracy\": 0,\n",
    "                  \"precision\": 0,\n",
    "                  \"recall\": 0,\n",
    "                  \"f1\": 0,\n",
    "                  \"roc_auc\": 0\n",
    "              }\n",
    "      inicio = time.time()\n",
    "\n",
    "      # Use datos_multinomial for MultinomialNB\n",
    "      folds = validacion_cruzada(datos, atributos_rf, concepto, k=5, random=True) # Changed datos_multinomial to datos and atributos_gnb to atributos_rf\n",
    "\n",
    "      for x_train, y_train, x_test, y_test in folds:\n",
    "                  best_rf_model.fit(x_train, y_train)\n",
    "                  pred = best_rf_model.predict(x_test)\n",
    "                  scoring_funcs[\"cohen_kappa\"] += cohen_kappa_score(y_test, pred)\n",
    "                  scoring_funcs[\"accuracy\"] += accuracy_score(y_test, pred)\n",
    "                  scoring_funcs[\"precision\"] += precision_score(y_test, pred, average=\"weighted\")\n",
    "                  scoring_funcs[\"recall\"] += recall_score(y_test, pred, average=\"weighted\")\n",
    "                  scoring_funcs[\"f1\"] += f1_score(y_test, pred, average=\"weighted\")\n",
    "                  probs = best_rf_model.predict_proba(x_test)\n",
    "                  if len(set(y_test)) == 2:\n",
    "                      # binaria\n",
    "                      scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs[:, 1])\n",
    "                  else:\n",
    "                      # multiclase\n",
    "                      scoring_funcs[\"roc_auc\"] += roc_auc_score(y_test, probs, multi_class=\"ovr\")\n",
    "\n",
    "      fin = time.time()\n",
    "      tiempo_total = fin - inicio\n",
    "      for key in scoring_funcs:\n",
    "              scoring_funcs[key] /= len(folds)\n",
    "\n",
    "      print(f\"  Average Metrics with attributes {atributos_rf}: {scoring_funcs}\")\n",
    "      print(f\"  Total Time: {tiempo_total:.2f} seconds\")\n",
    "      f.write(f\"  Average Metrics with attributes {atributos_rf}: {scoring_funcs}, Total Time: {tiempo_total:.2f} seconds\\n\") # Added writing to file\n",
    "      f1.append(scoring_funcs['f1'])\n",
    "      f.write(f\"F1 = {f1}\\n\")\n",
    "      auc.append(scoring_funcs['roc_auc'])\n",
    "      f.write(f\"AUC = {auc}\\n\")\n",
    "      acc.append(scoring_funcs['accuracy'])\n",
    "      f.write(f\"Accuracy = {acc}\\n\")\n",
    "      f.write(f\"Total Time: {tiempo_total:.2f} seconds\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
